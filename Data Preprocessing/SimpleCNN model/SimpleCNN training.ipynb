{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L7 Computer vision group project\n",
    "## L7 CV Group - 2\n",
    "### Ben Snow\n",
    "### Nick Lindfield\n",
    "### Ashavidya Kusuma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depth prediction using video game data for real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the problem\n",
    "With the rise of autonomous vehicles, on-camera image processing and augmented reality (AR), there is an increasing need for accurate depth prediction\n",
    "Current methods produce highly noisy results and lack detail, frequently failing to separate background and foreground objects. [14]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image cannot be found](depth_comparison.png \"Depth comaprison\")\n",
    "Figure 1. A comaprison of predicted and ground truth depth predictions from various neural network depth prediction algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current uses for depth prediction/estimation are:\n",
    "- Autonomous driving - predicting the distance of objects on the road, such as other motorists, pedestrians and cyclists.\n",
    "- Image processing - blurring foreground subject from the background.\n",
    "- AR - object occlusion, placing digital characters behind objects such as tables.\n",
    "\n",
    "\n",
    "With this project, we aim to overcome the difficulties of collecting real-world data by using augmented data from video games, we will extract the depth buffer and RGB values to be used as our training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some advantages and disadvantages of real-world and augmented data are:\n",
    "\n",
    "\n",
    "Disadvantages of real-world data\n",
    "- DIS - Expensive -transporting the equipment and staff to the location \n",
    "- DIS - Requires specialized equipment\n",
    "- DIS - Weather condition can affect the accuracy of the data\n",
    "\n",
    "\n",
    "Advantage of video game data\n",
    "- ADV - Cheap and easy to come by\n",
    "- ADV - A larger amount of data can be collected\n",
    "- ADV - Control of lighting and weather conditions \n",
    "- ADV - Accurate depth in poor weather conditions easily\n",
    "- ADV - Can create edge cases but these don’t happen naturally\n",
    "- ADV - No transportation and logistical costs -- Saves the environment\n",
    "- ADV - Procedural generation of datasets\n",
    "\n",
    "\n",
    "Disadvantages of video game data\n",
    "\n",
    "\n",
    "- DIS - May not be an accurate estimation of the real world\n",
    "- DIS - Overfitting to similar environments\n",
    "- DIS - Our data may be too perfect, real-world data can contain noise and other artefacts\n",
    "\n",
    "\n",
    "To analyse our results we will test them against real-world datasets, such as DrivingStereo, KITTI.\n",
    "We intend to build upon existing papers that obtain depth data from video games.  Existing solutions exist but they are not open source and freely available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth detection and video games\n",
    "Advancement of the realistic nature of Computer-Generated (CG) environments could become the basis for training artificial agents. Instead of spending time driving around physical cars in the real world to collect data, a CG environment could be created to simulate the same data. Advantages of CG environments over real-world environments could be that data is much faster to produce, AI’s could learn faster and edge cases (broken down vehicles, towing caravans etc…) could be easily inserted into the simulation at will instead of waiting for them to happen in real life. Transfer learning could be exploited to utilise the training performed in the CG environment to the real-world.\n",
    "\n",
    "Existing techniques are beginning to be used for training self-driving cars this way. Nvidia have created a virtual training environment for training driverless cars for Toyota called Nvidia Constellation. This system allows for the training, testing and evaluation of driverless AI in many thousands of different scenarios allowing for billions of miles of training before hitting the road.\n",
    "\n",
    "Edge case simulations can easily be inserted into the CG environment. Existing scenes from previous CG projects and games can be directly used and included in the training set. This will be advantageous for the AI’s learning capabilities as exposure to as many scenarios as possible generates experience for real-world events. Taken further, simulations of natural disasters, traffic jams, piles ups and crashes can all be generated in CG without any real-world danger! Fully autonomous vehicles should be equipped with the experience to tackle these edge cases, should they need to deal with them.\n",
    "\n",
    "Modern video game engines are capable of creating incredibly complex scene topologies in real time with physically based movements and interactions. An example of this being Grand Theft Auto 5 in which players are free to roam in a realistic rendering of a city, San Andreas, based on Los Angeles. Players can drive a multitude of cars, motorcycles and aircraft around the city on numerous roads and paths. This environment could be an ideal playground for building and training autonomous vehicles for use in the real world. One example of this is in a youtube series by user ‘Sentdex’ who attempted to use a Deep Convolutional Network based on AlexNet to drive on the streets of San Andreas. The model is available on github for download.\n",
    "\n",
    "In a video game, depth field training data can be extracted directly from the video game engine and is called a z-buffer. This been demonstrated by Adrian Courrèges.\n",
    "\n",
    "Existing techniques are available for constructing a depth map from a single frame and from binocular stereo frames. The referenced single frame method employs traditional computer vision techniques whereas the stereo method uses a CNN architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Aims and objectives\n",
    "We aim to generate a dataset consisting of RGB and Depth channels in a wide range of environments and weather conditions.\n",
    "We aim to create and evaluate depth estimation algorithms, utilising PyTorch, OpenCV and a combination of traditional computer vision methods, finally evaluating our results on a range of testing datasets.\n",
    "The following measurable objectives have been identified:\n",
    "- Generate a synthetic image and depth dataset from the video game GTA V\n",
    "- Use data augmentation techniques to increase the amount of data\n",
    "- Use a driving bot and mods to autonomously collect data in different weather conditions and environments\n",
    "- Create a traditional (OpenCV) depth prediction algorithm\n",
    "- Create a neural network only depth prediction algorithm\n",
    "- Create a depth prediction algorithm neural network trained on data augmented with OpenCV techniques\n",
    "- Evaluate the depth prediction model on various real-world and virtual testing datasets including the accepted standard KITTI dataset and a new, more varied dataset, DrivingStereo (Feb 2020).\n",
    "- Compare our model to the model found in our references below, as they have already trained a model on a narrow video game dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data acquisition and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GTA V\n",
    "\n",
    "GTA V is a closed source video game meaning that there is no direct access to the source code available. As a result, the GTA V modding community has found multiple different ways of accessing parts of the rendering pipeline. These methods typically rely on injecting a 'DirectX11 driver' into the game before frame drawing time to intercept data used in the rendering stage. It is here that the depth information is stored.\n",
    "Data collection was split into 3 parts at the beginning of the project and are defined below.\n",
    "\n",
    "Simple collection\n",
    "- Extract order 10 RGB and depth image pairs from GTAV\n",
    "- Drive around in one environment with constant weather conditions, no occlusions and no data augmentation\n",
    "- Data collected so that initial models have data to work with\n",
    "\n",
    "Moderate collection\n",
    "- Use a bot to automatically drive around and drastically increase the dataset size\n",
    "- Use GTA mods to alter the weather conditions and times of day\n",
    "- Drive around new locations such as city and off-road\n",
    "- Implement low-level data augmentation such as translations and reflections\n",
    "\n",
    "Hard collection\n",
    "- Better data augmentation\n",
    "- Add in varied occlusions and domain adaptation\n",
    "- Image style transfer from real-world data to synthetic data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple collection\n",
    "\n",
    "A repository called [GTAVisionExport](https://github.com/umautobots/GTAVisionExport) was used as a starting point to extract single frames of Colour, Depth and Stencil Depth from GTAV. Saved files are stored as .raw images and, as such, cannot be directly imported into python natively. The section 'Image formats' discusses this further.\n",
    "\n",
    "Extraction code within the GTAVisionExport was altered to the following to extract the depth, RGB/colour image and stencil (not used) images. If the 'L' key is pressed, the depth, stencil and color buffers are written to the game file directory.\n",
    "\n",
    "![image cannot be found](Simple_collection_cpp.png \"Extraction code\")\n",
    "Figure 2: Extraction code written in C++ to output RGB, Depth and stencil depth images from GTAV.\n",
    "\n",
    "\n",
    "GTAVisionExport gives all saved images the same filenames (color.raw, depth.raw and stencil.raw). Taking multiple screenshots overwrites the currently saved files. Moving these files out of the game directory then taking another screenshot allows multiple different screenshots to be saved. This is cumbersome and annoying. As such, the GTAVisionExport source code will be changed so that new files are saved with a timestamp and to a new folder for easy, more organised storage.\n",
    "\n",
    "A full description of how to install GTAVisionExport can be seen in the [group google document here.](https://docs.google.com/document/d/1UcQl8Q-COs9_vZ65RKXD8DnIcmRnXzqsJBdfcmd6iJY/edit?usp=sharing)\n",
    "\n",
    "\n",
    "As ‘Simple collection’ only requires on the order of 10 colour and depth images, the manual moving and renaming files technique was be used.\n",
    "\n",
    "Example image outputs from Simple collection can be seen below:\n",
    "\n",
    "![image cannot be found](sample_simple_rgb.png \"Simple RGB\")\n",
    "Figure 3. Example of a colour screenshot extracted from in-game.\n",
    "\n",
    "![image cannot be found](sample_depth_rgb.png \"Simple depth\")\n",
    "Figure 4. Depth information is shown with colour gradients, the more yellow/red the item the closer it is and vice versa for blue and purple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image formats\n",
    "\n",
    "Images extracted from GTAV are stored as .raw files and as such, two functions, `import_raw_colour_image` and `import_raw_depth_image` were written to load the images into numpy arrays.\n",
    "\n",
    "The shape of colour images are: \t (720, 1280, 4)\n",
    "\n",
    "And for depth images they are: \t     (720, 1280)\n",
    " \n",
    "Colour images are read in as 'unit8' with 4 channels, RGBA\n",
    "This means that for every one of the 720*1280 pixels there are 4 numbers that represent the Red, Green, Blue and Alpha channels in the image.\n",
    "\n",
    "Depth images are read in as 'float32' with 1 channel, depth\n",
    "Depth values of Zero relate to infinite depth in the scene and the larger the number the closer to the camera an object is\n",
    "The following conversion formula can be used to convert GTAV depth to real world metres:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data output from simple collection\n",
    "\n",
    "\n",
    "The resulting data from Simple collection consists of 6 colour images of resolution 720x1280 and 6 associated depth maps. Images were taken from within the default GTA V car, on the roads around the starting house, all were taken in the same, daytime lighting conditions with clear weather.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moderate collection\n",
    "\n",
    "Simple collection relied on manually pressing a keyboard key to capture an RGB image and depth map pair, part of moderate collection is to automate this process. To achieve this, and autoclicker software [AutoHotKey](https://www.autohotkey.com/) was used. A simple script to press the capture key automatically every 600ms was written, this is visible via Ben's [github repo](https://github.com/BenSnow6/depth_estimation/blob/master/Data_Collection/Moderate%20collection/testScript.ahk.ahk).\n",
    "\n",
    "In addition to this, the extraction code was altered to allow for multiple images to be outputted without overwriting previous images. The process is outlined below:\n",
    "- On ‘l’ key press\n",
    "- Open notepad file with a number stored in it\n",
    "- Attach number to ‘depth.raw’ and ‘colour.raw’ strings\n",
    "- Save the outputs in folders, one for depth and one for colour\n",
    "- Increment number stored in notepad.\n",
    "\n",
    "The following C++ code was written to achieve this.\n",
    "\n",
    "![image not found](Moderate_collection_cpp.png \"Moderate collection extraction code\")\n",
    "Figure 5. Extraction code written in C++ altered from the GTAVisionExport tool to add numeric labels to image filenames during collection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAutodrive\n",
    "In order to truly collect data autonomously the automatic driving modification called [VAutodrive](https://www.gta5-mods.com/scripts/vautodrive) (Five Auto drive) was downloaded and utilised. It is simple to use: get in a car, change the view to 1st person (by pressing ‘V’), go to the map, set a waypoint (double click on road) then press ctrl+J to start the autopilot. By default the pilot will drive at 25mph, obeying traffic laws and driving non erratically. After starting VAutopilot use the autohotkey script and press Ctrl+L to start it, this will start automatic collection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NativeUI\n",
    "To change the weather conditions in GTAV, a plugin called NativeUI was used allowing access to an in-game menu. Open this menu with ‘F4’ and use the number pad to navigate it. In NativeUI the weather conditions can be changed, along with the time of day. These were altered and automatic collection was used to collect scenes of around 8000 frames in different weather conditions and times of day.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output from Moderate collection\n",
    "\n",
    "Moderate collection resulted in 7857 colour and 7857 associated depth images extracted from GTAV in 5 different weather conditions. All images are stored in .raw files with resolutions of 720x1280 and a size of 3.6MB each. The output is summarised in Table 1 below.\n",
    "\n",
    "Conditions___| Number of colour images____| Number of depth images____| Size of data (GigaBytes) \n",
    "---|:---:|:---:|:---:\n",
    "Sunny | 500 | 500 | 3.43\n",
    "Snowy | 1000 | 1000 | 6.86\n",
    "Foggy_dark | 1100| 1100 | 7.55\n",
    "Blizzard | 3000 | 3000 | 20.5\n",
    "Rain_night | 2257 | 2257 | 15.4\n",
    "Total | 7857 | 7857 | 53.9 |\n",
    "Table 1. Characteristics of the data collected in moderate collection.\n",
    "\n",
    "## File structure\n",
    "Files are stored in folders with subfolders for Colour and Depth. Each colour file is named colour_0000x.raw where x is the frame number. For example, the range of file names in the ‘Sunny’ conditions are colour_00001.raw to colour_00500.raw and depth_00001.raw to depth_00500.raw.\n",
    "Data was collected in the order of Table 1, above. It follows that the first colour filenames in the Snowy collection are colour_00501.raw to colour_01500.raw and the same for depth.\n",
    "Images are stored to separate colour and depth directories and for each instance of new conditions, a new directory folder is made and used (Sunny/colour, Snowy/depth etc…). A full list of driving conditions along with file labels can be found under the ‘Conditions.txt’ in the base of the /Moderate_collection directory on [One Drive](https://livebournemouthac-my.sharepoint.com/:f:/g/personal/bsnow_bournemouth_ac_uk/EmXxIrfiQg1IlJkc18oqVbwBEm-4461czkd9OvFbvjH1UA?e=5YOjrU).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installs and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Collecting pickle-mixin\n  Downloading pickle-mixin-1.0.2.tar.gz (5.1 kB)\nBuilding wheels for collected packages: pickle-mixin\n  Building wheel for pickle-mixin (setup.py): started\n  Building wheel for pickle-mixin (setup.py): finished with status 'done'\n  Created wheel for pickle-mixin: filename=pickle_mixin-1.0.2-py3-none-any.whl size=6002 sha256=606dfa158d75027c849e08d00715d939094ca5e3cc814bccdd3aa0b99fe0412e\n  Stored in directory: c:\\users\\ben\\appdata\\local\\pip\\cache\\wheels\\2a\\a4\\6c\\83bfbc3b94f1bb43d634b07a6a893fd437a45c58b29aea5142\nSuccessfully built pickle-mixin\nInstalling collected packages: pickle-mixin\nSuccessfully installed pickle-mixin-1.0.2\n"
    }
   ],
   "source": [
    "!pip install pickle-mixin\n",
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from Evaluation_procedure.eval_functions import isValid, get_depth, calc_errors, predict_and_gt, mean_and_std_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from Functions import import_raw_colour_image, import_raw_depth_image, show_depth_image, show_img\n",
    "import os\n",
    "from os import walk\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import math\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the csv data structure\n",
    "\n",
    "A csv containing the details of the Moderate Collection data was created, this file is read in to two variables, `folder_names` and `num_files`. These will be used to create a list of filenames from which will then be used to construct the dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('..\\data_descriptions.csv', newline='') as csvfile: ###### data_descriptions csv must be in this relative location\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    count = 0\n",
    "    for row in spamreader:\n",
    "        if count == 0:\n",
    "            folder_names = row ## store names of folders in the directory\n",
    "        else:\n",
    "            num_files = row ## store number of files within folder\n",
    "        count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(num_files)):\n",
    "    num_files[i] = int(num_files[i]) ## convert number of files from string to int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a list of filenames for use in the dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_numbers = [\"{0:05}\".format(i) for i in range(1, sum(num_files)+1)]\n",
    "colour_filenames = []\n",
    "depth_filenames = []\n",
    "for num in list_of_numbers:\n",
    "    colour_filenames.append(f\"colour_{num}.raw\")  ## append formatted number labels to file names\n",
    "    depth_filenames.append(f\"depth_{num}.raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class\n",
    "\n",
    "Due to having over 60GB of images in the Moderate Collection dataset it is impossible to simultaneously load all of them into the 16GB of available memory. As such a custom PyTorch dataset class will be created along with a data loader allowing for batches of n images to be loaded into memory at one time.\n",
    "\n",
    "The dataset class, named ModerateDataset, loads the images from a fixed directory. For this to work on a given computer, the ‘Path’ must be changed to the base directory of the Moderate Dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModerateDataset(Dataset):\n",
    "\n",
    "    def __init__(self, col_dir='', depth_dir='', transform=None, trans_on=False):\n",
    "        self.path_names = {}\n",
    "        for folder in folder_names:\n",
    "            self.path_names[f\"{folder}\"] = {}\n",
    "        for folder in folder_names:\n",
    "            self.path_names[f'{folder}']['colour'] = {}\n",
    "            self.path_names[f'{folder}']['depth'] = {}\n",
    "        for i in range(1, num_files[0]):\n",
    "            self.path_names['Sunny']['colour'][f\"{i}\"] = {}\n",
    "            self.path_names['Sunny']['depth'][f\"{i}\"] = {}\n",
    "        print(\"*************MAKE SURE THE PATH FILE IN THE FOR LOOP IS THE BASE IMAGE DIRECTORY ON YOUR COMPUTER**************\")\n",
    "        count = 0\n",
    "        for folder in folder_names:\n",
    "            for i in range(0, num_files[folder_names.index(folder)]):\n",
    "                self.path_names[f'{folder}']['colour'][f'{i+1}'] = Path(f\"C:/Users/Ben/OneDrive - Bournemouth University/Computer Vision/Moderate collection/{folder}/colour/{colour_filenames[count+i]}\")  ## Change this path here!!!!\n",
    "                self.path_names[f'{folder}']['depth'][f'{i+1}'] = Path(f\"C:/Users/Ben/OneDrive - Bournemouth University/Computer Vision/Moderate collection/{folder}/depth/{depth_filenames[count+i]}\")   ## Change this path here!!!!\n",
    "            count = count + num_files[folder_names.index(folder)]\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.col_dir = col_dir\n",
    "        self.depth_dir = depth_dir\n",
    "        self.trans_on = trans_on\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        if idx == 0:\n",
    "            \n",
    "            self.col_dir = self.path_names[f'{folder_names[0]}']['colour'][f'{idx+1}']\n",
    "            self.depth_dir = self.path_names[f'{folder_names[0]}']['depth'][f'{idx+1}']\n",
    "        \n",
    "        if (idx>0 and idx <= num_files[0]):  ## 1-500\n",
    "\n",
    "            self.col_dir = self.path_names[f'{folder_names[0]}']['colour'][f'{idx}']\n",
    "            self.depth_dir = self.path_names[f'{folder_names[0]}']['depth'][f'{idx}']\n",
    "\n",
    "        elif (idx > num_files[0] and idx < (sum(num_files[:2])+1)): ## 501 - 1500\n",
    "\n",
    "            self.col_dir = self.path_names[f'{folder_names[1]}']['colour'][f'{idx-num_files[0]}']\n",
    "            self.depth_dir = self.path_names[f'{folder_names[1]}']['depth'][f'{idx-num_files[0]}']\n",
    "\n",
    "        elif (idx > sum(num_files[:2]) and idx < (sum(num_files[:3])+1) ): ## 1501 - 2600\n",
    "\n",
    "            self.col_dir = self.path_names[f'{folder_names[2]}']['colour'][f'{idx-sum(num_files[:2])}'] # -1500\n",
    "            self.depth_dir = self.path_names[f'{folder_names[2]}']['depth'][f'{idx-sum(num_files[:2])}']\n",
    "\n",
    "        elif (idx > sum(num_files[:3]) and idx < (sum(num_files[:4])+1) ): ## 2601 - 5600\n",
    "\n",
    "            self.col_dir = self.path_names[f'{folder_names[3]}']['colour'][f'{idx-sum(num_files[:3])}'] #-2600\n",
    "            self.depth_dir = self.path_names[f'{folder_names[3]}']['depth'][f'{idx-sum(num_files[:3])}']\n",
    "            \n",
    "        elif (idx > sum(num_files[:4]) and idx < (sum(num_files[:5])+1) ): ## 5601 - 7857\n",
    "\n",
    "            self.col_dir = self.path_names[f'{folder_names[4]}']['colour'][f'{idx-sum(num_files[:4])}'] # -5600\n",
    "            self.depth_dir = self.path_names[f'{folder_names[4]}']['depth'][f'{idx-sum(num_files[:4])}']\n",
    "\n",
    "        elif (idx > sum(num_files)):\n",
    "            raise NameError('Index outside of range')\n",
    "\n",
    "        col_img = import_raw_colour_image(self.col_dir)\n",
    "        depth_img = import_raw_depth_image(self.depth_dir)\n",
    "        if self.trans_on == True:\n",
    "            col_img = torch.from_numpy(np.flip(col_img,axis=0).copy()) # apply any transforms\n",
    "            depth_img = torch.from_numpy(np.flip(depth_img,axis=0).copy()) # apply any transforms\n",
    "            col_img = col_img.transpose(0,2)\n",
    "            col_img = col_img.transpose(1,2)\n",
    "        if self.transform: # if any transforms were given to initialiser\n",
    "            col_img = self.transform(col_img) # apply any transforms\n",
    "        return col_img, depth_img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum(num_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an instance of the dataset in order to create training, validation and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "*************MAKE SURE THE PATH FILE IN THE FOR LOOP IS THE BASE IMAGE DIRECTORY ON YOUR COMPUTER**************\n"
    }
   ],
   "source": [
    "total_Data = ModerateDataset(trans_on=True)  ## instancing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, validation and test splitting\n",
    "\n",
    "It is of vital importance to establish the separation of three datasets: training, validation and testing. Training data is used to train the neural network model and validation data is used to check that the model is not overfitting to the training data. Testing data is used to check the performance of the trained model on unseen data to evaluate performance with a set of predefined metrics (defined in the evaluation procedure section).\n",
    "\n",
    "A train, validation, testing split of 80/10/10 has been used to create three datasets: train_dataset, val_dataset and test_dataset. This split is commonly used in machine learning research. These datasets all inherit from the ModerateDasaset class. For each of these datasets, a data loader was created to load in a batch of images at once instead of loading the entire dataset to memory. To train the model, the training and validation dataloaders are used. This ensures that no testing data is used in any step of training the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(total_Data))\n",
    "val_size = int((len(total_Data) - train_size)/2)\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(total_Data, [train_size, val_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 16\n",
    "tr_dl  = DataLoader(train_dataset,  batch_size=batch_sz, shuffle=True,  num_workers=0)\n",
    "val_dl = DataLoader(val_dataset,  batch_size=batch_sz, shuffle=True,  num_workers=0)\n",
    "test_dl = DataLoader(test_dataset,  batch_size=batch_sz, shuffle=True,  num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN model\n",
    "\n",
    "One of the key deliverables in the project proposal is to create a simple neural network architecture that uses an RGB image as an input outputs a depth image. This is realised below by the use of a convolutional neural network. The network, referred to as the 'Simple CNN' model, is defined below. It consists of two convolutional and two deconvolutional layers. A 3 channel colour image is input into the first conv layer, this increases the number of images channels to 6. After this, a rectified linear unit activation function is applied to the convolved data. The image is then passed through another conv layer increasing the channels to 12. This then leads to two deconv layers, outputting a singular channeled depth image with the same resolution as the imput image, this is ensured by the use of the kernal size, stride, and padding variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3,  out_channels=6, kernel_size=3, stride=1, padding=1), \n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=6, out_channels=12, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose2d(in_channels = 12, out_channels=6, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose2d(in_channels = 6, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
    "    nn.Sigmoid()\n",
    ").cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a model summary to show the structure of the network and setting the image dimensions to (3,720,1280) and the batch size to 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [16, 6, 720, 1280]             168\n              ReLU-2         [16, 6, 720, 1280]               0\n            Conv2d-3        [16, 12, 720, 1280]             660\n              ReLU-4        [16, 12, 720, 1280]               0\n   ConvTranspose2d-5         [16, 6, 720, 1280]             654\n              ReLU-6         [16, 6, 720, 1280]               0\n   ConvTranspose2d-7         [16, 1, 720, 1280]              55\n           Sigmoid-8         [16, 1, 720, 1280]               0\n================================================================\nTotal params: 1,537\nTrainable params: 1,537\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 168.75\nForward/backward pass size (MB): 5625.00\nParams size (MB): 0.01\nEstimated Total Size (MB): 5793.76\n----------------------------------------------------------------\n"
    }
   ],
   "source": [
    "summary(net, (3,720,1280), 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Here, a fitting function is defined in which a neural network, training dataloader and validation dataloader are passed. The user can modify the loss function, number of training epochs, learning rate and weight decay used to train the network.\n",
    "\n",
    "The fitting/training function used to train the model is defined by the following for the SimpleCNN:\n",
    "- Loss function: Mean square error loss\n",
    "- Epochs: 2\n",
    "- Learning rate: 1x10-3\n",
    "- Weight decay: 1x10-3\n",
    "- Optimiser: Adam\n",
    "- Training batch size = 16\n",
    "- Validation batch size = 16\n",
    "- Shuffling: Training=True, Validation=False\n",
    "- Metrics that are tracked: Training loss, validation loss, validation accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(net, tr_dl, val_dl, loss=nn.MSELoss(), epochs=3, lr=3e-3, wd=1e-3):   \n",
    "\n",
    "    Ltr_hist, Lval_hist = [], []    \n",
    "    opt = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "    for epoch in trange(epochs):\n",
    "        \n",
    "        L = []\n",
    "        dl = (iter(tr_dl))\n",
    "        count_train = 0\n",
    "        for xb, yb in tqdm(dl, leave=False):\n",
    "            xb, yb = xb.float(), yb.float()\n",
    "            xb, yb = xb.cuda(), yb.cuda()\n",
    "            y_ = net(xb)\n",
    "            l = loss(y_, yb)\n",
    "            opt.zero_grad()\n",
    "            l.backward()\n",
    "            opt.step()\n",
    "            L.append(l.detach().cpu().numpy())\n",
    "            print(f\"Training on batch {count_train} of {int(train_size/batch_sz)}\")\n",
    "            count_train+= 1\n",
    "\n",
    "        # disable gradient calculations for validation\n",
    "        for p in net.parameters(): p.requires_grad = False\n",
    "\n",
    "        Lval, Aval = [], []\n",
    "        val_it = iter(val_dl)\n",
    "        val_count = 0\n",
    "        for xb, yb in tqdm(val_it, leave=False):\n",
    "            xb, yb = xb.float(), yb.float()\n",
    "            xb, yb = xb.cuda(), yb.cuda()\n",
    "            y_ = net(xb)\n",
    "            l = loss(y_, yb)\n",
    "            Lval.append(l.detach().cpu().numpy())\n",
    "            Aval.append((y_.max(dim=1)[1] == yb).float().mean().cpu().numpy())\n",
    "            print(f\"Validating on batch {val_count} of {int(val_size/batch_sz)}\")\n",
    "            val_count+= 1\n",
    "\n",
    "        # enable gradient calculations for next epoch \n",
    "        for p in net.parameters(): p.requires_grad = True \n",
    "            \n",
    "        Ltr_hist.append(np.mean(L))\n",
    "        Lval_hist.append(np.mean(Lval))\n",
    "        print(f'training loss: {np.mean(L):0.4f}\\tvalidation loss: {np.mean(Lval):0.4f}\\tvalidation accuracy: {np.mean(Aval):0.2f}')\n",
    "    return Ltr_hist, Lval_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "\n",
    "For the SimpleCNN it was not imperative to train the model for a large number of epochs since the goal was to produce data that could be used to test the evaluation procedure.\n",
    "Due to the large amount of training data, training on a Nvidia 2070 Max Q 8GB and an i7-8750H with 16GB ram takes around 30 minutes per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4cae10bf84574ca99f93e2050a44b768"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=393.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "800cd70e92ca4b659cf98ff86c35851a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training on batch 0 of 392\nTraining on batch 1 of 392\nTraining on batch 2 of 392\nTraining on batch 3 of 392\nTraining on batch 4 of 392\nTraining on batch 5 of 392\nTraining on batch 6 of 392\nTraining on batch 7 of 392\nTraining on batch 8 of 392\nTraining on batch 9 of 392\nTraining on batch 10 of 392\nTraining on batch 11 of 392\nTraining on batch 12 of 392\nTraining on batch 13 of 392\nTraining on batch 14 of 392\nTraining on batch 15 of 392\nTraining on batch 16 of 392\nTraining on batch 17 of 392\nTraining on batch 18 of 392\nTraining on batch 19 of 392\nTraining on batch 20 of 392\nTraining on batch 21 of 392\nTraining on batch 22 of 392\nTraining on batch 23 of 392\nTraining on batch 24 of 392\nTraining on batch 25 of 392\nTraining on batch 26 of 392\nTraining on batch 27 of 392\nTraining on batch 28 of 392\nTraining on batch 29 of 392\nTraining on batch 30 of 392\nTraining on batch 31 of 392\nTraining on batch 32 of 392\nTraining on batch 33 of 392\nTraining on batch 34 of 392\nTraining on batch 35 of 392\nTraining on batch 36 of 392\nTraining on batch 37 of 392\nTraining on batch 38 of 392\nTraining on batch 39 of 392\nTraining on batch 40 of 392\nTraining on batch 41 of 392\nTraining on batch 42 of 392\nTraining on batch 43 of 392\nTraining on batch 44 of 392\nTraining on batch 45 of 392\nTraining on batch 46 of 392\nTraining on batch 47 of 392\nTraining on batch 48 of 392\nTraining on batch 49 of 392\nTraining on batch 50 of 392\nTraining on batch 51 of 392\nTraining on batch 52 of 392\nTraining on batch 53 of 392\nTraining on batch 54 of 392\nTraining on batch 55 of 392\nTraining on batch 56 of 392\nTraining on batch 57 of 392\nTraining on batch 58 of 392\nTraining on batch 59 of 392\nTraining on batch 60 of 392\nTraining on batch 61 of 392\nTraining on batch 62 of 392\nTraining on batch 63 of 392\nTraining on batch 64 of 392\nTraining on batch 65 of 392\nTraining on batch 66 of 392\nTraining on batch 67 of 392\nTraining on batch 68 of 392\nTraining on batch 69 of 392\nTraining on batch 70 of 392\nTraining on batch 71 of 392\nTraining on batch 72 of 392\nTraining on batch 73 of 392\nTraining on batch 74 of 392\nTraining on batch 75 of 392\nTraining on batch 76 of 392\nTraining on batch 77 of 392\nTraining on batch 78 of 392\nTraining on batch 79 of 392\nTraining on batch 80 of 392\nTraining on batch 81 of 392\nTraining on batch 82 of 392\nTraining on batch 83 of 392\nTraining on batch 84 of 392\nTraining on batch 85 of 392\nTraining on batch 86 of 392\nTraining on batch 87 of 392\nTraining on batch 88 of 392\nTraining on batch 89 of 392\nTraining on batch 90 of 392\nTraining on batch 91 of 392\nTraining on batch 92 of 392\nTraining on batch 93 of 392\nTraining on batch 94 of 392\nTraining on batch 95 of 392\nTraining on batch 96 of 392\nTraining on batch 97 of 392\nTraining on batch 98 of 392\nTraining on batch 99 of 392\nTraining on batch 100 of 392\nTraining on batch 101 of 392\nTraining on batch 102 of 392\nTraining on batch 103 of 392\nTraining on batch 104 of 392\nTraining on batch 105 of 392\nTraining on batch 106 of 392\nTraining on batch 107 of 392\nTraining on batch 108 of 392\nTraining on batch 109 of 392\nTraining on batch 110 of 392\nTraining on batch 111 of 392\nTraining on batch 112 of 392\nTraining on batch 113 of 392\nTraining on batch 114 of 392\nTraining on batch 115 of 392\nTraining on batch 116 of 392\nTraining on batch 117 of 392\nTraining on batch 118 of 392\nTraining on batch 119 of 392\nTraining on batch 120 of 392\nTraining on batch 121 of 392\nTraining on batch 122 of 392\nTraining on batch 123 of 392\nTraining on batch 124 of 392\nTraining on batch 125 of 392\nTraining on batch 126 of 392\nTraining on batch 127 of 392\nTraining on batch 128 of 392\nTraining on batch 129 of 392\nTraining on batch 130 of 392\nTraining on batch 131 of 392\nTraining on batch 132 of 392\nTraining on batch 133 of 392\nTraining on batch 134 of 392\nTraining on batch 135 of 392\nTraining on batch 136 of 392\nTraining on batch 137 of 392\nTraining on batch 138 of 392\nTraining on batch 139 of 392\nTraining on batch 140 of 392\nTraining on batch 141 of 392\nTraining on batch 142 of 392\nTraining on batch 143 of 392\nTraining on batch 144 of 392\nTraining on batch 145 of 392\nTraining on batch 146 of 392\nTraining on batch 147 of 392\nTraining on batch 148 of 392\nTraining on batch 149 of 392\nTraining on batch 150 of 392\nTraining on batch 151 of 392\nTraining on batch 152 of 392\nTraining on batch 153 of 392\nTraining on batch 154 of 392\nTraining on batch 155 of 392\nTraining on batch 156 of 392\nTraining on batch 157 of 392\nTraining on batch 158 of 392\nTraining on batch 159 of 392\nTraining on batch 160 of 392\nTraining on batch 161 of 392\nTraining on batch 162 of 392\nTraining on batch 163 of 392\nTraining on batch 164 of 392\nTraining on batch 165 of 392\nTraining on batch 166 of 392\nTraining on batch 167 of 392\nTraining on batch 168 of 392\nTraining on batch 169 of 392\nTraining on batch 170 of 392\nTraining on batch 171 of 392\nTraining on batch 172 of 392\nTraining on batch 173 of 392\nTraining on batch 174 of 392\nTraining on batch 175 of 392\nTraining on batch 176 of 392\nTraining on batch 177 of 392\nTraining on batch 178 of 392\nTraining on batch 179 of 392\nTraining on batch 180 of 392\nTraining on batch 181 of 392\nTraining on batch 182 of 392\nTraining on batch 183 of 392\nTraining on batch 184 of 392\nTraining on batch 185 of 392\nTraining on batch 186 of 392\nTraining on batch 187 of 392\nTraining on batch 188 of 392\nTraining on batch 189 of 392\nTraining on batch 190 of 392\nTraining on batch 191 of 392\nTraining on batch 192 of 392\nTraining on batch 193 of 392\nTraining on batch 194 of 392\nTraining on batch 195 of 392\nTraining on batch 196 of 392\nTraining on batch 197 of 392\nTraining on batch 198 of 392\nTraining on batch 199 of 392\nTraining on batch 200 of 392\nTraining on batch 201 of 392\nTraining on batch 202 of 392\nTraining on batch 203 of 392\nTraining on batch 204 of 392\nTraining on batch 205 of 392\nTraining on batch 206 of 392\nTraining on batch 207 of 392\nTraining on batch 208 of 392\nTraining on batch 209 of 392\nTraining on batch 210 of 392\nTraining on batch 211 of 392\nTraining on batch 212 of 392\nTraining on batch 213 of 392\nTraining on batch 214 of 392\nTraining on batch 215 of 392\nTraining on batch 216 of 392\nTraining on batch 217 of 392\nTraining on batch 218 of 392\nTraining on batch 219 of 392\nTraining on batch 220 of 392\nTraining on batch 221 of 392\nTraining on batch 222 of 392\nTraining on batch 223 of 392\nTraining on batch 224 of 392\nTraining on batch 225 of 392\nTraining on batch 226 of 392\nTraining on batch 227 of 392\nTraining on batch 228 of 392\nTraining on batch 229 of 392\nTraining on batch 230 of 392\nTraining on batch 231 of 392\nTraining on batch 232 of 392\nTraining on batch 233 of 392\nTraining on batch 234 of 392\nTraining on batch 235 of 392\nTraining on batch 236 of 392\nTraining on batch 237 of 392\nTraining on batch 238 of 392\nTraining on batch 239 of 392\nTraining on batch 240 of 392\nTraining on batch 241 of 392\nTraining on batch 242 of 392\nTraining on batch 243 of 392\nTraining on batch 244 of 392\nTraining on batch 245 of 392\nTraining on batch 246 of 392\nTraining on batch 247 of 392\nTraining on batch 248 of 392\nTraining on batch 249 of 392\nTraining on batch 250 of 392\nTraining on batch 251 of 392\nTraining on batch 252 of 392\nTraining on batch 253 of 392\nTraining on batch 254 of 392\nTraining on batch 255 of 392\nTraining on batch 256 of 392\nTraining on batch 257 of 392\nTraining on batch 258 of 392\nTraining on batch 259 of 392\nTraining on batch 260 of 392\nTraining on batch 261 of 392\nTraining on batch 262 of 392\nTraining on batch 263 of 392\nTraining on batch 264 of 392\nTraining on batch 265 of 392\nTraining on batch 266 of 392\nTraining on batch 267 of 392\nTraining on batch 268 of 392\nTraining on batch 269 of 392\nTraining on batch 270 of 392\nTraining on batch 271 of 392\nTraining on batch 272 of 392\nTraining on batch 273 of 392\nTraining on batch 274 of 392\nTraining on batch 275 of 392\nTraining on batch 276 of 392\nTraining on batch 277 of 392\nTraining on batch 278 of 392\nTraining on batch 279 of 392\nTraining on batch 280 of 392\nTraining on batch 281 of 392\nTraining on batch 282 of 392\nTraining on batch 283 of 392\nTraining on batch 284 of 392\nTraining on batch 285 of 392\nTraining on batch 286 of 392\nTraining on batch 287 of 392\nTraining on batch 288 of 392\nTraining on batch 289 of 392\nTraining on batch 290 of 392\nTraining on batch 291 of 392\nTraining on batch 292 of 392\nTraining on batch 293 of 392\nTraining on batch 294 of 392\nTraining on batch 295 of 392\nTraining on batch 296 of 392\nTraining on batch 297 of 392\nTraining on batch 298 of 392\nTraining on batch 299 of 392\nTraining on batch 300 of 392\nTraining on batch 301 of 392\nTraining on batch 302 of 392\nTraining on batch 303 of 392\nTraining on batch 304 of 392\nTraining on batch 305 of 392\nTraining on batch 306 of 392\nTraining on batch 307 of 392\nTraining on batch 308 of 392\nTraining on batch 309 of 392\nTraining on batch 310 of 392\nTraining on batch 311 of 392\nTraining on batch 312 of 392\nTraining on batch 313 of 392\nTraining on batch 314 of 392\nTraining on batch 315 of 392\nTraining on batch 316 of 392\nTraining on batch 317 of 392\nTraining on batch 318 of 392\nTraining on batch 319 of 392\nTraining on batch 320 of 392\nTraining on batch 321 of 392\nTraining on batch 322 of 392\nTraining on batch 323 of 392\nTraining on batch 324 of 392\nTraining on batch 325 of 392\nTraining on batch 326 of 392\nTraining on batch 327 of 392\nTraining on batch 328 of 392\nTraining on batch 329 of 392\nTraining on batch 330 of 392\nTraining on batch 331 of 392\nTraining on batch 332 of 392\nTraining on batch 333 of 392\nTraining on batch 334 of 392\nTraining on batch 335 of 392\nTraining on batch 336 of 392\nTraining on batch 337 of 392\nTraining on batch 338 of 392\nTraining on batch 339 of 392\nTraining on batch 340 of 392\nTraining on batch 341 of 392\nTraining on batch 342 of 392\nTraining on batch 343 of 392\nTraining on batch 344 of 392\nTraining on batch 345 of 392\nTraining on batch 346 of 392\nTraining on batch 347 of 392\nTraining on batch 348 of 392\nTraining on batch 349 of 392\nTraining on batch 350 of 392\nTraining on batch 351 of 392\nTraining on batch 352 of 392\nTraining on batch 353 of 392\nTraining on batch 354 of 392\nTraining on batch 355 of 392\nTraining on batch 356 of 392\nTraining on batch 357 of 392\nTraining on batch 358 of 392\nTraining on batch 359 of 392\nTraining on batch 360 of 392\nTraining on batch 361 of 392\nTraining on batch 362 of 392\nTraining on batch 363 of 392\nTraining on batch 364 of 392\nTraining on batch 365 of 392\nTraining on batch 366 of 392\nTraining on batch 367 of 392\nTraining on batch 368 of 392\nTraining on batch 369 of 392\nTraining on batch 370 of 392\nTraining on batch 371 of 392\nTraining on batch 372 of 392\nTraining on batch 373 of 392\nTraining on batch 374 of 392\nTraining on batch 375 of 392\nTraining on batch 376 of 392\nTraining on batch 377 of 392\nTraining on batch 378 of 392\nTraining on batch 379 of 392\nTraining on batch 380 of 392\nTraining on batch 381 of 392\nTraining on batch 382 of 392\nTraining on batch 383 of 392\nTraining on batch 384 of 392\nTraining on batch 385 of 392\nTraining on batch 386 of 392\nTraining on batch 387 of 392\nTraining on batch 388 of 392\nTraining on batch 389 of 392\nTraining on batch 390 of 392\nTraining on batch 391 of 392\nTraining on batch 392 of 392\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51ce54e73c014e4688c09cf5ab35b7ff"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Validating on batch 0 of 49\nValidating on batch 1 of 49\nValidating on batch 2 of 49\nValidating on batch 3 of 49\nValidating on batch 4 of 49\nValidating on batch 5 of 49\nValidating on batch 6 of 49\nValidating on batch 7 of 49\nValidating on batch 8 of 49\nValidating on batch 9 of 49\nValidating on batch 10 of 49\nValidating on batch 11 of 49\nValidating on batch 12 of 49\nValidating on batch 13 of 49\nValidating on batch 14 of 49\nValidating on batch 15 of 49\nValidating on batch 16 of 49\nValidating on batch 17 of 49\nValidating on batch 18 of 49\nValidating on batch 19 of 49\nValidating on batch 20 of 49\nValidating on batch 21 of 49\nValidating on batch 22 of 49\nValidating on batch 23 of 49\nValidating on batch 24 of 49\nValidating on batch 25 of 49\nValidating on batch 26 of 49\nValidating on batch 27 of 49\nValidating on batch 28 of 49\nValidating on batch 29 of 49\nValidating on batch 30 of 49\nValidating on batch 31 of 49\nValidating on batch 32 of 49\nValidating on batch 33 of 49\nValidating on batch 34 of 49\nValidating on batch 35 of 49\nValidating on batch 36 of 49\nValidating on batch 37 of 49\nValidating on batch 38 of 49\nValidating on batch 39 of 49\nValidating on batch 40 of 49\nValidating on batch 41 of 49\nValidating on batch 42 of 49\nValidating on batch 43 of 49\nValidating on batch 44 of 49\nValidating on batch 45 of 49\nValidating on batch 46 of 49\nValidating on batch 47 of 49\nValidating on batch 48 of 49\nValidating on batch 49 of 49\ntraining loss: 0.0056\tvalidation loss: 0.0013\tvalidation accuracy: 0.15\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=393.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad3a198db67643baaa6f9e43cf82cbef"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training on batch 0 of 392\nTraining on batch 1 of 392\nTraining on batch 2 of 392\nTraining on batch 3 of 392\nTraining on batch 4 of 392\nTraining on batch 5 of 392\nTraining on batch 6 of 392\nTraining on batch 7 of 392\nTraining on batch 8 of 392\nTraining on batch 9 of 392\nTraining on batch 10 of 392\nTraining on batch 11 of 392\nTraining on batch 12 of 392\nTraining on batch 13 of 392\nTraining on batch 14 of 392\nTraining on batch 15 of 392\nTraining on batch 16 of 392\nTraining on batch 17 of 392\nTraining on batch 18 of 392\nTraining on batch 19 of 392\nTraining on batch 20 of 392\nTraining on batch 21 of 392\nTraining on batch 22 of 392\nTraining on batch 23 of 392\nTraining on batch 24 of 392\nTraining on batch 25 of 392\nTraining on batch 26 of 392\nTraining on batch 27 of 392\nTraining on batch 28 of 392\nTraining on batch 29 of 392\nTraining on batch 30 of 392\nTraining on batch 31 of 392\nTraining on batch 32 of 392\nTraining on batch 33 of 392\nTraining on batch 34 of 392\nTraining on batch 35 of 392\nTraining on batch 36 of 392\nTraining on batch 37 of 392\nTraining on batch 38 of 392\nTraining on batch 39 of 392\nTraining on batch 40 of 392\nTraining on batch 41 of 392\nTraining on batch 42 of 392\nTraining on batch 43 of 392\nTraining on batch 44 of 392\nTraining on batch 45 of 392\nTraining on batch 46 of 392\nTraining on batch 47 of 392\nTraining on batch 48 of 392\nTraining on batch 49 of 392\nTraining on batch 50 of 392\nTraining on batch 51 of 392\nTraining on batch 52 of 392\nTraining on batch 53 of 392\nTraining on batch 54 of 392\nTraining on batch 55 of 392\nTraining on batch 56 of 392\nTraining on batch 57 of 392\nTraining on batch 58 of 392\nTraining on batch 59 of 392\nTraining on batch 60 of 392\nTraining on batch 61 of 392\nTraining on batch 62 of 392\nTraining on batch 63 of 392\nTraining on batch 64 of 392\nTraining on batch 65 of 392\nTraining on batch 66 of 392\nTraining on batch 67 of 392\nTraining on batch 68 of 392\nTraining on batch 69 of 392\nTraining on batch 70 of 392\nTraining on batch 71 of 392\nTraining on batch 72 of 392\nTraining on batch 73 of 392\nTraining on batch 74 of 392\nTraining on batch 75 of 392\nTraining on batch 76 of 392\nTraining on batch 77 of 392\nTraining on batch 78 of 392\nTraining on batch 79 of 392\nTraining on batch 80 of 392\nTraining on batch 81 of 392\nTraining on batch 82 of 392\nTraining on batch 83 of 392\nTraining on batch 84 of 392\nTraining on batch 85 of 392\nTraining on batch 86 of 392\nTraining on batch 87 of 392\nTraining on batch 88 of 392\nTraining on batch 89 of 392\nTraining on batch 90 of 392\nTraining on batch 91 of 392\nTraining on batch 92 of 392\nTraining on batch 93 of 392\nTraining on batch 94 of 392\nTraining on batch 95 of 392\nTraining on batch 96 of 392\nTraining on batch 97 of 392\nTraining on batch 98 of 392\nTraining on batch 99 of 392\nTraining on batch 100 of 392\nTraining on batch 101 of 392\nTraining on batch 102 of 392\nTraining on batch 103 of 392\nTraining on batch 104 of 392\nTraining on batch 105 of 392\nTraining on batch 106 of 392\nTraining on batch 107 of 392\nTraining on batch 108 of 392\nTraining on batch 109 of 392\nTraining on batch 110 of 392\nTraining on batch 111 of 392\nTraining on batch 112 of 392\nTraining on batch 113 of 392\nTraining on batch 114 of 392\nTraining on batch 115 of 392\nTraining on batch 116 of 392\nTraining on batch 117 of 392\nTraining on batch 118 of 392\nTraining on batch 119 of 392\nTraining on batch 120 of 392\nTraining on batch 121 of 392\nTraining on batch 122 of 392\nTraining on batch 123 of 392\nTraining on batch 124 of 392\nTraining on batch 125 of 392\nTraining on batch 126 of 392\nTraining on batch 127 of 392\nTraining on batch 128 of 392\nTraining on batch 129 of 392\nTraining on batch 130 of 392\nTraining on batch 131 of 392\nTraining on batch 132 of 392\nTraining on batch 133 of 392\nTraining on batch 134 of 392\nTraining on batch 135 of 392\nTraining on batch 136 of 392\nTraining on batch 137 of 392\nTraining on batch 138 of 392\nTraining on batch 139 of 392\nTraining on batch 140 of 392\nTraining on batch 141 of 392\nTraining on batch 142 of 392\nTraining on batch 143 of 392\nTraining on batch 144 of 392\nTraining on batch 145 of 392\nTraining on batch 146 of 392\nTraining on batch 147 of 392\nTraining on batch 148 of 392\nTraining on batch 149 of 392\nTraining on batch 150 of 392\nTraining on batch 151 of 392\nTraining on batch 152 of 392\nTraining on batch 153 of 392\nTraining on batch 154 of 392\nTraining on batch 155 of 392\nTraining on batch 156 of 392\nTraining on batch 157 of 392\nTraining on batch 158 of 392\nTraining on batch 159 of 392\nTraining on batch 160 of 392\nTraining on batch 161 of 392\nTraining on batch 162 of 392\nTraining on batch 163 of 392\nTraining on batch 164 of 392\nTraining on batch 165 of 392\nTraining on batch 166 of 392\nTraining on batch 167 of 392\nTraining on batch 168 of 392\nTraining on batch 169 of 392\nTraining on batch 170 of 392\nTraining on batch 171 of 392\nTraining on batch 172 of 392\nTraining on batch 173 of 392\nTraining on batch 174 of 392\nTraining on batch 175 of 392\nTraining on batch 176 of 392\nTraining on batch 177 of 392\nTraining on batch 178 of 392\nTraining on batch 179 of 392\nTraining on batch 180 of 392\nTraining on batch 181 of 392\nTraining on batch 182 of 392\nTraining on batch 183 of 392\nTraining on batch 184 of 392\nTraining on batch 185 of 392\nTraining on batch 186 of 392\nTraining on batch 187 of 392\nTraining on batch 188 of 392\nTraining on batch 189 of 392\nTraining on batch 190 of 392\nTraining on batch 191 of 392\nTraining on batch 192 of 392\nTraining on batch 193 of 392\nTraining on batch 194 of 392\nTraining on batch 195 of 392\nTraining on batch 196 of 392\nTraining on batch 197 of 392\nTraining on batch 198 of 392\nTraining on batch 199 of 392\nTraining on batch 200 of 392\nTraining on batch 201 of 392\nTraining on batch 202 of 392\nTraining on batch 203 of 392\nTraining on batch 204 of 392\nTraining on batch 205 of 392\nTraining on batch 206 of 392\nTraining on batch 207 of 392\nTraining on batch 208 of 392\nTraining on batch 209 of 392\nTraining on batch 210 of 392\nTraining on batch 211 of 392\nTraining on batch 212 of 392\nTraining on batch 213 of 392\nTraining on batch 214 of 392\nTraining on batch 215 of 392\nTraining on batch 216 of 392\nTraining on batch 217 of 392\nTraining on batch 218 of 392\nTraining on batch 219 of 392\nTraining on batch 220 of 392\nTraining on batch 221 of 392\nTraining on batch 222 of 392\nTraining on batch 223 of 392\nTraining on batch 224 of 392\nTraining on batch 225 of 392\nTraining on batch 226 of 392\nTraining on batch 227 of 392\nTraining on batch 228 of 392\nTraining on batch 229 of 392\nTraining on batch 230 of 392\nTraining on batch 231 of 392\nTraining on batch 232 of 392\nTraining on batch 233 of 392\nTraining on batch 234 of 392\nTraining on batch 235 of 392\nTraining on batch 236 of 392\nTraining on batch 237 of 392\nTraining on batch 238 of 392\nTraining on batch 239 of 392\nTraining on batch 240 of 392\nTraining on batch 241 of 392\nTraining on batch 242 of 392\nTraining on batch 243 of 392\nTraining on batch 244 of 392\nTraining on batch 245 of 392\nTraining on batch 246 of 392\nTraining on batch 247 of 392\nTraining on batch 248 of 392\nTraining on batch 249 of 392\nTraining on batch 250 of 392\nTraining on batch 251 of 392\nTraining on batch 252 of 392\nTraining on batch 253 of 392\nTraining on batch 254 of 392\nTraining on batch 255 of 392\nTraining on batch 256 of 392\nTraining on batch 257 of 392\nTraining on batch 258 of 392\nTraining on batch 259 of 392\nTraining on batch 260 of 392\nTraining on batch 261 of 392\nTraining on batch 262 of 392\nTraining on batch 263 of 392\nTraining on batch 264 of 392\nTraining on batch 265 of 392\nTraining on batch 266 of 392\nTraining on batch 267 of 392\nTraining on batch 268 of 392\nTraining on batch 269 of 392\nTraining on batch 270 of 392\nTraining on batch 271 of 392\nTraining on batch 272 of 392\nTraining on batch 273 of 392\nTraining on batch 274 of 392\nTraining on batch 275 of 392\nTraining on batch 276 of 392\nTraining on batch 277 of 392\nTraining on batch 278 of 392\nTraining on batch 279 of 392\nTraining on batch 280 of 392\nTraining on batch 281 of 392\nTraining on batch 282 of 392\nTraining on batch 283 of 392\nTraining on batch 284 of 392\nTraining on batch 285 of 392\nTraining on batch 286 of 392\nTraining on batch 287 of 392\nTraining on batch 288 of 392\nTraining on batch 289 of 392\nTraining on batch 290 of 392\nTraining on batch 291 of 392\nTraining on batch 292 of 392\nTraining on batch 293 of 392\nTraining on batch 294 of 392\nTraining on batch 295 of 392\nTraining on batch 296 of 392\nTraining on batch 297 of 392\nTraining on batch 298 of 392\nTraining on batch 299 of 392\nTraining on batch 300 of 392\nTraining on batch 301 of 392\nTraining on batch 302 of 392\nTraining on batch 303 of 392\nTraining on batch 304 of 392\nTraining on batch 305 of 392\nTraining on batch 306 of 392\nTraining on batch 307 of 392\nTraining on batch 308 of 392\nTraining on batch 309 of 392\nTraining on batch 310 of 392\nTraining on batch 311 of 392\nTraining on batch 312 of 392\nTraining on batch 313 of 392\nTraining on batch 314 of 392\nTraining on batch 315 of 392\nTraining on batch 316 of 392\nTraining on batch 317 of 392\nTraining on batch 318 of 392\nTraining on batch 319 of 392\nTraining on batch 320 of 392\nTraining on batch 321 of 392\nTraining on batch 322 of 392\nTraining on batch 323 of 392\nTraining on batch 324 of 392\nTraining on batch 325 of 392\nTraining on batch 326 of 392\nTraining on batch 327 of 392\nTraining on batch 328 of 392\nTraining on batch 329 of 392\nTraining on batch 330 of 392\nTraining on batch 331 of 392\nTraining on batch 332 of 392\nTraining on batch 333 of 392\nTraining on batch 334 of 392\nTraining on batch 335 of 392\nTraining on batch 336 of 392\nTraining on batch 337 of 392\nTraining on batch 338 of 392\nTraining on batch 339 of 392\nTraining on batch 340 of 392\nTraining on batch 341 of 392\nTraining on batch 342 of 392\nTraining on batch 343 of 392\nTraining on batch 344 of 392\nTraining on batch 345 of 392\nTraining on batch 346 of 392\nTraining on batch 347 of 392\nTraining on batch 348 of 392\nTraining on batch 349 of 392\nTraining on batch 350 of 392\nTraining on batch 351 of 392\nTraining on batch 352 of 392\nTraining on batch 353 of 392\nTraining on batch 354 of 392\nTraining on batch 355 of 392\nTraining on batch 356 of 392\nTraining on batch 357 of 392\nTraining on batch 358 of 392\nTraining on batch 359 of 392\nTraining on batch 360 of 392\nTraining on batch 361 of 392\nTraining on batch 362 of 392\nTraining on batch 363 of 392\nTraining on batch 364 of 392\nTraining on batch 365 of 392\nTraining on batch 366 of 392\nTraining on batch 367 of 392\nTraining on batch 368 of 392\nTraining on batch 369 of 392\nTraining on batch 370 of 392\nTraining on batch 371 of 392\nTraining on batch 372 of 392\nTraining on batch 373 of 392\nTraining on batch 374 of 392\nTraining on batch 375 of 392\nTraining on batch 376 of 392\nTraining on batch 377 of 392\nTraining on batch 378 of 392\nTraining on batch 379 of 392\nTraining on batch 380 of 392\nTraining on batch 381 of 392\nTraining on batch 382 of 392\nTraining on batch 383 of 392\nTraining on batch 384 of 392\nTraining on batch 385 of 392\nTraining on batch 386 of 392\nTraining on batch 387 of 392\nTraining on batch 388 of 392\nTraining on batch 389 of 392\nTraining on batch 390 of 392\nTraining on batch 391 of 392\nTraining on batch 392 of 392\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b945c347c1b549038e0100c3c7b0ad49"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Validating on batch 0 of 49\nValidating on batch 1 of 49\nValidating on batch 2 of 49\nValidating on batch 3 of 49\nValidating on batch 4 of 49\nValidating on batch 5 of 49\nValidating on batch 6 of 49\nValidating on batch 7 of 49\nValidating on batch 8 of 49\nValidating on batch 9 of 49\nValidating on batch 10 of 49\nValidating on batch 11 of 49\nValidating on batch 12 of 49\nValidating on batch 13 of 49\nValidating on batch 14 of 49\nValidating on batch 15 of 49\nValidating on batch 16 of 49\nValidating on batch 17 of 49\nValidating on batch 18 of 49\nValidating on batch 19 of 49\nValidating on batch 20 of 49\nValidating on batch 21 of 49\nValidating on batch 22 of 49\nValidating on batch 23 of 49\nValidating on batch 24 of 49\nValidating on batch 25 of 49\nValidating on batch 26 of 49\nValidating on batch 27 of 49\nValidating on batch 28 of 49\nValidating on batch 29 of 49\nValidating on batch 30 of 49\nValidating on batch 31 of 49\nValidating on batch 32 of 49\nValidating on batch 33 of 49\nValidating on batch 34 of 49\nValidating on batch 35 of 49\nValidating on batch 36 of 49\nValidating on batch 37 of 49\nValidating on batch 38 of 49\nValidating on batch 39 of 49\nValidating on batch 40 of 49\nValidating on batch 41 of 49\nValidating on batch 42 of 49\nValidating on batch 43 of 49\nValidating on batch 44 of 49\nValidating on batch 45 of 49\nValidating on batch 46 of 49\nValidating on batch 47 of 49\nValidating on batch 48 of 49\nValidating on batch 49 of 49\ntraining loss: 0.0013\tvalidation loss: 0.0013\tvalidation accuracy: 0.15\n\n"
    }
   ],
   "source": [
    "Ltr_hist, Lval_hist = fit(net.cuda(), tr_dl, val_dl, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and saving the trained model\n",
    "\n",
    "To save time re-training a model from scratch, the model is saved and can be reloaded without needing retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'model_21052020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(net, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_load_trained_model = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Sequential(\n  (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU()\n  (2): Conv2d(6, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (3): ReLU()\n  (4): ConvTranspose2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (5): ReLU()\n  (6): ConvTranspose2d(6, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (7): Sigmoid()\n)"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "re_load_trained_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "\n",
    "## Motivation for evaluation\n",
    "After the models have been trained it is important to understand how well they perform at the task of predicting a depth map from an RGB image. To do this, a portion of the Moderate Collection data was set aside as a testing dataset. This test dataset contains 786 RGB/Depth image pairs that the models have not been exposed to. These image pairs, therefore, can be used to evaluate how well the trained models perform on unseen data from the same overall dataset.\n",
    "\n",
    "\n",
    "\n",
    "Each RGB image will be passed through the models and a predicted depth map will be produced. This depth map will then be compared to the ground truth depth map associated with it. There are numerous ways in which to compare these depth maps. A simple method could be showing an observer both depth maps and the RGB image and asking them which they think is best and where in the image the predicted depth map lacks clarity. This process takes a large amount of time per depth map, on the order of a minute, and the results generated are qualitative and not quantitative. This can be done for a few images to get a small insight into the appearance of the predicted depth maps. To evaluate all 786 RGB/Depth maps, a quantitative approach is needed.\n",
    "\n",
    "\n",
    "\n",
    "One way of comparing one depth map to another is to compare them pixel by pixel. The difference between predicted depth and ground truth can be found for each pixel and the total difference can be calculated by summing these individual differences. An average difference from one depth map to the other can then be calculated by dividing the total summed differences and dividing by the number of pixels in the depth map. This is known as the mean difference error. A pitfall of this error is that negative differences in depth can cancel out positive differences leading to an unreliable result. To negate this effect, we will take the absolute value of the difference for each comparison. This will ensure that the total difference calculated is positive and that the errors correctly compound. This is known as the MAE-Mean Absolute Error and is defined in the ‘Error metrics’ list under ‘Evaluation procedure’ below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating predictions\n",
    "\n",
    "To perform evaluation, the network must be fed RGB images to generate predicted depth images. These are then used inconjunction with the ground truth depth images for mathematical comparison.\n",
    "\n",
    "The following function: `predict_and_gt` feeds a dataloader to a given network to generate predicted depth maps and to store the associated ground truth counter parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91687cf06c7a4d68b7a0ae152ce4ec3f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-adc678b34639>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mval_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_gts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_and_gt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mre_load_trained_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_gts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_and_gt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mre_load_trained_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Ben\\Documents\\EngD 1st year\\Computer Vision\\Group project\\Github\\depth_estimation\\Evaluation_procedure\\eval_functions.py\u001b[0m in \u001b[0;36mpredict_and_gt\u001b[1;34m(_dl, _val_size, _batch_size, _model)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[0mcount_test_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[0mchunk_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_val_size\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcount_test_batches\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;31m# initialise list of predicted depths\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\bul7cv\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m                 \u001b[1;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\bul7cv\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1105\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1107\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1108\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\bul7cv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\bul7cv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\bul7cv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\bul7cv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\bul7cv\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-d2e09e3b7844>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mcol_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_raw_colour_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mdepth_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_raw_depth_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdepth_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrans_on\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mcol_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_img\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# apply any transforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Ben\\Documents\\EngD 1st year\\Computer Vision\\Group project\\Github\\depth_estimation\\Data Preprocessing\\SimpleCNN model\\Functions.py\u001b[0m in \u001b[0;36mimport_raw_depth_image\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mimport_raw_depth_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# val_preds, val_gts = predict_and_gt(val_dl, val_size, batch_sz, re_load_trained_model)\n",
    "# test_preds, test_gts = predict_and_gt(test_dl, val_size, batch_sz, re_load_trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, the predictions and ground truths are compared by use of another function called `mean_and_std_errors`. This function returns a list of 9 mean errors with associated standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Calculating errors for batch 0 of 49\nCalculating errors for batch 1 of 49\nCalculating errors for batch 2 of 49\nCalculating errors for batch 3 of 49\nCalculating errors for batch 4 of 49\nCalculating errors for batch 5 of 49\nCalculating errors for batch 6 of 49\nCalculating errors for batch 7 of 49\nCalculating errors for batch 8 of 49\nCalculating errors for batch 9 of 49\nCalculating errors for batch 10 of 49\nCalculating errors for batch 11 of 49\nCalculating errors for batch 12 of 49\nCalculating errors for batch 13 of 49\nCalculating errors for batch 14 of 49\nCalculating errors for batch 15 of 49\nCalculating errors for batch 16 of 49\nCalculating errors for batch 17 of 49\nCalculating errors for batch 18 of 49\nCalculating errors for batch 19 of 49\nCalculating errors for batch 20 of 49\nCalculating errors for batch 21 of 49\nCalculating errors for batch 22 of 49\nCalculating errors for batch 23 of 49\nCalculating errors for batch 24 of 49\nCalculating errors for batch 25 of 49\nCalculating errors for batch 26 of 49\nCalculating errors for batch 27 of 49\nCalculating errors for batch 28 of 49\nCalculating errors for batch 29 of 49\nCalculating errors for batch 30 of 49\nCalculating errors for batch 31 of 49\nCalculating errors for batch 32 of 49\nCalculating errors for batch 33 of 49\nCalculating errors for batch 34 of 49\nCalculating errors for batch 35 of 49\nCalculating errors for batch 36 of 49\nCalculating errors for batch 37 of 49\nCalculating errors for batch 38 of 49\nCalculating errors for batch 39 of 49\nCalculating errors for batch 40 of 49\nCalculating errors for batch 41 of 49\nCalculating errors for batch 42 of 49\nCalculating errors for batch 43 of 49\nCalculating errors for batch 44 of 49\nCalculating errors for batch 45 of 49\nCalculating errors for batch 46 of 49\nCalculating errors for batch 47 of 49\nIncrimenting for batch 0 of 48\nIncrimenting for batch 1 of 48\nIncrimenting for batch 2 of 48\nIncrimenting for batch 3 of 48\nIncrimenting for batch 4 of 48\nIncrimenting for batch 5 of 48\nIncrimenting for batch 6 of 48\nIncrimenting for batch 7 of 48\nIncrimenting for batch 8 of 48\nIncrimenting for batch 9 of 48\nIncrimenting for batch 10 of 48\nIncrimenting for batch 11 of 48\nIncrimenting for batch 12 of 48\nIncrimenting for batch 13 of 48\nIncrimenting for batch 14 of 48\nIncrimenting for batch 15 of 48\nIncrimenting for batch 16 of 48\nIncrimenting for batch 17 of 48\nIncrimenting for batch 18 of 48\nIncrimenting for batch 19 of 48\nIncrimenting for batch 20 of 48\nIncrimenting for batch 21 of 48\nIncrimenting for batch 22 of 48\nIncrimenting for batch 23 of 48\nIncrimenting for batch 24 of 48\nIncrimenting for batch 25 of 48\nIncrimenting for batch 26 of 48\nIncrimenting for batch 27 of 48\nIncrimenting for batch 28 of 48\nIncrimenting for batch 29 of 48\nIncrimenting for batch 30 of 48\nIncrimenting for batch 31 of 48\nIncrimenting for batch 32 of 48\nIncrimenting for batch 33 of 48\nIncrimenting for batch 34 of 48\nIncrimenting for batch 35 of 48\nIncrimenting for batch 36 of 48\nIncrimenting for batch 37 of 48\nIncrimenting for batch 38 of 48\nIncrimenting for batch 39 of 48\nIncrimenting for batch 40 of 48\nIncrimenting for batch 41 of 48\nIncrimenting for batch 42 of 48\nIncrimenting for batch 43 of 48\nIncrimenting for batch 44 of 48\nIncrimenting for batch 45 of 48\nIncrimenting for batch 46 of 48\nIncrimenting for batch 47 of 48\nSum sqr for batch 0 of 48\nSum sqr for batch 1 of 48\nSum sqr for batch 2 of 48\nSum sqr for batch 3 of 48\nSum sqr for batch 4 of 48\nSum sqr for batch 5 of 48\nSum sqr for batch 6 of 48\nSum sqr for batch 7 of 48\nSum sqr for batch 8 of 48\nSum sqr for batch 9 of 48\nSum sqr for batch 10 of 48\nSum sqr for batch 11 of 48\nSum sqr for batch 12 of 48\nSum sqr for batch 13 of 48\nSum sqr for batch 14 of 48\nSum sqr for batch 15 of 48\nSum sqr for batch 16 of 48\nSum sqr for batch 17 of 48\nSum sqr for batch 18 of 48\nSum sqr for batch 19 of 48\nSum sqr for batch 20 of 48\nSum sqr for batch 21 of 48\nSum sqr for batch 22 of 48\nSum sqr for batch 23 of 48\nSum sqr for batch 24 of 48\nSum sqr for batch 25 of 48\nSum sqr for batch 26 of 48\nSum sqr for batch 27 of 48\nSum sqr for batch 28 of 48\nSum sqr for batch 29 of 48\nSum sqr for batch 30 of 48\nSum sqr for batch 31 of 48\nSum sqr for batch 32 of 48\nSum sqr for batch 33 of 48\nSum sqr for batch 34 of 48\nSum sqr for batch 35 of 48\nSum sqr for batch 36 of 48\nSum sqr for batch 37 of 48\nSum sqr for batch 38 of 48\nSum sqr for batch 39 of 48\nSum sqr for batch 40 of 48\nSum sqr for batch 41 of 48\nSum sqr for batch 42 of 48\nSum sqr for batch 43 of 48\nSum sqr for batch 44 of 48\nSum sqr for batch 45 of 48\nSum sqr for batch 46 of 48\nSum sqr for batch 47 of 48\n"
    }
   ],
   "source": [
    "val_means, val_stds = mean_and_std_errors(val_preds, val_gts, val_size, batch_sz)\n",
    "# test_means, test_stds = mean_and_std_errors(test_preds, test_gts, val_size, batch_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0.15292886050578838,\n 0.20256076692918262,\n 169.7575138927575,\n 337.4968641438327,\n 1.3821823514353018,\n 1.6306671019519026,\n 1.0416930808944187,\n 14.377977729056317,\n 35337.66780938274]"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "test_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0.1523476418379662,\n 0.20036514733386704,\n 206.6017020603569,\n 403.54414631860675,\n 1.4401937428260463,\n 1.6913869644066872,\n 1.0672831635531381,\n 15.333102927912666,\n 12584.9235118754]"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "val_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0.08957746447381942,\n 0.11092838123774677,\n 535.9647437798461,\n 1322.1912593962645,\n 0.6154654399257186,\n 0.7207509679744946,\n 0.5480206815117135,\n 68.6091373080213,\n 678769.5709696717]"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "test_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0.09106520633516951,\n 0.1122705230413874,\n 742.6923745083199,\n 1464.9612830367291,\n 0.6856167844947316,\n 0.788101654134482,\n 0.5521365983354526,\n 50.39952812354277,\n 116227.66628677824]"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "val_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_errors = [\"standard difference\" , \"square_difference\" , \"inverse difference\" , \"inverse squared\" , \"logarithm difference\" , \"log square difference\" , \"scale invariant log\" , \"absolute relavitve\" , \"square relative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "standard difference \t\t 0.1523 \t+-\t 0.09107\nsquare_difference \t\t 0.2004 \t+-\t 0.1123\ninverse difference \t\t 206.6 \t+-\t 742.7\ninverse squared \t\t 403.5 \t+-\t 1465\nlogarithm difference \t\t 1.44 \t+-\t 0.6856\nlog square difference \t\t 1.691 \t+-\t 0.7881\nscale invariant log \t\t 1.067 \t+-\t 0.5521\nabsolute relavitve \t\t 15.33 \t+-\t 50.4\nsquare relative \t\t 1.258e+04 \t+-\t 1.162e+05\n"
    }
   ],
   "source": [
    "for i, j, k in zip(val_means, list_of_errors, val_stds):\n",
    "    print(f\"{j} \\t\\t\", '{:.4g}'.format(i), \"\\t+-\\t\", '{:.4g}'.format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_saved = 'C:/Users/Ben/OneDrive - Bournemouth University/Computer Vision/Datasets/Saved_preds/val_means.pckl'\n",
    "# f = open(path_saved, 'wb')\n",
    "# pickle.dump(val_means, f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating and storing predictions takes a long time, instead of running these functions from scratch, we will load in calculated error means and standard deviations that have been computed earlier. To calculate these for yourself, uncomment the two blocks of code above and run them (this takes on the order of one hour)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_stat_list = [\"val_means\", \"val_stds\", \"test_means\", \"test_stds\"]\n",
    "error_dictionary = {}\n",
    "for name in saved_stat_list:\n",
    "    error_dictionary[f\"{name}\"] = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in saved_stat_list:\n",
    "    path_saved = f'C:/Users/Ben/OneDrive - Bournemouth University/Computer Vision/Datasets/Saved_preds/{name}.pckl'\n",
    "    f = open(path_saved, 'rb')\n",
    "    error_dictionary[f\"{name}\"] = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How metrics are calculated\n",
    "\n",
    "A function within `mean_and_std_errors` has been written called calc_errors and it accepts two depth maps: predicted and ground truth.\n",
    "\n",
    "- First it checks that the sizes of the compared depth maps are equal, and if true it continues\n",
    "- Then there are two loops, one over each pixel in the height of the image and the other over each pixel in the width\n",
    "- This will form a full loop over all of the pixels in the image\n",
    "- For each pixel the depth in the predicted and ground truth image are found at their given location\n",
    "- Next, a validity check is performed to check that neither of the depth values are negative (negative depth is not within the range of accepted values of the Moderate Dataset and negative values are assigned to invalid depths in the KITTI dataset)\n",
    "- For each pixel position (u,v) in the image, an array of differences is calculated\n",
    "- These are stored and then summed over the whole image in order to calculate the error metrics\n",
    "- While this is happening, a counter of valid pixels is incremented, this counts how many non-negative pixel comparisons are made during the loops\n",
    "- After all pixels are looped over and the error sum variables are calculated for the whole image the error sum variables are normalised\n",
    "- Normalisation involves dividing the errors by the number of valid pixels\n",
    "- After normalisation, a list of the nine error metrics is returned\n",
    "\n",
    "### Averaging over an entire dataset\n",
    "\n",
    "The process outlined above will calculate 9 error metrics describing the difference between single predicted and ground truth depth images.\n",
    "As mentioned previously, the testing dataset from Moderate Collection contains 786 RBG and depth images.\n",
    "As such, errors for all 786 images will be calculated and their mean and standard deviation will then be determined.\n",
    "\n",
    "This will then give a list of 9 mean errors and 9 associated standard deviations in those errors for the entire testing dataset.\n",
    "The motivation for this is to calculate metrics representative of an entire testing dataset and with that, have uncertainty values allowing for statistical comparison between any other testing set.\n",
    "This is especially important for comparing our performance on the Moderate collection testing set and the KITTI depth benchmark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saved predictions and ground truths can be retrieved instead of calculating predictions from scratch with the following functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for re-loading saved variables\n",
    "# # f = open('test_gts.pckl', 'wb')\n",
    "# # pickle.dump(test_gts, f)\n",
    "# # f.close()\n",
    "\n",
    "path_saved = 'C:/Users/Ben/OneDrive - Bournemouth University/Computer Vision/Datasets/Saved_preds/test_gts.pckl'\n",
    "f = open(path_saved, 'rb')\n",
    "test_gts = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_saved = 'C:/Users/Ben/OneDrive - Bournemouth University/Computer Vision/Datasets/Saved_preds/test_preds.pckl'\n",
    "f = open(path_saved, 'rb')\n",
    "test_preds = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_saved = 'C:/Users/Ben/OneDrive - Bournemouth University/Computer Vision/Datasets/Saved_preds/val_preds.pckl'\n",
    "f = open(path_saved, 'rb')\n",
    "val_preds = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_saved = 'C:/Users/Ben/OneDrive - Bournemouth University/Computer Vision/Datasets/Saved_preds/val_gts.pckl'\n",
    "f = open(path_saved, 'rb')\n",
    "val_gts = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train dataloader is called tr_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Whirrrrrr calculating..... 0 of 392.8125\nWhirrrrrr calculating..... 1 of 392.8125\nWhirrrrrr calculating..... 2 of 392.8125\nWhirrrrrr calculating..... 3 of 392.8125\nWhirrrrrr calculating..... 4 of 392.8125\nWhirrrrrr calculating..... 5 of 392.8125\nWhirrrrrr calculating..... 6 of 392.8125\nWhirrrrrr calculating..... 7 of 392.8125\nWhirrrrrr calculating..... 8 of 392.8125\nWhirrrrrr calculating..... 9 of 392.8125\nWhirrrrrr calculating..... 10 of 392.8125\nWhirrrrrr calculating..... 11 of 392.8125\nWhirrrrrr calculating..... 12 of 392.8125\nWhirrrrrr calculating..... 13 of 392.8125\nWhirrrrrr calculating..... 14 of 392.8125\nWhirrrrrr calculating..... 15 of 392.8125\nWhirrrrrr calculating..... 16 of 392.8125\nWhirrrrrr calculating..... 17 of 392.8125\nWhirrrrrr calculating..... 18 of 392.8125\nWhirrrrrr calculating..... 19 of 392.8125\nWhirrrrrr calculating..... 20 of 392.8125\nWhirrrrrr calculating..... 21 of 392.8125\nWhirrrrrr calculating..... 22 of 392.8125\nWhirrrrrr calculating..... 23 of 392.8125\nWhirrrrrr calculating..... 24 of 392.8125\nWhirrrrrr calculating..... 25 of 392.8125\nWhirrrrrr calculating..... 26 of 392.8125\nWhirrrrrr calculating..... 27 of 392.8125\nWhirrrrrr calculating..... 28 of 392.8125\nWhirrrrrr calculating..... 29 of 392.8125\nWhirrrrrr calculating..... 30 of 392.8125\nWhirrrrrr calculating..... 31 of 392.8125\nWhirrrrrr calculating..... 32 of 392.8125\nWhirrrrrr calculating..... 33 of 392.8125\nWhirrrrrr calculating..... 34 of 392.8125\nWhirrrrrr calculating..... 35 of 392.8125\nWhirrrrrr calculating..... 36 of 392.8125\nWhirrrrrr calculating..... 37 of 392.8125\nWhirrrrrr calculating..... 38 of 392.8125\nWhirrrrrr calculating..... 39 of 392.8125\nWhirrrrrr calculating..... 40 of 392.8125\nWhirrrrrr calculating..... 41 of 392.8125\nWhirrrrrr calculating..... 42 of 392.8125\nWhirrrrrr calculating..... 43 of 392.8125\nWhirrrrrr calculating..... 44 of 392.8125\nWhirrrrrr calculating..... 45 of 392.8125\nWhirrrrrr calculating..... 46 of 392.8125\nWhirrrrrr calculating..... 47 of 392.8125\nWhirrrrrr calculating..... 48 of 392.8125\nWhirrrrrr calculating..... 49 of 392.8125\nWhirrrrrr calculating..... 50 of 392.8125\nWhirrrrrr calculating..... 51 of 392.8125\nWhirrrrrr calculating..... 52 of 392.8125\nWhirrrrrr calculating..... 53 of 392.8125\nWhirrrrrr calculating..... 54 of 392.8125\nWhirrrrrr calculating..... 55 of 392.8125\nWhirrrrrr calculating..... 56 of 392.8125\nWhirrrrrr calculating..... 57 of 392.8125\nWhirrrrrr calculating..... 58 of 392.8125\nWhirrrrrr calculating..... 59 of 392.8125\nWhirrrrrr calculating..... 60 of 392.8125\nWhirrrrrr calculating..... 61 of 392.8125\nWhirrrrrr calculating..... 62 of 392.8125\nWhirrrrrr calculating..... 63 of 392.8125\nWhirrrrrr calculating..... 64 of 392.8125\nWhirrrrrr calculating..... 65 of 392.8125\nWhirrrrrr calculating..... 66 of 392.8125\nWhirrrrrr calculating..... 67 of 392.8125\nWhirrrrrr calculating..... 68 of 392.8125\nWhirrrrrr calculating..... 69 of 392.8125\nWhirrrrrr calculating..... 70 of 392.8125\nWhirrrrrr calculating..... 71 of 392.8125\nWhirrrrrr calculating..... 72 of 392.8125\nWhirrrrrr calculating..... 73 of 392.8125\nWhirrrrrr calculating..... 74 of 392.8125\nWhirrrrrr calculating..... 75 of 392.8125\nWhirrrrrr calculating..... 76 of 392.8125\nWhirrrrrr calculating..... 77 of 392.8125\nWhirrrrrr calculating..... 78 of 392.8125\nWhirrrrrr calculating..... 79 of 392.8125\nWhirrrrrr calculating..... 80 of 392.8125\nWhirrrrrr calculating..... 81 of 392.8125\nWhirrrrrr calculating..... 82 of 392.8125\nWhirrrrrr calculating..... 83 of 392.8125\nWhirrrrrr calculating..... 84 of 392.8125\nWhirrrrrr calculating..... 85 of 392.8125\nWhirrrrrr calculating..... 86 of 392.8125\nWhirrrrrr calculating..... 87 of 392.8125\nWhirrrrrr calculating..... 88 of 392.8125\nWhirrrrrr calculating..... 89 of 392.8125\nWhirrrrrr calculating..... 90 of 392.8125\nWhirrrrrr calculating..... 91 of 392.8125\nWhirrrrrr calculating..... 92 of 392.8125\nWhirrrrrr calculating..... 93 of 392.8125\nWhirrrrrr calculating..... 94 of 392.8125\nWhirrrrrr calculating..... 95 of 392.8125\nWhirrrrrr calculating..... 96 of 392.8125\nWhirrrrrr calculating..... 97 of 392.8125\nWhirrrrrr calculating..... 98 of 392.8125\nWhirrrrrr calculating..... 99 of 392.8125\nWhirrrrrr calculating..... 100 of 392.8125\nWhirrrrrr calculating..... 101 of 392.8125\nWhirrrrrr calculating..... 102 of 392.8125\nWhirrrrrr calculating..... 103 of 392.8125\nWhirrrrrr calculating..... 104 of 392.8125\nWhirrrrrr calculating..... 105 of 392.8125\nWhirrrrrr calculating..... 106 of 392.8125\nWhirrrrrr calculating..... 107 of 392.8125\nWhirrrrrr calculating..... 108 of 392.8125\nWhirrrrrr calculating..... 109 of 392.8125\nWhirrrrrr calculating..... 110 of 392.8125\nWhirrrrrr calculating..... 111 of 392.8125\nWhirrrrrr calculating..... 112 of 392.8125\nWhirrrrrr calculating..... 113 of 392.8125\nWhirrrrrr calculating..... 114 of 392.8125\nWhirrrrrr calculating..... 115 of 392.8125\nWhirrrrrr calculating..... 116 of 392.8125\nWhirrrrrr calculating..... 117 of 392.8125\nWhirrrrrr calculating..... 118 of 392.8125\nWhirrrrrr calculating..... 119 of 392.8125\nWhirrrrrr calculating..... 120 of 392.8125\nWhirrrrrr calculating..... 121 of 392.8125\nWhirrrrrr calculating..... 122 of 392.8125\nWhirrrrrr calculating..... 123 of 392.8125\nWhirrrrrr calculating..... 124 of 392.8125\nWhirrrrrr calculating..... 125 of 392.8125\nWhirrrrrr calculating..... 126 of 392.8125\nWhirrrrrr calculating..... 127 of 392.8125\nWhirrrrrr calculating..... 128 of 392.8125\nWhirrrrrr calculating..... 129 of 392.8125\nWhirrrrrr calculating..... 130 of 392.8125\nWhirrrrrr calculating..... 131 of 392.8125\nWhirrrrrr calculating..... 132 of 392.8125\nWhirrrrrr calculating..... 133 of 392.8125\nWhirrrrrr calculating..... 134 of 392.8125\nWhirrrrrr calculating..... 135 of 392.8125\nWhirrrrrr calculating..... 136 of 392.8125\nWhirrrrrr calculating..... 137 of 392.8125\nWhirrrrrr calculating..... 138 of 392.8125\nWhirrrrrr calculating..... 139 of 392.8125\nWhirrrrrr calculating..... 140 of 392.8125\nWhirrrrrr calculating..... 141 of 392.8125\nWhirrrrrr calculating..... 142 of 392.8125\nWhirrrrrr calculating..... 143 of 392.8125\nWhirrrrrr calculating..... 144 of 392.8125\nWhirrrrrr calculating..... 145 of 392.8125\nWhirrrrrr calculating..... 146 of 392.8125\nWhirrrrrr calculating..... 147 of 392.8125\nWhirrrrrr calculating..... 148 of 392.8125\nWhirrrrrr calculating..... 149 of 392.8125\nWhirrrrrr calculating..... 150 of 392.8125\nWhirrrrrr calculating..... 151 of 392.8125\nWhirrrrrr calculating..... 152 of 392.8125\nWhirrrrrr calculating..... 153 of 392.8125\nWhirrrrrr calculating..... 154 of 392.8125\nWhirrrrrr calculating..... 155 of 392.8125\nWhirrrrrr calculating..... 156 of 392.8125\nWhirrrrrr calculating..... 157 of 392.8125\nWhirrrrrr calculating..... 158 of 392.8125\nWhirrrrrr calculating..... 159 of 392.8125\nWhirrrrrr calculating..... 160 of 392.8125\nWhirrrrrr calculating..... 161 of 392.8125\nWhirrrrrr calculating..... 162 of 392.8125\nWhirrrrrr calculating..... 163 of 392.8125\nWhirrrrrr calculating..... 164 of 392.8125\nWhirrrrrr calculating..... 165 of 392.8125\nWhirrrrrr calculating..... 166 of 392.8125\nWhirrrrrr calculating..... 167 of 392.8125\nWhirrrrrr calculating..... 168 of 392.8125\nWhirrrrrr calculating..... 169 of 392.8125\nWhirrrrrr calculating..... 170 of 392.8125\nWhirrrrrr calculating..... 171 of 392.8125\nWhirrrrrr calculating..... 172 of 392.8125\nWhirrrrrr calculating..... 173 of 392.8125\nWhirrrrrr calculating..... 174 of 392.8125\nWhirrrrrr calculating..... 175 of 392.8125\nWhirrrrrr calculating..... 176 of 392.8125\nWhirrrrrr calculating..... 177 of 392.8125\nWhirrrrrr calculating..... 178 of 392.8125\nWhirrrrrr calculating..... 179 of 392.8125\nWhirrrrrr calculating..... 180 of 392.8125\nWhirrrrrr calculating..... 181 of 392.8125\nWhirrrrrr calculating..... 182 of 392.8125\nWhirrrrrr calculating..... 183 of 392.8125\nWhirrrrrr calculating..... 184 of 392.8125\nWhirrrrrr calculating..... 185 of 392.8125\nWhirrrrrr calculating..... 186 of 392.8125\nWhirrrrrr calculating..... 187 of 392.8125\nWhirrrrrr calculating..... 188 of 392.8125\nWhirrrrrr calculating..... 189 of 392.8125\nWhirrrrrr calculating..... 190 of 392.8125\nWhirrrrrr calculating..... 191 of 392.8125\nWhirrrrrr calculating..... 192 of 392.8125\nWhirrrrrr calculating..... 193 of 392.8125\nWhirrrrrr calculating..... 194 of 392.8125\nWhirrrrrr calculating..... 195 of 392.8125\nWhirrrrrr calculating..... 196 of 392.8125\nWhirrrrrr calculating..... 197 of 392.8125\nWhirrrrrr calculating..... 198 of 392.8125\nWhirrrrrr calculating..... 199 of 392.8125\nWhirrrrrr calculating..... 200 of 392.8125\nWhirrrrrr calculating..... 201 of 392.8125\nWhirrrrrr calculating..... 202 of 392.8125\nWhirrrrrr calculating..... 203 of 392.8125\nWhirrrrrr calculating..... 204 of 392.8125\nWhirrrrrr calculating..... 205 of 392.8125\nWhirrrrrr calculating..... 206 of 392.8125\nWhirrrrrr calculating..... 207 of 392.8125\nWhirrrrrr calculating..... 208 of 392.8125\nWhirrrrrr calculating..... 209 of 392.8125\nWhirrrrrr calculating..... 210 of 392.8125\nWhirrrrrr calculating..... 211 of 392.8125\nWhirrrrrr calculating..... 212 of 392.8125\nWhirrrrrr calculating..... 213 of 392.8125\nWhirrrrrr calculating..... 214 of 392.8125\nWhirrrrrr calculating..... 215 of 392.8125\nWhirrrrrr calculating..... 216 of 392.8125\nWhirrrrrr calculating..... 217 of 392.8125\nWhirrrrrr calculating..... 218 of 392.8125\nWhirrrrrr calculating..... 219 of 392.8125\nWhirrrrrr calculating..... 220 of 392.8125\nWhirrrrrr calculating..... 221 of 392.8125\nWhirrrrrr calculating..... 222 of 392.8125\nWhirrrrrr calculating..... 223 of 392.8125\nWhirrrrrr calculating..... 224 of 392.8125\nWhirrrrrr calculating..... 225 of 392.8125\nWhirrrrrr calculating..... 226 of 392.8125\nWhirrrrrr calculating..... 227 of 392.8125\nWhirrrrrr calculating..... 228 of 392.8125\nWhirrrrrr calculating..... 229 of 392.8125\nWhirrrrrr calculating..... 230 of 392.8125\nWhirrrrrr calculating..... 231 of 392.8125\nWhirrrrrr calculating..... 232 of 392.8125\nWhirrrrrr calculating..... 233 of 392.8125\nWhirrrrrr calculating..... 234 of 392.8125\nWhirrrrrr calculating..... 235 of 392.8125\nWhirrrrrr calculating..... 236 of 392.8125\nWhirrrrrr calculating..... 237 of 392.8125\nWhirrrrrr calculating..... 238 of 392.8125\nWhirrrrrr calculating..... 239 of 392.8125\nWhirrrrrr calculating..... 240 of 392.8125\nWhirrrrrr calculating..... 241 of 392.8125\nWhirrrrrr calculating..... 242 of 392.8125\nWhirrrrrr calculating..... 243 of 392.8125\nWhirrrrrr calculating..... 244 of 392.8125\nWhirrrrrr calculating..... 245 of 392.8125\nWhirrrrrr calculating..... 246 of 392.8125\nWhirrrrrr calculating..... 247 of 392.8125\nWhirrrrrr calculating..... 248 of 392.8125\nWhirrrrrr calculating..... 249 of 392.8125\nWhirrrrrr calculating..... 250 of 392.8125\nWhirrrrrr calculating..... 251 of 392.8125\nWhirrrrrr calculating..... 252 of 392.8125\nWhirrrrrr calculating..... 253 of 392.8125\nWhirrrrrr calculating..... 254 of 392.8125\nWhirrrrrr calculating..... 255 of 392.8125\nWhirrrrrr calculating..... 256 of 392.8125\nWhirrrrrr calculating..... 257 of 392.8125\nWhirrrrrr calculating..... 258 of 392.8125\nWhirrrrrr calculating..... 259 of 392.8125\nWhirrrrrr calculating..... 260 of 392.8125\nWhirrrrrr calculating..... 261 of 392.8125\nWhirrrrrr calculating..... 262 of 392.8125\nWhirrrrrr calculating..... 263 of 392.8125\nWhirrrrrr calculating..... 264 of 392.8125\nWhirrrrrr calculating..... 265 of 392.8125\nWhirrrrrr calculating..... 266 of 392.8125\nWhirrrrrr calculating..... 267 of 392.8125\nWhirrrrrr calculating..... 268 of 392.8125\nWhirrrrrr calculating..... 269 of 392.8125\nWhirrrrrr calculating..... 270 of 392.8125\nWhirrrrrr calculating..... 271 of 392.8125\nWhirrrrrr calculating..... 272 of 392.8125\nWhirrrrrr calculating..... 273 of 392.8125\nWhirrrrrr calculating..... 274 of 392.8125\nWhirrrrrr calculating..... 275 of 392.8125\nWhirrrrrr calculating..... 276 of 392.8125\nWhirrrrrr calculating..... 277 of 392.8125\nWhirrrrrr calculating..... 278 of 392.8125\nWhirrrrrr calculating..... 279 of 392.8125\nWhirrrrrr calculating..... 280 of 392.8125\nWhirrrrrr calculating..... 281 of 392.8125\nWhirrrrrr calculating..... 282 of 392.8125\nWhirrrrrr calculating..... 283 of 392.8125\nWhirrrrrr calculating..... 284 of 392.8125\nWhirrrrrr calculating..... 285 of 392.8125\nWhirrrrrr calculating..... 286 of 392.8125\nWhirrrrrr calculating..... 287 of 392.8125\nWhirrrrrr calculating..... 288 of 392.8125\nWhirrrrrr calculating..... 289 of 392.8125\nWhirrrrrr calculating..... 290 of 392.8125\nWhirrrrrr calculating..... 291 of 392.8125\nWhirrrrrr calculating..... 292 of 392.8125\nWhirrrrrr calculating..... 293 of 392.8125\nWhirrrrrr calculating..... 294 of 392.8125\nWhirrrrrr calculating..... 295 of 392.8125\nWhirrrrrr calculating..... 296 of 392.8125\nWhirrrrrr calculating..... 297 of 392.8125\nWhirrrrrr calculating..... 298 of 392.8125\nWhirrrrrr calculating..... 299 of 392.8125\nWhirrrrrr calculating..... 300 of 392.8125\nWhirrrrrr calculating..... 301 of 392.8125\nWhirrrrrr calculating..... 302 of 392.8125\nWhirrrrrr calculating..... 303 of 392.8125\nWhirrrrrr calculating..... 304 of 392.8125\nWhirrrrrr calculating..... 305 of 392.8125\nWhirrrrrr calculating..... 306 of 392.8125\nWhirrrrrr calculating..... 307 of 392.8125\nWhirrrrrr calculating..... 308 of 392.8125\nWhirrrrrr calculating..... 309 of 392.8125\nWhirrrrrr calculating..... 310 of 392.8125\nWhirrrrrr calculating..... 311 of 392.8125\nWhirrrrrr calculating..... 312 of 392.8125\nWhirrrrrr calculating..... 313 of 392.8125\nWhirrrrrr calculating..... 314 of 392.8125\nWhirrrrrr calculating..... 315 of 392.8125\nWhirrrrrr calculating..... 316 of 392.8125\nWhirrrrrr calculating..... 317 of 392.8125\nWhirrrrrr calculating..... 318 of 392.8125\nWhirrrrrr calculating..... 319 of 392.8125\nWhirrrrrr calculating..... 320 of 392.8125\nWhirrrrrr calculating..... 321 of 392.8125\nWhirrrrrr calculating..... 322 of 392.8125\nWhirrrrrr calculating..... 323 of 392.8125\nWhirrrrrr calculating..... 324 of 392.8125\nWhirrrrrr calculating..... 325 of 392.8125\nWhirrrrrr calculating..... 326 of 392.8125\nWhirrrrrr calculating..... 327 of 392.8125\nWhirrrrrr calculating..... 328 of 392.8125\nWhirrrrrr calculating..... 329 of 392.8125\nWhirrrrrr calculating..... 330 of 392.8125\nWhirrrrrr calculating..... 331 of 392.8125\nWhirrrrrr calculating..... 332 of 392.8125\nWhirrrrrr calculating..... 333 of 392.8125\nWhirrrrrr calculating..... 334 of 392.8125\nWhirrrrrr calculating..... 335 of 392.8125\nWhirrrrrr calculating..... 336 of 392.8125\nWhirrrrrr calculating..... 337 of 392.8125\nWhirrrrrr calculating..... 338 of 392.8125\nWhirrrrrr calculating..... 339 of 392.8125\nWhirrrrrr calculating..... 340 of 392.8125\nWhirrrrrr calculating..... 341 of 392.8125\nWhirrrrrr calculating..... 342 of 392.8125\nWhirrrrrr calculating..... 343 of 392.8125\nWhirrrrrr calculating..... 344 of 392.8125\nWhirrrrrr calculating..... 345 of 392.8125\nWhirrrrrr calculating..... 346 of 392.8125\nWhirrrrrr calculating..... 347 of 392.8125\nWhirrrrrr calculating..... 348 of 392.8125\nWhirrrrrr calculating..... 349 of 392.8125\nWhirrrrrr calculating..... 350 of 392.8125\nWhirrrrrr calculating..... 351 of 392.8125\nWhirrrrrr calculating..... 352 of 392.8125\nWhirrrrrr calculating..... 353 of 392.8125\nWhirrrrrr calculating..... 354 of 392.8125\nWhirrrrrr calculating..... 355 of 392.8125\nWhirrrrrr calculating..... 356 of 392.8125\nWhirrrrrr calculating..... 357 of 392.8125\nWhirrrrrr calculating..... 358 of 392.8125\nWhirrrrrr calculating..... 359 of 392.8125\nWhirrrrrr calculating..... 360 of 392.8125\nWhirrrrrr calculating..... 361 of 392.8125\nWhirrrrrr calculating..... 362 of 392.8125\nWhirrrrrr calculating..... 363 of 392.8125\nWhirrrrrr calculating..... 364 of 392.8125\nWhirrrrrr calculating..... 365 of 392.8125\nWhirrrrrr calculating..... 366 of 392.8125\nWhirrrrrr calculating..... 367 of 392.8125\nWhirrrrrr calculating..... 368 of 392.8125\nWhirrrrrr calculating..... 369 of 392.8125\nWhirrrrrr calculating..... 370 of 392.8125\nWhirrrrrr calculating..... 371 of 392.8125\nWhirrrrrr calculating..... 372 of 392.8125\nWhirrrrrr calculating..... 373 of 392.8125\nWhirrrrrr calculating..... 374 of 392.8125\nWhirrrrrr calculating..... 375 of 392.8125\nWhirrrrrr calculating..... 376 of 392.8125\nWhirrrrrr calculating..... 377 of 392.8125\nWhirrrrrr calculating..... 378 of 392.8125\nWhirrrrrr calculating..... 379 of 392.8125\nWhirrrrrr calculating..... 380 of 392.8125\nWhirrrrrr calculating..... 381 of 392.8125\nWhirrrrrr calculating..... 382 of 392.8125\nWhirrrrrr calculating..... 383 of 392.8125\nWhirrrrrr calculating..... 384 of 392.8125\nWhirrrrrr calculating..... 385 of 392.8125\nWhirrrrrr calculating..... 386 of 392.8125\nWhirrrrrr calculating..... 387 of 392.8125\nWhirrrrrr calculating..... 388 of 392.8125\nWhirrrrrr calculating..... 389 of 392.8125\nWhirrrrrr calculating..... 390 of 392.8125\nWhirrrrrr calculating..... 391 of 392.8125\nWhirrrrrr calculating..... 392 of 392.8125\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'count_var' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-a6474dff2bc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mcount_var\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtr_dl\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mbatch_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'count_var' is not defined"
     ]
    }
   ],
   "source": [
    "mean = 0.0\n",
    "count_mn = 0\n",
    "for images, _ in tr_dl:\n",
    "    images = images.float()\n",
    "    batch_samples = images.size(0) \n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    print(f\"Whirrrrrr calculating..... {count_mn} of {train_size/batch_sz}\")\n",
    "    count_mn+= 1\n",
    "mean = mean / len(tr_dl.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([ 98.2207, 101.6702, 102.9898])"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Whirrrrrr calculating..... 0 of 392.8125\nWhirrrrrr calculating..... 1 of 392.8125\nWhirrrrrr calculating..... 2 of 392.8125\nWhirrrrrr calculating..... 3 of 392.8125\nWhirrrrrr calculating..... 4 of 392.8125\nWhirrrrrr calculating..... 5 of 392.8125\nWhirrrrrr calculating..... 6 of 392.8125\nWhirrrrrr calculating..... 7 of 392.8125\nWhirrrrrr calculating..... 8 of 392.8125\nWhirrrrrr calculating..... 9 of 392.8125\nWhirrrrrr calculating..... 10 of 392.8125\nWhirrrrrr calculating..... 11 of 392.8125\nWhirrrrrr calculating..... 12 of 392.8125\nWhirrrrrr calculating..... 13 of 392.8125\nWhirrrrrr calculating..... 14 of 392.8125\nWhirrrrrr calculating..... 15 of 392.8125\nWhirrrrrr calculating..... 16 of 392.8125\nWhirrrrrr calculating..... 17 of 392.8125\nWhirrrrrr calculating..... 18 of 392.8125\nWhirrrrrr calculating..... 19 of 392.8125\nWhirrrrrr calculating..... 20 of 392.8125\nWhirrrrrr calculating..... 21 of 392.8125\nWhirrrrrr calculating..... 22 of 392.8125\nWhirrrrrr calculating..... 23 of 392.8125\nWhirrrrrr calculating..... 24 of 392.8125\nWhirrrrrr calculating..... 25 of 392.8125\nWhirrrrrr calculating..... 26 of 392.8125\nWhirrrrrr calculating..... 27 of 392.8125\nWhirrrrrr calculating..... 28 of 392.8125\nWhirrrrrr calculating..... 29 of 392.8125\nWhirrrrrr calculating..... 30 of 392.8125\nWhirrrrrr calculating..... 31 of 392.8125\nWhirrrrrr calculating..... 32 of 392.8125\nWhirrrrrr calculating..... 33 of 392.8125\nWhirrrrrr calculating..... 34 of 392.8125\nWhirrrrrr calculating..... 35 of 392.8125\nWhirrrrrr calculating..... 36 of 392.8125\nWhirrrrrr calculating..... 37 of 392.8125\nWhirrrrrr calculating..... 38 of 392.8125\nWhirrrrrr calculating..... 39 of 392.8125\nWhirrrrrr calculating..... 40 of 392.8125\nWhirrrrrr calculating..... 41 of 392.8125\nWhirrrrrr calculating..... 42 of 392.8125\nWhirrrrrr calculating..... 43 of 392.8125\nWhirrrrrr calculating..... 44 of 392.8125\nWhirrrrrr calculating..... 45 of 392.8125\nWhirrrrrr calculating..... 46 of 392.8125\nWhirrrrrr calculating..... 47 of 392.8125\nWhirrrrrr calculating..... 48 of 392.8125\nWhirrrrrr calculating..... 49 of 392.8125\nWhirrrrrr calculating..... 50 of 392.8125\nWhirrrrrr calculating..... 51 of 392.8125\nWhirrrrrr calculating..... 52 of 392.8125\nWhirrrrrr calculating..... 53 of 392.8125\nWhirrrrrr calculating..... 54 of 392.8125\nWhirrrrrr calculating..... 55 of 392.8125\nWhirrrrrr calculating..... 56 of 392.8125\nWhirrrrrr calculating..... 57 of 392.8125\nWhirrrrrr calculating..... 58 of 392.8125\nWhirrrrrr calculating..... 59 of 392.8125\nWhirrrrrr calculating..... 60 of 392.8125\nWhirrrrrr calculating..... 61 of 392.8125\nWhirrrrrr calculating..... 62 of 392.8125\nWhirrrrrr calculating..... 63 of 392.8125\nWhirrrrrr calculating..... 64 of 392.8125\nWhirrrrrr calculating..... 65 of 392.8125\nWhirrrrrr calculating..... 66 of 392.8125\nWhirrrrrr calculating..... 67 of 392.8125\nWhirrrrrr calculating..... 68 of 392.8125\nWhirrrrrr calculating..... 69 of 392.8125\nWhirrrrrr calculating..... 70 of 392.8125\nWhirrrrrr calculating..... 71 of 392.8125\nWhirrrrrr calculating..... 72 of 392.8125\nWhirrrrrr calculating..... 73 of 392.8125\nWhirrrrrr calculating..... 74 of 392.8125\nWhirrrrrr calculating..... 75 of 392.8125\nWhirrrrrr calculating..... 76 of 392.8125\nWhirrrrrr calculating..... 77 of 392.8125\nWhirrrrrr calculating..... 78 of 392.8125\nWhirrrrrr calculating..... 79 of 392.8125\nWhirrrrrr calculating..... 80 of 392.8125\nWhirrrrrr calculating..... 81 of 392.8125\nWhirrrrrr calculating..... 82 of 392.8125\nWhirrrrrr calculating..... 83 of 392.8125\nWhirrrrrr calculating..... 84 of 392.8125\nWhirrrrrr calculating..... 85 of 392.8125\nWhirrrrrr calculating..... 86 of 392.8125\nWhirrrrrr calculating..... 87 of 392.8125\nWhirrrrrr calculating..... 88 of 392.8125\nWhirrrrrr calculating..... 89 of 392.8125\nWhirrrrrr calculating..... 90 of 392.8125\nWhirrrrrr calculating..... 91 of 392.8125\nWhirrrrrr calculating..... 92 of 392.8125\nWhirrrrrr calculating..... 93 of 392.8125\nWhirrrrrr calculating..... 94 of 392.8125\nWhirrrrrr calculating..... 95 of 392.8125\nWhirrrrrr calculating..... 96 of 392.8125\nWhirrrrrr calculating..... 97 of 392.8125\nWhirrrrrr calculating..... 98 of 392.8125\nWhirrrrrr calculating..... 99 of 392.8125\nWhirrrrrr calculating..... 100 of 392.8125\nWhirrrrrr calculating..... 101 of 392.8125\nWhirrrrrr calculating..... 102 of 392.8125\nWhirrrrrr calculating..... 103 of 392.8125\nWhirrrrrr calculating..... 104 of 392.8125\nWhirrrrrr calculating..... 105 of 392.8125\nWhirrrrrr calculating..... 106 of 392.8125\nWhirrrrrr calculating..... 107 of 392.8125\nWhirrrrrr calculating..... 108 of 392.8125\nWhirrrrrr calculating..... 109 of 392.8125\nWhirrrrrr calculating..... 110 of 392.8125\nWhirrrrrr calculating..... 111 of 392.8125\nWhirrrrrr calculating..... 112 of 392.8125\nWhirrrrrr calculating..... 113 of 392.8125\nWhirrrrrr calculating..... 114 of 392.8125\nWhirrrrrr calculating..... 115 of 392.8125\nWhirrrrrr calculating..... 116 of 392.8125\nWhirrrrrr calculating..... 117 of 392.8125\nWhirrrrrr calculating..... 118 of 392.8125\nWhirrrrrr calculating..... 119 of 392.8125\nWhirrrrrr calculating..... 120 of 392.8125\nWhirrrrrr calculating..... 121 of 392.8125\nWhirrrrrr calculating..... 122 of 392.8125\nWhirrrrrr calculating..... 123 of 392.8125\nWhirrrrrr calculating..... 124 of 392.8125\nWhirrrrrr calculating..... 125 of 392.8125\nWhirrrrrr calculating..... 126 of 392.8125\nWhirrrrrr calculating..... 127 of 392.8125\nWhirrrrrr calculating..... 128 of 392.8125\nWhirrrrrr calculating..... 129 of 392.8125\nWhirrrrrr calculating..... 130 of 392.8125\nWhirrrrrr calculating..... 131 of 392.8125\nWhirrrrrr calculating..... 132 of 392.8125\nWhirrrrrr calculating..... 133 of 392.8125\nWhirrrrrr calculating..... 134 of 392.8125\nWhirrrrrr calculating..... 135 of 392.8125\nWhirrrrrr calculating..... 136 of 392.8125\nWhirrrrrr calculating..... 137 of 392.8125\nWhirrrrrr calculating..... 138 of 392.8125\nWhirrrrrr calculating..... 139 of 392.8125\nWhirrrrrr calculating..... 140 of 392.8125\nWhirrrrrr calculating..... 141 of 392.8125\nWhirrrrrr calculating..... 142 of 392.8125\nWhirrrrrr calculating..... 143 of 392.8125\nWhirrrrrr calculating..... 144 of 392.8125\nWhirrrrrr calculating..... 145 of 392.8125\nWhirrrrrr calculating..... 146 of 392.8125\nWhirrrrrr calculating..... 147 of 392.8125\nWhirrrrrr calculating..... 148 of 392.8125\nWhirrrrrr calculating..... 149 of 392.8125\nWhirrrrrr calculating..... 150 of 392.8125\nWhirrrrrr calculating..... 151 of 392.8125\nWhirrrrrr calculating..... 152 of 392.8125\nWhirrrrrr calculating..... 153 of 392.8125\nWhirrrrrr calculating..... 154 of 392.8125\nWhirrrrrr calculating..... 155 of 392.8125\nWhirrrrrr calculating..... 156 of 392.8125\nWhirrrrrr calculating..... 157 of 392.8125\nWhirrrrrr calculating..... 158 of 392.8125\nWhirrrrrr calculating..... 159 of 392.8125\nWhirrrrrr calculating..... 160 of 392.8125\nWhirrrrrr calculating..... 161 of 392.8125\nWhirrrrrr calculating..... 162 of 392.8125\nWhirrrrrr calculating..... 163 of 392.8125\nWhirrrrrr calculating..... 164 of 392.8125\nWhirrrrrr calculating..... 165 of 392.8125\nWhirrrrrr calculating..... 166 of 392.8125\nWhirrrrrr calculating..... 167 of 392.8125\nWhirrrrrr calculating..... 168 of 392.8125\nWhirrrrrr calculating..... 169 of 392.8125\nWhirrrrrr calculating..... 170 of 392.8125\nWhirrrrrr calculating..... 171 of 392.8125\nWhirrrrrr calculating..... 172 of 392.8125\nWhirrrrrr calculating..... 173 of 392.8125\nWhirrrrrr calculating..... 174 of 392.8125\nWhirrrrrr calculating..... 175 of 392.8125\nWhirrrrrr calculating..... 176 of 392.8125\nWhirrrrrr calculating..... 177 of 392.8125\nWhirrrrrr calculating..... 178 of 392.8125\nWhirrrrrr calculating..... 179 of 392.8125\nWhirrrrrr calculating..... 180 of 392.8125\nWhirrrrrr calculating..... 181 of 392.8125\nWhirrrrrr calculating..... 182 of 392.8125\nWhirrrrrr calculating..... 183 of 392.8125\nWhirrrrrr calculating..... 184 of 392.8125\nWhirrrrrr calculating..... 185 of 392.8125\nWhirrrrrr calculating..... 186 of 392.8125\nWhirrrrrr calculating..... 187 of 392.8125\nWhirrrrrr calculating..... 188 of 392.8125\nWhirrrrrr calculating..... 189 of 392.8125\nWhirrrrrr calculating..... 190 of 392.8125\nWhirrrrrr calculating..... 191 of 392.8125\nWhirrrrrr calculating..... 192 of 392.8125\nWhirrrrrr calculating..... 193 of 392.8125\nWhirrrrrr calculating..... 194 of 392.8125\nWhirrrrrr calculating..... 195 of 392.8125\nWhirrrrrr calculating..... 196 of 392.8125\nWhirrrrrr calculating..... 197 of 392.8125\nWhirrrrrr calculating..... 198 of 392.8125\nWhirrrrrr calculating..... 199 of 392.8125\nWhirrrrrr calculating..... 200 of 392.8125\nWhirrrrrr calculating..... 201 of 392.8125\nWhirrrrrr calculating..... 202 of 392.8125\nWhirrrrrr calculating..... 203 of 392.8125\nWhirrrrrr calculating..... 204 of 392.8125\nWhirrrrrr calculating..... 205 of 392.8125\nWhirrrrrr calculating..... 206 of 392.8125\nWhirrrrrr calculating..... 207 of 392.8125\nWhirrrrrr calculating..... 208 of 392.8125\nWhirrrrrr calculating..... 209 of 392.8125\nWhirrrrrr calculating..... 210 of 392.8125\nWhirrrrrr calculating..... 211 of 392.8125\nWhirrrrrr calculating..... 212 of 392.8125\nWhirrrrrr calculating..... 213 of 392.8125\nWhirrrrrr calculating..... 214 of 392.8125\nWhirrrrrr calculating..... 215 of 392.8125\nWhirrrrrr calculating..... 216 of 392.8125\nWhirrrrrr calculating..... 217 of 392.8125\nWhirrrrrr calculating..... 218 of 392.8125\nWhirrrrrr calculating..... 219 of 392.8125\nWhirrrrrr calculating..... 220 of 392.8125\nWhirrrrrr calculating..... 221 of 392.8125\nWhirrrrrr calculating..... 222 of 392.8125\nWhirrrrrr calculating..... 223 of 392.8125\nWhirrrrrr calculating..... 224 of 392.8125\nWhirrrrrr calculating..... 225 of 392.8125\nWhirrrrrr calculating..... 226 of 392.8125\nWhirrrrrr calculating..... 227 of 392.8125\nWhirrrrrr calculating..... 228 of 392.8125\nWhirrrrrr calculating..... 229 of 392.8125\nWhirrrrrr calculating..... 230 of 392.8125\nWhirrrrrr calculating..... 231 of 392.8125\nWhirrrrrr calculating..... 232 of 392.8125\nWhirrrrrr calculating..... 233 of 392.8125\nWhirrrrrr calculating..... 234 of 392.8125\nWhirrrrrr calculating..... 235 of 392.8125\nWhirrrrrr calculating..... 236 of 392.8125\nWhirrrrrr calculating..... 237 of 392.8125\nWhirrrrrr calculating..... 238 of 392.8125\nWhirrrrrr calculating..... 239 of 392.8125\nWhirrrrrr calculating..... 240 of 392.8125\nWhirrrrrr calculating..... 241 of 392.8125\nWhirrrrrr calculating..... 242 of 392.8125\nWhirrrrrr calculating..... 243 of 392.8125\nWhirrrrrr calculating..... 244 of 392.8125\nWhirrrrrr calculating..... 245 of 392.8125\nWhirrrrrr calculating..... 246 of 392.8125\nWhirrrrrr calculating..... 247 of 392.8125\nWhirrrrrr calculating..... 248 of 392.8125\nWhirrrrrr calculating..... 249 of 392.8125\nWhirrrrrr calculating..... 250 of 392.8125\nWhirrrrrr calculating..... 251 of 392.8125\nWhirrrrrr calculating..... 252 of 392.8125\nWhirrrrrr calculating..... 253 of 392.8125\nWhirrrrrr calculating..... 254 of 392.8125\nWhirrrrrr calculating..... 255 of 392.8125\nWhirrrrrr calculating..... 256 of 392.8125\nWhirrrrrr calculating..... 257 of 392.8125\nWhirrrrrr calculating..... 258 of 392.8125\nWhirrrrrr calculating..... 259 of 392.8125\nWhirrrrrr calculating..... 260 of 392.8125\nWhirrrrrr calculating..... 261 of 392.8125\nWhirrrrrr calculating..... 262 of 392.8125\nWhirrrrrr calculating..... 263 of 392.8125\nWhirrrrrr calculating..... 264 of 392.8125\nWhirrrrrr calculating..... 265 of 392.8125\nWhirrrrrr calculating..... 266 of 392.8125\nWhirrrrrr calculating..... 267 of 392.8125\nWhirrrrrr calculating..... 268 of 392.8125\nWhirrrrrr calculating..... 269 of 392.8125\nWhirrrrrr calculating..... 270 of 392.8125\nWhirrrrrr calculating..... 271 of 392.8125\nWhirrrrrr calculating..... 272 of 392.8125\nWhirrrrrr calculating..... 273 of 392.8125\nWhirrrrrr calculating..... 274 of 392.8125\nWhirrrrrr calculating..... 275 of 392.8125\nWhirrrrrr calculating..... 276 of 392.8125\nWhirrrrrr calculating..... 277 of 392.8125\nWhirrrrrr calculating..... 278 of 392.8125\nWhirrrrrr calculating..... 279 of 392.8125\nWhirrrrrr calculating..... 280 of 392.8125\nWhirrrrrr calculating..... 281 of 392.8125\nWhirrrrrr calculating..... 282 of 392.8125\nWhirrrrrr calculating..... 283 of 392.8125\nWhirrrrrr calculating..... 284 of 392.8125\nWhirrrrrr calculating..... 285 of 392.8125\nWhirrrrrr calculating..... 286 of 392.8125\nWhirrrrrr calculating..... 287 of 392.8125\nWhirrrrrr calculating..... 288 of 392.8125\nWhirrrrrr calculating..... 289 of 392.8125\nWhirrrrrr calculating..... 290 of 392.8125\nWhirrrrrr calculating..... 291 of 392.8125\nWhirrrrrr calculating..... 292 of 392.8125\nWhirrrrrr calculating..... 293 of 392.8125\nWhirrrrrr calculating..... 294 of 392.8125\nWhirrrrrr calculating..... 295 of 392.8125\nWhirrrrrr calculating..... 296 of 392.8125\nWhirrrrrr calculating..... 297 of 392.8125\nWhirrrrrr calculating..... 298 of 392.8125\nWhirrrrrr calculating..... 299 of 392.8125\nWhirrrrrr calculating..... 300 of 392.8125\nWhirrrrrr calculating..... 301 of 392.8125\nWhirrrrrr calculating..... 302 of 392.8125\nWhirrrrrr calculating..... 303 of 392.8125\nWhirrrrrr calculating..... 304 of 392.8125\nWhirrrrrr calculating..... 305 of 392.8125\nWhirrrrrr calculating..... 306 of 392.8125\nWhirrrrrr calculating..... 307 of 392.8125\nWhirrrrrr calculating..... 308 of 392.8125\nWhirrrrrr calculating..... 309 of 392.8125\nWhirrrrrr calculating..... 310 of 392.8125\nWhirrrrrr calculating..... 311 of 392.8125\nWhirrrrrr calculating..... 312 of 392.8125\nWhirrrrrr calculating..... 313 of 392.8125\nWhirrrrrr calculating..... 314 of 392.8125\nWhirrrrrr calculating..... 315 of 392.8125\nWhirrrrrr calculating..... 316 of 392.8125\nWhirrrrrr calculating..... 317 of 392.8125\nWhirrrrrr calculating..... 318 of 392.8125\nWhirrrrrr calculating..... 319 of 392.8125\nWhirrrrrr calculating..... 320 of 392.8125\nWhirrrrrr calculating..... 321 of 392.8125\nWhirrrrrr calculating..... 322 of 392.8125\nWhirrrrrr calculating..... 323 of 392.8125\nWhirrrrrr calculating..... 324 of 392.8125\nWhirrrrrr calculating..... 325 of 392.8125\nWhirrrrrr calculating..... 326 of 392.8125\nWhirrrrrr calculating..... 327 of 392.8125\nWhirrrrrr calculating..... 328 of 392.8125\nWhirrrrrr calculating..... 329 of 392.8125\nWhirrrrrr calculating..... 330 of 392.8125\nWhirrrrrr calculating..... 331 of 392.8125\nWhirrrrrr calculating..... 332 of 392.8125\nWhirrrrrr calculating..... 333 of 392.8125\nWhirrrrrr calculating..... 334 of 392.8125\nWhirrrrrr calculating..... 335 of 392.8125\nWhirrrrrr calculating..... 336 of 392.8125\nWhirrrrrr calculating..... 337 of 392.8125\nWhirrrrrr calculating..... 338 of 392.8125\nWhirrrrrr calculating..... 339 of 392.8125\nWhirrrrrr calculating..... 340 of 392.8125\nWhirrrrrr calculating..... 341 of 392.8125\nWhirrrrrr calculating..... 342 of 392.8125\nWhirrrrrr calculating..... 343 of 392.8125\nWhirrrrrr calculating..... 344 of 392.8125\nWhirrrrrr calculating..... 345 of 392.8125\nWhirrrrrr calculating..... 346 of 392.8125\nWhirrrrrr calculating..... 347 of 392.8125\nWhirrrrrr calculating..... 348 of 392.8125\nWhirrrrrr calculating..... 349 of 392.8125\nWhirrrrrr calculating..... 350 of 392.8125\nWhirrrrrr calculating..... 351 of 392.8125\nWhirrrrrr calculating..... 352 of 392.8125\nWhirrrrrr calculating..... 353 of 392.8125\nWhirrrrrr calculating..... 354 of 392.8125\nWhirrrrrr calculating..... 355 of 392.8125\nWhirrrrrr calculating..... 356 of 392.8125\nWhirrrrrr calculating..... 357 of 392.8125\nWhirrrrrr calculating..... 358 of 392.8125\nWhirrrrrr calculating..... 359 of 392.8125\nWhirrrrrr calculating..... 360 of 392.8125\nWhirrrrrr calculating..... 361 of 392.8125\nWhirrrrrr calculating..... 362 of 392.8125\nWhirrrrrr calculating..... 363 of 392.8125\nWhirrrrrr calculating..... 364 of 392.8125\nWhirrrrrr calculating..... 365 of 392.8125\nWhirrrrrr calculating..... 366 of 392.8125\nWhirrrrrr calculating..... 367 of 392.8125\nWhirrrrrr calculating..... 368 of 392.8125\nWhirrrrrr calculating..... 369 of 392.8125\nWhirrrrrr calculating..... 370 of 392.8125\nWhirrrrrr calculating..... 371 of 392.8125\nWhirrrrrr calculating..... 372 of 392.8125\nWhirrrrrr calculating..... 373 of 392.8125\nWhirrrrrr calculating..... 374 of 392.8125\nWhirrrrrr calculating..... 375 of 392.8125\nWhirrrrrr calculating..... 376 of 392.8125\nWhirrrrrr calculating..... 377 of 392.8125\nWhirrrrrr calculating..... 378 of 392.8125\nWhirrrrrr calculating..... 379 of 392.8125\nWhirrrrrr calculating..... 380 of 392.8125\nWhirrrrrr calculating..... 381 of 392.8125\nWhirrrrrr calculating..... 382 of 392.8125\nWhirrrrrr calculating..... 383 of 392.8125\nWhirrrrrr calculating..... 384 of 392.8125\nWhirrrrrr calculating..... 385 of 392.8125\nWhirrrrrr calculating..... 386 of 392.8125\nWhirrrrrr calculating..... 387 of 392.8125\nWhirrrrrr calculating..... 388 of 392.8125\nWhirrrrrr calculating..... 389 of 392.8125\nWhirrrrrr calculating..... 390 of 392.8125\nWhirrrrrr calculating..... 391 of 392.8125\nWhirrrrrr calculating..... 392 of 392.8125\n"
    }
   ],
   "source": [
    "\n",
    "var = 0.0\n",
    "count_var =0\n",
    "for images, _ in tr_dl:\n",
    "    batch_samples = images.size(0)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    var += ((images - mean.unsqueeze(1))**2).sum([0,2])\n",
    "    print(f\"Whirrrrrr calculating..... {count_var} of {train_size/batch_sz}\")\n",
    "    count_var+= 1\n",
    "std = torch.sqrt(var / (len(tr_dl.dataset)*720*1280))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([63.4003, 64.1523, 64.4491])"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Moderate_tr_stats = mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean is tensor([ 98.2207, 101.6702, 102.9898])\n",
    "std is tensor([63.4003, 64.1523, 64.4491])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Normalize(mean=tensor([ 98.2207, 101.6702, 102.9898]), std=tensor([63.4003, 64.1523, 64.4491]))"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "transforms.Normalize(*Moderate_tr_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "*************MAKE SURE THE PATH FILE IN THE FOR LOOP IS THE BASE IMAGE DIRECTORY ON YOUR COMPUTER**************\n"
    }
   ],
   "source": [
    "normed_dataset = ModerateDataset(trans_on=True, transform=transforms.Normalize(*Moderate_tr_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_dl = DataLoader(normed_dataset,  batch_size=batch_sz, shuffle=True,  num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kitti dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'C:/Users/Ben/OneDrive - Bournemouth University/Computer Vision/Datasets/Kitti/depth_selection/val_selection_cropped/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kitti(Dataset):\n",
    "  def __init__(self, data_root, transform=torchvision.transforms.ToTensor(),):\n",
    "    self.samples = {}\n",
    "    self.transform = transform\n",
    "    for file in os.listdir(data_root):\n",
    "      subfolder_list = [ os.path.join( data_root, file , subfolder ) for subfolder in os.listdir(os.path.join(data_root, file))]\n",
    "      self.samples[file] = subfolder_list\n",
    "\n",
    "    keys = [ key for key in self.samples.keys()]\n",
    "    self.RGB_DIRS   = self.samples[keys[0]]\n",
    "    self.DEPTH_DIRS = self.samples[keys[1]]\n",
    "\n",
    "    self.length = int(sum([len(self.samples[key]) for key in self.samples.keys()])*0.5)\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.length\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    RGB_IMAGES   = Image.open(self.RGB_DIRS[index])\n",
    "    DEPTH_IMAGES = Image.open(self.DEPTH_DIRS[index])\n",
    "\n",
    "    if self.transform:\n",
    "      RGB_IMAGES   = self.transform(RGB_IMAGES)\n",
    "      DEPTH_IMAGES = self.transform(DEPTH_IMAGES)    \n",
    "      \n",
    "    return DEPTH_IMAGES, RGB_IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_kitti_data = Kitti(root_dir)\n",
    "kitti_small, kitti_rest = torch.utils.data.random_split(total_kitti_data, [40, 960])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 16\n",
    "kitti_dataloader = DataLoader(total_kitti_data,  batch_size=batch_sz, shuffle=True,  num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitti_small_dl = DataLoader(kitti_small,  batch_size=batch_sz, shuffle=True,  num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d363db3b64b428f89f28ba1010c9f00"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ""
    }
   ],
   "source": [
    "small_preds, small_gts = predict_and_gt(kitti_small_dl, len(kitti_small), batch_sz, re_load_trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=63.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef9468766f1245858ff00e89f79246cb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ""
    }
   ],
   "source": [
    "kitti_preds, kitti_gts = predict_and_gt(kitti_dataloader, len(total_kitti_data), batch_sz, re_load_trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(kitti_gts)):\n",
    "    for j in range(len(kitti_gts[0])):\n",
    "        kitti_gts[i][j] = np.squeeze(kitti_gts[i][j], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(small_gts)):\n",
    "    for j in range(len(small_gts[0])):\n",
    "        small_gts[i][j] = np.squeeze(small_gts[i][j], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_saved = 'C:/Users/Ben/OneDrive - Bournemouth University/Computer Vision/Datasets/Saved_preds/kitti_preds.pckl'\n",
    "# f = open(path_saved, 'wb')\n",
    "# pickle.dump(kitti_gts, f)\n",
    "# f.close()\n",
    "f = open(path_saved, 'rb')\n",
    "kitti_preds = pickle.load(f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Calculating errors for batch 0 of 62\nCalculating errors for batch 1 of 62\nCalculating errors for batch 2 of 62\nCalculating errors for batch 3 of 62\nCalculating errors for batch 4 of 62\nCalculating errors for batch 5 of 62\nCalculating errors for batch 6 of 62\nCalculating errors for batch 7 of 62\nCalculating errors for batch 8 of 62\nCalculating errors for batch 9 of 62\nCalculating errors for batch 10 of 62\nCalculating errors for batch 11 of 62\nCalculating errors for batch 12 of 62\nCalculating errors for batch 13 of 62\nCalculating errors for batch 14 of 62\nCalculating errors for batch 15 of 62\nCalculating errors for batch 16 of 62\nCalculating errors for batch 17 of 62\nCalculating errors for batch 18 of 62\nCalculating errors for batch 19 of 62\nCalculating errors for batch 20 of 62\nCalculating errors for batch 21 of 62\nCalculating errors for batch 22 of 62\nCalculating errors for batch 23 of 62\nCalculating errors for batch 24 of 62\nCalculating errors for batch 25 of 62\nCalculating errors for batch 26 of 62\nCalculating errors for batch 27 of 62\nCalculating errors for batch 28 of 62\nCalculating errors for batch 29 of 62\nCalculating errors for batch 30 of 62\nCalculating errors for batch 31 of 62\nCalculating errors for batch 32 of 62\nCalculating errors for batch 33 of 62\nCalculating errors for batch 34 of 62\nCalculating errors for batch 35 of 62\nCalculating errors for batch 36 of 62\nCalculating errors for batch 37 of 62\nCalculating errors for batch 38 of 62\nCalculating errors for batch 39 of 62\nCalculating errors for batch 40 of 62\nCalculating errors for batch 41 of 62\nCalculating errors for batch 42 of 62\nCalculating errors for batch 43 of 62\nCalculating errors for batch 44 of 62\nCalculating errors for batch 45 of 62\nCalculating errors for batch 46 of 62\nCalculating errors for batch 47 of 62\nCalculating errors for batch 48 of 62\nCalculating errors for batch 49 of 62\nCalculating errors for batch 50 of 62\nCalculating errors for batch 51 of 62\nCalculating errors for batch 52 of 62\nCalculating errors for batch 53 of 62\nCalculating errors for batch 54 of 62\nCalculating errors for batch 55 of 62\nCalculating errors for batch 56 of 62\nCalculating errors for batch 57 of 62\nCalculating errors for batch 58 of 62\nCalculating errors for batch 59 of 62\nCalculating errors for batch 60 of 62\nIncrimenting for batch 0 of 61\nIncrimenting for batch 1 of 61\nIncrimenting for batch 2 of 61\nIncrimenting for batch 3 of 61\nIncrimenting for batch 4 of 61\nIncrimenting for batch 5 of 61\nIncrimenting for batch 6 of 61\nIncrimenting for batch 7 of 61\nIncrimenting for batch 8 of 61\nIncrimenting for batch 9 of 61\nIncrimenting for batch 10 of 61\nIncrimenting for batch 11 of 61\nIncrimenting for batch 12 of 61\nIncrimenting for batch 13 of 61\nIncrimenting for batch 14 of 61\nIncrimenting for batch 15 of 61\nIncrimenting for batch 16 of 61\nIncrimenting for batch 17 of 61\nIncrimenting for batch 18 of 61\nIncrimenting for batch 19 of 61\nIncrimenting for batch 20 of 61\nIncrimenting for batch 21 of 61\nIncrimenting for batch 22 of 61\nIncrimenting for batch 23 of 61\nIncrimenting for batch 24 of 61\nIncrimenting for batch 25 of 61\nIncrimenting for batch 26 of 61\nIncrimenting for batch 27 of 61\nIncrimenting for batch 28 of 61\nIncrimenting for batch 29 of 61\nIncrimenting for batch 30 of 61\nIncrimenting for batch 31 of 61\nIncrimenting for batch 32 of 61\nIncrimenting for batch 33 of 61\nIncrimenting for batch 34 of 61\nIncrimenting for batch 35 of 61\nIncrimenting for batch 36 of 61\nIncrimenting for batch 37 of 61\nIncrimenting for batch 38 of 61\nIncrimenting for batch 39 of 61\nIncrimenting for batch 40 of 61\nIncrimenting for batch 41 of 61\nIncrimenting for batch 42 of 61\nIncrimenting for batch 43 of 61\nIncrimenting for batch 44 of 61\nIncrimenting for batch 45 of 61\nIncrimenting for batch 46 of 61\nIncrimenting for batch 47 of 61\nIncrimenting for batch 48 of 61\nIncrimenting for batch 49 of 61\nIncrimenting for batch 50 of 61\nIncrimenting for batch 51 of 61\nIncrimenting for batch 52 of 61\nIncrimenting for batch 53 of 61\nIncrimenting for batch 54 of 61\nIncrimenting for batch 55 of 61\nIncrimenting for batch 56 of 61\nIncrimenting for batch 57 of 61\nIncrimenting for batch 58 of 61\nIncrimenting for batch 59 of 61\nIncrimenting for batch 60 of 61\nSum sqr for batch 0 of 61\nSum sqr for batch 1 of 61\nSum sqr for batch 2 of 61\nSum sqr for batch 3 of 61\nSum sqr for batch 4 of 61\nSum sqr for batch 5 of 61\nSum sqr for batch 6 of 61\nSum sqr for batch 7 of 61\nSum sqr for batch 8 of 61\nSum sqr for batch 9 of 61\nSum sqr for batch 10 of 61\nSum sqr for batch 11 of 61\nSum sqr for batch 12 of 61\nSum sqr for batch 13 of 61\nSum sqr for batch 14 of 61\nSum sqr for batch 15 of 61\nSum sqr for batch 16 of 61\nSum sqr for batch 17 of 61\nSum sqr for batch 18 of 61\nSum sqr for batch 19 of 61\nSum sqr for batch 20 of 61\nSum sqr for batch 21 of 61\nSum sqr for batch 22 of 61\nSum sqr for batch 23 of 61\nSum sqr for batch 24 of 61\nSum sqr for batch 25 of 61\nSum sqr for batch 26 of 61\nSum sqr for batch 27 of 61\nSum sqr for batch 28 of 61\nSum sqr for batch 29 of 61\nSum sqr for batch 30 of 61\nSum sqr for batch 31 of 61\nSum sqr for batch 32 of 61\nSum sqr for batch 33 of 61\nSum sqr for batch 34 of 61\nSum sqr for batch 35 of 61\nSum sqr for batch 36 of 61\nSum sqr for batch 37 of 61\nSum sqr for batch 38 of 61\nSum sqr for batch 39 of 61\nSum sqr for batch 40 of 61\nSum sqr for batch 41 of 61\nSum sqr for batch 42 of 61\nSum sqr for batch 43 of 61\nSum sqr for batch 44 of 61\nSum sqr for batch 45 of 61\nSum sqr for batch 46 of 61\nSum sqr for batch 47 of 61\nSum sqr for batch 48 of 61\nSum sqr for batch 49 of 61\nSum sqr for batch 50 of 61\nSum sqr for batch 51 of 61\nSum sqr for batch 52 of 61\nSum sqr for batch 53 of 61\nSum sqr for batch 54 of 61\nSum sqr for batch 55 of 61\nSum sqr for batch 56 of 61\nSum sqr for batch 57 of 61\nSum sqr for batch 58 of 61\nSum sqr for batch 59 of 61\nSum sqr for batch 60 of 61\n"
    }
   ],
   "source": [
    "kitti_means, kitti_stds = mean_and_std_errors(kitti_preds, kitti_gts, len(total_kitti_data), batch_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[3676.845358138522,\n 4553.388690419297,\n 2.291738913256914,\n 2.2955693022502075,\n 8.25475876510341,\n 8.27114218538775,\n 0.5173058833246852,\n 0.9148723111323095,\n 0.9147446485930459]"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "kitti_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[580.3330349295508,\n 763.3701968566533,\n 0.20892947574606474,\n 0.20947265670611862,\n 0.7409090534398427,\n 0.7425474944954199,\n 0.07644160760414093,\n 0.08129594208857985,\n 0.08128460117823653]"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "kitti_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_saved = 'C:/Users/Ben/OneDrive - Bournemouth University/Computer Vision/Datasets/Saved_preds/kitti_stds.pckl'\n",
    "f = open(path_saved, 'wb')\n",
    "pickle.dump(kitti_stds, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "inv_err_count is 2.671990419030787\ninv_err_count is 5.209514118094075\ninv_err_count is 7.723767793693632\ninv_err_count is 10.149483881723356\ninv_err_count is 12.401186392457227\ninv_err_count is 14.677149942374392\ninv_err_count is 17.055348043724564\ninv_err_count is 19.496025514331773\ninv_err_count is 21.74469733540207\ninv_err_count is 24.26428909321259\ninv_err_count is 26.671381271172248\ninv_err_count is 29.213874604689263\ninv_err_count is 31.735638661606792\ninv_err_count is 34.24553361524333\ninv_err_count is 36.728184726741766\nval size is: 40\n"
    }
   ],
   "source": [
    "small_means, small_stds = mean_and_std_errors(small_preds, small_gts, len(kitti_small), batch_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "40"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "len(kitti_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[1476.0871162346887,\n 1850.278744799978,\n 0.9386037040805875,\n 0.9402427662252112,\n 3.3726493793030863,\n 3.379557017721986,\n 0.21440780853684505,\n 0.37494599450589444,\n 0.374892000246371]"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "small_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[1539.9871201818867,\n 1936.0552721457273,\n 0.9582299401336529,\n 0.959914244338768,\n 3.443070523115934,\n 3.4501232419339067,\n 0.22294905654845834,\n 0.3826776533295292,\n 0.38262254610790003]"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "small_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}