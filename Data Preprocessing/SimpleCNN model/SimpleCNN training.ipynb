{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L7 Computer vision group project\n",
    "## L7 CV Group - 2\n",
    "### Ben Snow\n",
    "### Nick Lindfield\n",
    "### Ashavidya Kusuma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depth prediction using video game data for real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the problem\n",
    "With the rise of autonomous vehicles, on-camera image processing and augmented reality (AR), there is an increasing need for accurate depth prediction\n",
    "Current methods produce highly noisy results and lack detail, frequently failing to separate background and foreground objects. [14]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image cannot be found](depth_comparison.png \"Depth comaprison\")\n",
    "Figure 1. A comaprison of predicted and ground truth depth predictions from various neural network depth prediction algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current uses for depth prediction/estimation are:\n",
    "- Autonomous driving - predicting the distance of objects on the road, such as other motorists, pedestrians and cyclists.\n",
    "- Image processing - blurring foreground subject from the background.\n",
    "- AR - object occlusion, placing digital characters behind objects such as tables.\n",
    "\n",
    "\n",
    "With this project, we aim to overcome the difficulties of collecting real-world data by using augmented data from video games, we will extract the depth buffer and RGB values to be used as our training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some advantages and disadvantages of real-world and augmented data are:\n",
    "\n",
    "\n",
    "Disadvantages of real-world data\n",
    "- DIS - Expensive -transporting the equipment and staff to the location \n",
    "- DIS - Requires specialized equipment\n",
    "- DIS - Weather condition can affect the accuracy of the data\n",
    "\n",
    "\n",
    "Advantage of video game data\n",
    "- ADV - Cheap and easy to come by\n",
    "- ADV - A larger amount of data can be collected\n",
    "- ADV - Control of lighting and weather conditions \n",
    "- ADV - Accurate depth in poor weather conditions easily\n",
    "- ADV - Can create edge cases but these don’t happen naturally\n",
    "- ADV - No transportation and logistical costs -- Saves the environment\n",
    "- ADV - Procedural generation of datasets\n",
    "\n",
    "\n",
    "Disadvantages of video game data\n",
    "\n",
    "\n",
    "- DIS - May not be an accurate estimation of the real world\n",
    "- DIS - Overfitting to similar environments\n",
    "- DIS - Our data may be too perfect, real-world data can contain noise and other artefacts\n",
    "\n",
    "\n",
    "To analyse our results we will test them against real-world datasets, such as DrivingStereo, KITTI.\n",
    "We intend to build upon existing papers that obtain depth data from video games.  Existing solutions exist but they are not open source and freely available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth detection and video games\n",
    "Advancement of the realistic nature of Computer-Generated (CG) environments could become the basis for training artificial agents. Instead of spending time driving around physical cars in the real world to collect data, a CG environment could be created to simulate the same data. Advantages of CG environments over real-world environments could be that data is much faster to produce, AI’s could learn faster and edge cases (broken down vehicles, towing caravans etc…) could be easily inserted into the simulation at will instead of waiting for them to happen in real life. Transfer learning could be exploited to utilise the training performed in the CG environment to the real-world.\n",
    "\n",
    "Existing techniques are beginning to be used for training self-driving cars this way. Nvidia have created a virtual training environment for training driverless cars for Toyota called Nvidia Constellation. This system allows for the training, testing and evaluation of driverless AI in many thousands of different scenarios allowing for billions of miles of training before hitting the road.\n",
    "\n",
    "Edge case simulations can easily be inserted into the CG environment. Existing scenes from previous CG projects and games can be directly used and included in the training set. This will be advantageous for the AI’s learning capabilities as exposure to as many scenarios as possible generates experience for real-world events. Taken further, simulations of natural disasters, traffic jams, piles ups and crashes can all be generated in CG without any real-world danger! Fully autonomous vehicles should be equipped with the experience to tackle these edge cases, should they need to deal with them.\n",
    "\n",
    "Modern video game engines are capable of creating incredibly complex scene topologies in real time with physically based movements and interactions. An example of this being Grand Theft Auto 5 in which players are free to roam in a realistic rendering of a city, San Andreas, based on Los Angeles. Players can drive a multitude of cars, motorcycles and aircraft around the city on numerous roads and paths. This environment could be an ideal playground for building and training autonomous vehicles for use in the real world. One example of this is in a youtube series by user ‘Sentdex’ who attempted to use a Deep Convolutional Network based on AlexNet to drive on the streets of San Andreas. The model is available on github for download.\n",
    "\n",
    "In a video game, depth field training data can be extracted directly from the video game engine and is called a z-buffer. This been demonstrated by Adrian Courrèges.\n",
    "\n",
    "Existing techniques are available for constructing a depth map from a single frame and from binocular stereo frames. The referenced single frame method employs traditional computer vision techniques whereas the stereo method uses a CNN architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Aims and objectives\n",
    "We aim to generate a dataset consisting of RGB and Depth channels in a wide range of environments and weather conditions.\n",
    "We aim to create and evaluate depth estimation algorithms, utilising PyTorch, OpenCV and a combination of traditional computer vision methods, finally evaluating our results on a range of testing datasets.\n",
    "The following measurable objectives have been identified:\n",
    "- Generate a synthetic image and depth dataset from the video game GTA V\n",
    "- Use data augmentation techniques to increase the amount of data\n",
    "- Use a driving bot and mods to autonomously collect data in different weather conditions and environments\n",
    "- Create a traditional (OpenCV) depth prediction algorithm\n",
    "- Create a neural network only depth prediction algorithm\n",
    "- Create a depth prediction algorithm neural network trained on data augmented with OpenCV techniques\n",
    "- Evaluate the depth prediction model on various real-world and virtual testing datasets including the accepted standard KITTI dataset and a new, more varied dataset, DrivingStereo (Feb 2020).\n",
    "- Compare our model to the model found in our references below, as they have already trained a model on a narrow video game dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data acquisition and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GTA V\n",
    "\n",
    "GTA V is a closed source video game meaning that there is no direct access to the source code available. As a result, the GTA V modding community has found multiple different ways of accessing parts of the rendering pipeline. These methods typically rely on injecting a 'DirectX11 driver' into the game before frame drawing time to intercept data used in the rendering stage. It is here that the depth information is stored.\n",
    "Data collection was split into 3 parts at the beginning of the project and are defined below.\n",
    "\n",
    "Simple collection\n",
    "- Extract order 10 RGB and depth image pairs from GTAV\n",
    "- Drive around in one environment with constant weather conditions, no occlusions and no data augmentation\n",
    "- Data collected so that initial models have data to work with\n",
    "\n",
    "Moderate collection\n",
    "- Use a bot to automatically drive around and drastically increase the dataset size\n",
    "- Use GTA mods to alter the weather conditions and times of day\n",
    "- Drive around new locations such as city and off-road\n",
    "- Implement low-level data augmentation such as translations and reflections\n",
    "\n",
    "Hard collection\n",
    "- Better data augmentation\n",
    "- Add in varied occlusions and domain adaptation\n",
    "- Image style transfer from real-world data to synthetic data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple collection\n",
    "\n",
    "A repository called [GTAVisionExport](https://github.com/umautobots/GTAVisionExport) was used as a starting point to extract single frames of Colour, Depth and Stencil Depth from GTAV. Saved files are stored as .raw images and, as such, cannot be directly imported into python natively. The section 'Image formats' discusses this further.\n",
    "\n",
    "Extraction code within the GTAVisionExport was altered to the following to extract the depth, RGB/colour image and stencil (not used) images. If the 'L' key is pressed, the depth, stencil and color buffers are written to the game file directory.\n",
    "\n",
    "![image cannot be found](Simple_collection_cpp.png \"Extraction code\")\n",
    "Figure 2: Extraction code written in C++ to output RGB, Depth and stencil depth images from GTAV.\n",
    "\n",
    "\n",
    "GTAVisionExport gives all saved images the same filenames (color.raw, depth.raw and stencil.raw). Taking multiple screenshots overwrites the currently saved files. Moving these files out of the game directory then taking another screenshot allows multiple different screenshots to be saved. This is cumbersome and annoying. As such, the GTAVisionExport source code will be changed so that new files are saved with a timestamp and to a new folder for easy, more organised storage.\n",
    "\n",
    "A full description of how to install GTAVisionExport can be seen in the [group google document here.](https://docs.google.com/document/d/1UcQl8Q-COs9_vZ65RKXD8DnIcmRnXzqsJBdfcmd6iJY/edit?usp=sharing)\n",
    "\n",
    "\n",
    "As ‘Simple collection’ only requires on the order of 10 colour and depth images, the manual moving and renaming files technique was be used.\n",
    "\n",
    "Example image outputs from Simple collection can be seen below:\n",
    "\n",
    "![image cannot be found](sample_simple_rgb.png \"Simple RGB\")\n",
    "Figure 3. Example of a colour screenshot extracted from in-game.\n",
    "\n",
    "![image cannot be found](sample_depth_rgb.png \"Simple depth\")\n",
    "Figure 4. Depth information is shown with colour gradients, the more yellow/red the item the closer it is and vice versa for blue and purple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image formats\n",
    "\n",
    "Images extracted from GTAV are stored as .raw files and as such, two functions, `import_raw_colour_image` and `import_raw_depth_image` were written to load the images into numpy arrays.\n",
    "\n",
    "The shape of colour images are: \t (720, 1280, 4)\n",
    "\n",
    "And for depth images they are: \t     (720, 1280)\n",
    " \n",
    "Colour images are read in as 'unit8' with 4 channels, RGBA\n",
    "This means that for every one of the 720*1280 pixels there are 4 numbers that represent the Red, Green, Blue and Alpha channels in the image.\n",
    "\n",
    "Depth images are read in as 'float32' with 1 channel, depth\n",
    "Depth values of Zero relate to infinite depth in the scene and the larger the number the closer to the camera an object is\n",
    "The following conversion formula can be used to convert GTAV depth to real world metres:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data output from simple collection\n",
    "\n",
    "\n",
    "The resulting data from Simple collection consists of 6 colour images of resolution 720x1280 and 6 associated depth maps. Images were taken from within the default GTA V car, on the roads around the starting house, all were taken in the same, daytime lighting conditions with clear weather.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moderate collection\n",
    "\n",
    "Simple collection relied on manually pressing a keyboard key to capture an RGB image and depth map pair, part of moderate collection is to automate this process. To achieve this, and autoclicker software [AutoHotKey](https://www.autohotkey.com/) was used. A simple script to press the capture key automatically every 600ms was written, this is visible via Ben's [github repo](https://github.com/BenSnow6/depth_estimation/blob/master/Data_Collection/Moderate%20collection/testScript.ahk.ahk).\n",
    "\n",
    "In addition to this, the extraction code was altered to allow for multiple images to be outputted without overwriting previous images. The process is outlined below:\n",
    "- On ‘l’ key press\n",
    "- Open notepad file with a number stored in it\n",
    "- Attach number to ‘depth.raw’ and ‘colour.raw’ strings\n",
    "- Save the outputs in folders, one for depth and one for colour\n",
    "- Increment number stored in notepad.\n",
    "\n",
    "The following C++ code was written to achieve this.\n",
    "\n",
    "![image not found](Moderate_collection_cpp.png \"Moderate collection extraction code\")\n",
    "Figure 5. Extraction code written in C++ altered from the GTAVisionExport tool to add numeric labels to image filenames during collection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAutodrive\n",
    "In order to truly collect data autonomously the automatic driving modification called [VAutodrive](https://www.gta5-mods.com/scripts/vautodrive) (Five Auto drive) was downloaded and utilised. It is simple to use: get in a car, change the view to 1st person (by pressing ‘V’), go to the map, set a waypoint (double click on road) then press ctrl+J to start the autopilot. By default the pilot will drive at 25mph, obeying traffic laws and driving non erratically. After starting VAutopilot use the autohotkey script and press Ctrl+L to start it, this will start automatic collection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NativeUI\n",
    "To change the weather conditions in GTAV, a plugin called NativeUI was used allowing access to an in-game menu. Open this menu with ‘F4’ and use the number pad to navigate it. In NativeUI the weather conditions can be changed, along with the time of day. These were altered and automatic collection was used to collect scenes of around 8000 frames in different weather conditions and times of day.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output from Moderate collection\n",
    "\n",
    "Moderate collection resulted in 7857 colour and 7857 associated depth images extracted from GTAV in 5 different weather conditions. All images are stored in .raw files with resolutions of 720x1280 and a size of 3.6MB each. The output is summarised in Table 1 below.\n",
    "\n",
    "Conditions___| Number of colour images____| Number of depth images____| Size of data (GigaBytes) \n",
    "---|:---:|:---:|:---:\n",
    "Sunny | 500 | 500 | 3.43\n",
    "Snowy | 1000 | 1000 | 6.86\n",
    "Foggy_dark | 1100| 1100 | 7.55\n",
    "Blizzard | 3000 | 3000 | 20.5\n",
    "Rain_night | 2257 | 2257 | 15.4\n",
    "Total | 7857 | 7857 | 53.9 |\n",
    "Table 1. Characteristics of the data collected in moderate collection.\n",
    "\n",
    "## File structure\n",
    "Files are stored in folders with subfolders for Colour and Depth. Each colour file is named colour_0000x.raw where x is the frame number. For example, the range of file names in the ‘Sunny’ conditions are colour_00001.raw to colour_00500.raw and depth_00001.raw to depth_00500.raw.\n",
    "Data was collected in the order of Table 1, above. It follows that the first colour filenames in the Snowy collection are colour_00501.raw to colour_01500.raw and the same for depth.\n",
    "Images are stored to separate colour and depth directories and for each instance of new conditions, a new directory folder is made and used (Sunny/colour, Snowy/depth etc…). A full list of driving conditions along with file labels can be found under the ‘Conditions.txt’ in the base of the /Moderate_collection directory on [One Drive](https://livebournemouthac-my.sharepoint.com/:f:/g/personal/bsnow_bournemouth_ac_uk/EmXxIrfiQg1IlJkc18oqVbwBEm-4461czkd9OvFbvjH1UA?e=5YOjrU).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installs and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Collecting pickle-mixin\n  Downloading pickle-mixin-1.0.2.tar.gz (5.1 kB)\nBuilding wheels for collected packages: pickle-mixin\n  Building wheel for pickle-mixin (setup.py): started\n  Building wheel for pickle-mixin (setup.py): finished with status 'done'\n  Created wheel for pickle-mixin: filename=pickle_mixin-1.0.2-py3-none-any.whl size=6002 sha256=606dfa158d75027c849e08d00715d939094ca5e3cc814bccdd3aa0b99fe0412e\n  Stored in directory: c:\\users\\ben\\appdata\\local\\pip\\cache\\wheels\\2a\\a4\\6c\\83bfbc3b94f1bb43d634b07a6a893fd437a45c58b29aea5142\nSuccessfully built pickle-mixin\nInstalling collected packages: pickle-mixin\nSuccessfully installed pickle-mixin-1.0.2\n"
    }
   ],
   "source": [
    "!pip install pickle-mixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from Evaluation_procedure.eval_functions import isValid, get_depth, calc_errors, predict_and_gt, mean_and_std_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from Functions import import_raw_colour_image, import_raw_depth_image, show_depth_image, show_img\n",
    "import os\n",
    "from os import walk\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import math\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the csv data structure\n",
    "\n",
    "A csv containing the details of the Moderate Collection data was created, this file is read in to two variables, `folder_names` and `num_files`. These will be used to create a list of filenames from which will then be used to construct the dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('..\\data_descriptions.csv', newline='') as csvfile: ###### data_descriptions csv must be in this relative location\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    count = 0\n",
    "    for row in spamreader:\n",
    "        if count == 0:\n",
    "            folder_names = row ## store names of folders in the directory\n",
    "        else:\n",
    "            num_files = row ## store number of files within folder\n",
    "        count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(num_files)):\n",
    "    num_files[i] = int(num_files[i]) ## convert number of files from string to int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a list of filenames for use in the dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_numbers = [\"{0:05}\".format(i) for i in range(1, sum(num_files)+1)]\n",
    "colour_filenames = []\n",
    "depth_filenames = []\n",
    "for num in list_of_numbers:\n",
    "    colour_filenames.append(f\"colour_{num}.raw\")  ## append formatted number labels to file names\n",
    "    depth_filenames.append(f\"depth_{num}.raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class\n",
    "\n",
    "Due to having over 60GB of images in the Moderate Collection dataset it is impossible to simultaneously load all of them into the 16GB of available memory. As such a custom PyTorch dataset class will be created along with a data loader allowing for batches of n images to be loaded into memory at one time.\n",
    "\n",
    "The dataset class, named ModerateDataset, loads the images from a fixed directory. For this to work on a given computer, the ‘Path’ must be changed to the base directory of the Moderate Dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d2e09e3b7844>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mModerateDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrans_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfolder_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class ModerateDataset(Dataset):\n",
    "\n",
    "    def __init__(self, col_dir='', depth_dir='', transform=None, trans_on=False):\n",
    "        self.path_names = {}\n",
    "        for folder in folder_names:\n",
    "            self.path_names[f\"{folder}\"] = {}\n",
    "        for folder in folder_names:\n",
    "            self.path_names[f'{folder}']['colour'] = {}\n",
    "            self.path_names[f'{folder}']['depth'] = {}\n",
    "        for i in range(1, num_files[0]):\n",
    "            self.path_names['Sunny']['colour'][f\"{i}\"] = {}\n",
    "            self.path_names['Sunny']['depth'][f\"{i}\"] = {}\n",
    "        print(\"*************MAKE SURE THE PATH FILE IN THE FOR LOOP IS THE BASE IMAGE DIRECTORY ON YOUR COMPUTER**************\")\n",
    "        count = 0\n",
    "        for folder in folder_names:\n",
    "            for i in range(0, num_files[folder_names.index(folder)]):\n",
    "                self.path_names[f'{folder}']['colour'][f'{i+1}'] = Path(f\"C:/Users/Ben/OneDrive - Bournemouth University/Computer Vision/Moderate collection/{folder}/colour/{colour_filenames[count+i]}\")  ## Change this path here!!!!\n",
    "                self.path_names[f'{folder}']['depth'][f'{i+1}'] = Path(f\"C:/Users/Ben/OneDrive - Bournemouth University/Computer Vision/Moderate collection/{folder}/depth/{depth_filenames[count+i]}\")   ## Change this path here!!!!\n",
    "            count = count + num_files[folder_names.index(folder)]\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.col_dir = col_dir\n",
    "        self.depth_dir = depth_dir\n",
    "        self.trans_on = trans_on\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        if idx == 0:\n",
    "            \n",
    "            self.col_dir = self.path_names[f'{folder_names[0]}']['colour'][f'{idx+1}']\n",
    "            self.depth_dir = self.path_names[f'{folder_names[0]}']['depth'][f'{idx+1}']\n",
    "        \n",
    "        if (idx>0 and idx <= num_files[0]):  ## 1-500\n",
    "\n",
    "            self.col_dir = self.path_names[f'{folder_names[0]}']['colour'][f'{idx}']\n",
    "            self.depth_dir = self.path_names[f'{folder_names[0]}']['depth'][f'{idx}']\n",
    "\n",
    "        elif (idx > num_files[0] and idx < (sum(num_files[:2])+1)): ## 501 - 1500\n",
    "\n",
    "            self.col_dir = self.path_names[f'{folder_names[1]}']['colour'][f'{idx-num_files[0]}']\n",
    "            self.depth_dir = self.path_names[f'{folder_names[1]}']['depth'][f'{idx-num_files[0]}']\n",
    "\n",
    "        elif (idx > sum(num_files[:2]) and idx < (sum(num_files[:3])+1) ): ## 1501 - 2600\n",
    "\n",
    "            self.col_dir = self.path_names[f'{folder_names[2]}']['colour'][f'{idx-sum(num_files[:2])}'] # -1500\n",
    "            self.depth_dir = self.path_names[f'{folder_names[2]}']['depth'][f'{idx-sum(num_files[:2])}']\n",
    "\n",
    "        elif (idx > sum(num_files[:3]) and idx < (sum(num_files[:4])+1) ): ## 2601 - 5600\n",
    "\n",
    "            self.col_dir = self.path_names[f'{folder_names[3]}']['colour'][f'{idx-sum(num_files[:3])}'] #-2600\n",
    "            self.depth_dir = self.path_names[f'{folder_names[3]}']['depth'][f'{idx-sum(num_files[:3])}']\n",
    "            \n",
    "        elif (idx > sum(num_files[:4]) and idx < (sum(num_files[:5])+1) ): ## 5601 - 7857\n",
    "\n",
    "            self.col_dir = self.path_names[f'{folder_names[4]}']['colour'][f'{idx-sum(num_files[:4])}'] # -5600\n",
    "            self.depth_dir = self.path_names[f'{folder_names[4]}']['depth'][f'{idx-sum(num_files[:4])}']\n",
    "\n",
    "        elif (idx > sum(num_files)):\n",
    "            raise NameError('Index outside of range')\n",
    "\n",
    "        col_img = import_raw_colour_image(self.col_dir)\n",
    "        depth_img = import_raw_depth_image(self.depth_dir)\n",
    "        if self.trans_on == True:\n",
    "            col_img = torch.from_numpy(np.flip(col_img,axis=0).copy()) # apply any transforms\n",
    "            depth_img = torch.from_numpy(np.flip(depth_img,axis=0).copy()) # apply any transforms\n",
    "            col_img = col_img.transpose(0,2)\n",
    "            col_img = col_img.transpose(1,2)\n",
    "        if self.transform: # if any transforms were given to initialiser\n",
    "            col_img = self.transform(col_img) # apply any transforms\n",
    "        return col_img, depth_img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum(num_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an instance of the dataset in order to create training, validation and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "*************MAKE SURE THE PATH FILE IN THE FOR LOOP IS THE BASE IMAGE DIRECTORY ON YOUR COMPUTER**************\n"
    }
   ],
   "source": [
    "total_Data = ModerateDataset(trans_on=True)  ## instancing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, validation and test splitting\n",
    "\n",
    "It is of vital importance to establish the separation of three datasets: training, validation and testing. Training data is used to train the neural network model and validation data is used to check that the model is not overfitting to the training data. Testing data is used to check the performance of the trained model on unseen data to evaluate performance with a set of predefined metrics (defined in the evaluation procedure section).\n",
    "\n",
    "A train, validation, testing split of 80/10/10 has been used to create three datasets: train_dataset, val_dataset and test_dataset. This split is commonly used in machine learning research. These datasets all inherit from the ModerateDasaset class. For each of these datasets, a data loader was created to load in a batch of images at once instead of loading the entire dataset to memory. To train the model, the training and validation dataloaders are used. This ensures that no testing data is used in any step of training the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(total_Data))\n",
    "val_size = int((len(total_Data) - train_size)/2)\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(total_Data, [train_size, val_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 16\n",
    "tr_dl  = DataLoader(train_dataset,  batch_size=batch_sz, shuffle=True,  num_workers=0)\n",
    "val_dl = DataLoader(val_dataset,  batch_size=batch_sz, shuffle=True,  num_workers=0)\n",
    "test_dl = DataLoader(test_dataset,  batch_size=batch_sz, shuffle=True,  num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN model\n",
    "\n",
    "One of the key deliverables in the project proposal is to create a simple neural network architecture that uses an RGB image as an input outputs a depth image. This is realised below by the use of a convolutional neural network. The network, referred to as the 'Simple CNN' model, is defined below. It consists of two convolutional and two deconvolutional layers. A 3 channel colour image is input into the first conv layer, this increases the number of images channels to 6. After this, a rectified linear unit activation function is applied to the convolved data. The image is then passed through another conv layer increasing the channels to 12. This then leads to two deconv layers, outputting a singular channeled depth image with the same resolution as the imput image, this is ensured by the use of the kernal size, stride, and padding variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3,  out_channels=6, kernel_size=3, stride=1, padding=1), \n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=6, out_channels=12, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose2d(in_channels = 12, out_channels=6, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose2d(in_channels = 6, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU()\n",
    ").cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a model summary to show the structure of the network and setting the image dimensions to (3,720,1280) and the batch size to 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(net, (3,720,1280), 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Here, a fitting function is defined in which a neural network, training dataloader and validation dataloader are passed. The user can modify the loss function, number of training epochs, learning rate and weight decay used to train the network.\n",
    "\n",
    "The fitting/training function used to train the model is defined by the following for the SimpleCNN:\n",
    "- Loss function: Mean square error loss\n",
    "- Epochs: 2\n",
    "- Learning rate: 1x10-3\n",
    "- Weight decay: 1x10-3\n",
    "- Optimiser: Adam\n",
    "- Training batch size = 16\n",
    "- Validation batch size = 16\n",
    "- Shuffling: Training=True, Validation=False\n",
    "- Metrics that are tracked: Training loss, validation loss, validation accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(net, tr_dl, val_dl, loss=nn.MSELoss(), epochs=3, lr=3e-3, wd=1e-3):   \n",
    "\n",
    "    Ltr_hist, Lval_hist = [], []    \n",
    "    opt = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "    for epoch in trange(epochs):\n",
    "        \n",
    "        L = []\n",
    "        dl = (iter(tr_dl))\n",
    "        count_train = 0\n",
    "        for xb, yb in tqdm(dl, leave=False):\n",
    "            xb, yb = xb.float(), yb.float()\n",
    "            xb, yb = xb.cuda(), yb.cuda()\n",
    "            y_ = net(xb)\n",
    "            l = loss(y_, yb)\n",
    "            opt.zero_grad()\n",
    "            l.backward()\n",
    "            opt.step()\n",
    "            L.append(l.detach().cpu().numpy())\n",
    "            print(f\"Training on batch {count_train} of {int(train_size/batch_sz)}\")\n",
    "            count_train+= 1\n",
    "\n",
    "        # disable gradient calculations for validation\n",
    "        for p in net.parameters(): p.requires_grad = False\n",
    "\n",
    "        Lval, Aval = [], []\n",
    "        val_it = iter(val_dl)\n",
    "        val_count = 0\n",
    "        for xb, yb in tqdm(val_it, leave=False):\n",
    "            xb, yb = xb.float(), yb.float()\n",
    "            xb, yb = xb.cuda(), yb.cuda()\n",
    "            y_ = net(xb)\n",
    "            l = loss(y_, yb)\n",
    "            Lval.append(l.detach().cpu().numpy())\n",
    "            Aval.append((y_.max(dim=1)[1] == yb).float().mean().cpu().numpy())\n",
    "            print(f\"Validating on batch {val_count} of {int(val_size/batch_sz)}\")\n",
    "            val_count+= 1\n",
    "\n",
    "        # enable gradient calculations for next epoch \n",
    "        for p in net.parameters(): p.requires_grad = True \n",
    "            \n",
    "        Ltr_hist.append(np.mean(L))\n",
    "        Lval_hist.append(np.mean(Lval))\n",
    "        print(f'training loss: {np.mean(L):0.4f}\\tvalidation loss: {np.mean(Lval):0.4f}\\tvalidation accuracy: {np.mean(Aval):0.2f}')\n",
    "    return Ltr_hist, Lval_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "\n",
    "For the SimpleCNN it was not imperative to train the model for a large number of epochs since the goal was to produce data that could be used to test the evaluation procedure.\n",
    "Due to the large amount of training data, training on a Nvidia 2070 Max Q 8GB and an i7-8750H with 16GB ram takes around 30 minutes per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#Ltr_hist, Lval_hist = fit(net.cuda(), tr_dl, val_dl, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and saving the trained model\n",
    "\n",
    "To save time re-training a model from scratch, the model is saved and can be reloaded without needing retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'model_20042020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_load_trained_model = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Sequential(\n  (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU()\n  (2): Conv2d(6, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (3): ReLU()\n  (4): ConvTranspose2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (5): ReLU()\n  (6): ConvTranspose2d(6, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (7): ReLU()\n)"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "re_load_trained_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "\n",
    "## Motivation for evaluation\n",
    "After the models have been trained it is important to understand how well they perform at the task of predicting a depth map from an RGB image. To do this, a portion of the Moderate Collection data was set aside as a testing dataset. This test dataset contains 786 RGB/Depth image pairs that the models have not been exposed to. These image pairs, therefore, can be used to evaluate how well the trained models perform on unseen data from the same overall dataset.\n",
    "\n",
    "\n",
    "\n",
    "Each RGB image will be passed through the models and a predicted depth map will be produced. This depth map will then be compared to the ground truth depth map associated with it. There are numerous ways in which to compare these depth maps. A simple method could be showing an observer both depth maps and the RGB image and asking them which they think is best and where in the image the predicted depth map lacks clarity. This process takes a large amount of time per depth map, on the order of a minute, and the results generated are qualitative and not quantitative. This can be done for a few images to get a small insight into the appearance of the predicted depth maps. To evaluate all 786 RGB/Depth maps, a quantitative approach is needed.\n",
    "\n",
    "\n",
    "\n",
    "One way of comparing one depth map to another is to compare them pixel by pixel. The difference between predicted depth and ground truth can be found for each pixel and the total difference can be calculated by summing these individual differences. An average difference from one depth map to the other can then be calculated by dividing the total summed differences and dividing by the number of pixels in the depth map. This is known as the mean difference error. A pitfall of this error is that negative differences in depth can cancel out positive differences leading to an unreliable result. To negate this effect, we will take the absolute value of the difference for each comparison. This will ensure that the total difference calculated is positive and that the errors correctly compound. This is known as the MAE-Mean Absolute Error and is defined in the ‘Error metrics’ list under ‘Evaluation procedure’ below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2ceb2981df984a0382f5cd670fcb21c9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ""
    }
   ],
   "source": [
    "simple_predictions, simple_gts = predict_and_gt(val_dl, val_size, batch_sz, re_load_trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise a dictionary for holding the calculated errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_dictionary = {}\n",
    "chunk_size = val_size//batch_sz\n",
    "for i in range(chunk_size+1):\n",
    "        error_dictionary[f\"{i}\"] = {}\n",
    "for i in range(chunk_size+1):\n",
    "    for j in range(batch_sz):\n",
    "        error_dictionary[f\"{i}\"][f\"{j}\"] = {}\n",
    "# error_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Calculating errors for batch 0 of 49\nCalculating errors for batch 1 of 49\nCalculating errors for batch 2 of 49\nCalculating errors for batch 3 of 49\nCalculating errors for batch 4 of 49\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Ben\\Documents\\EngD 1st year\\Computer Vision\\Group project\\Github\\depth_estimation\\Evaluation_procedure\\eval_functions.py\u001b[0m in \u001b[0;36mcalc_errors\u001b[1;34m(pred_depth, grndt_depth)\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[0mdepth_of_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_depth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_depth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[0mdepth_of_gt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_depth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrndt_depth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0misValid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdepth_of_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdepth_of_gt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m          \u001b[1;31m# check non negative depth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                     \u001b[1;31m# calculate the absolute difference between depth maps for a given pixel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Ben\\Documents\\EngD 1st year\\Computer Vision\\Group project\\Github\\depth_estimation\\Evaluation_procedure\\eval_functions.py\u001b[0m in \u001b[0;36misValid\u001b[1;34m(depth_1, depth_2)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0misValid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdepth_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdepth_1\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdepth_2\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mMyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mException\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(chunk_size-1):\n",
    "    for j in range(batch_sz-1): # batch_sz):\n",
    "        error_dictionary[f\"{i}\"][f\"{j}\"] = calc_errors(simple_predictions[i][j], simple_gts[i][j])\n",
    "    print(f\"Calculating errors for batch {i} of {int(chunk_size)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate average errors over test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'numpy_depth_prediction' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-be1e6416f43c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mnumpy_depth_prediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m49\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'numpy_depth_prediction' is not defined"
     ]
    }
   ],
   "source": [
    "(numpy_depth_prediction[49][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 0 ns\n"
    }
   ],
   "source": [
    "%%time\n",
    "# initialisation of average errors\n",
    "difference_err_avg = 0\n",
    "sqr_diff_err_avg = 0\n",
    "inv_err_avg = 0\n",
    "inv_sqr_err_avg = 0\n",
    "log_err_avg = 0\n",
    "log_sqr_err_avg = 0\n",
    "log_non_abs_err_avg = 0\n",
    "abs_rel_err_avg = 0\n",
    "sqr_rel_err_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 2.44 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(0, int(val_size/batch_sz)):\n",
    "    for j in range(0, batch_sz-1):\n",
    "        difference_err_avg += error_dictionary[f\"{i}\"][f\"{j}\"][0]\n",
    "        sqr_diff_err_avg += error_dictionary[f\"{i}\"][f\"{j}\"][1]\n",
    "        inv_err_avg += error_dictionary[f\"{i}\"][f\"{j}\"][2]\n",
    "        inv_sqr_err_avg += error_dictionary[f\"{i}\"][f\"{j}\"][3]\n",
    "        log_err_avg += error_dictionary[f\"{i}\"][f\"{j}\"][4]\n",
    "        log_sqr_err_avg += error_dictionary[f\"{i}\"][f\"{j}\"][5]\n",
    "        log_non_abs_err_avg += error_dictionary[f\"{i}\"][f\"{j}\"][6]\n",
    "        abs_rel_err_avg += error_dictionary[f\"{i}\"][f\"{j}\"][7]\n",
    "        sqr_rel_err_avg += error_dictionary[f\"{i}\"][f\"{j}\"][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.15349015485873604 0.20413706169302126 216.92079890689172 449.87569844827283 1.4395006203028675 1.6912679620310802 1.0779036028588291 17.408615003311972 57917.195704816244\n"
    }
   ],
   "source": [
    "## divide by number of images to get average error\n",
    "difference_err_avg /= (val_size)\n",
    "sqr_diff_err_avg /= (val_size)\n",
    "inv_err_avg /= (val_size)\n",
    "inv_sqr_err_avg /= (val_size)\n",
    "log_err_avg /= (val_size)\n",
    "log_sqr_err_avg /= (val_size)\n",
    "log_non_abs_err_avg /= (val_size)\n",
    "abs_rel_err_avg /= (val_size)\n",
    "sqr_rel_err_avg /= (val_size)\n",
    "print(difference_err_avg, sqr_diff_err_avg, inv_err_avg, inv_sqr_err_avg, log_err_avg, log_sqr_err_avg, log_non_abs_err_avg, abs_rel_err_avg, sqr_rel_err_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate standard deviation in average errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise difference counters\n",
    "difference_err_count = 0\n",
    "sqr_diff_err_count = 0\n",
    "inv_err_count = 0\n",
    "inv_sqr_err_count = 0\n",
    "log_err_count = 0\n",
    "log_sqr_err_count = 0\n",
    "log_non_abs_err_count = 0\n",
    "abs_rel_err_count = 0\n",
    "sqr_rel_err_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 6.97 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "# sum squared differences\n",
    "for i in range(0, int(val_size/batch_sz)):\n",
    "    for j in range(0, batch_sz-1):\n",
    "            difference_err_count += (error_dictionary[f\"{i}\"][f\"{j}\"][0] - difference_err_avg)**2\n",
    "            sqr_diff_err_count += (error_dictionary[f\"{i}\"][f\"{j}\"][0] - sqr_diff_err_avg)**2\n",
    "            inv_err_count += (error_dictionary[f\"{i}\"][f\"{j}\"][0] - inv_err_avg)**2\n",
    "            inv_sqr_err_count += (error_dictionary[f\"{i}\"][f\"{j}\"][0] - inv_sqr_err_avg)**2\n",
    "            log_err_count += (error_dictionary[f\"{i}\"][f\"{j}\"][0] - log_err_avg)**2\n",
    "            log_sqr_err_count += (error_dictionary[f\"{i}\"][f\"{j}\"][0] - log_sqr_err_avg)**2\n",
    "            log_non_abs_err_count += (error_dictionary[f\"{i}\"][f\"{j}\"][0] - log_non_abs_err_avg)**2\n",
    "            abs_rel_err_count += (error_dictionary[f\"{i}\"][f\"{j}\"][0] - abs_rel_err_avg)**2\n",
    "            sqr_rel_err_count += (error_dictionary[f\"{i}\"][f\"{j}\"][0] - sqr_rel_err_avg)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide by number of test images\n",
    "difference_err_count /= val_size\n",
    "sqr_diff_err_count /= val_size\n",
    "inv_err_count /= val_size\n",
    "inv_sqr_err_count /= val_size\n",
    "log_err_count /= val_size\n",
    "log_sqr_err_count /= val_size\n",
    "log_non_abs_err_count /= val_size\n",
    "abs_rel_err_count /= val_size\n",
    "sqr_rel_err_count /= val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.0031852591413042137 0.003451686762223687 7.476418605301046 15.51154946000541 0.04410363930439457 0.05276895537876926 0.031676132664665964 0.594808591100741 1997.6868841917874\n"
    }
   ],
   "source": [
    "# square root\n",
    "difference_err_sigma = math.sqrt(difference_err_count)\n",
    "sqr_diff_err_sigma = math.sqrt(sqr_diff_err_count)\n",
    "inv_err_sigma = math.sqrt(inv_err_count)\n",
    "inv_sqr_err_sigma = math.sqrt(inv_sqr_err_count)\n",
    "log_err_sigma = math.sqrt(log_err_count)\n",
    "log_sqr_err_sigma = math.sqrt(log_sqr_err_count)\n",
    "log_non_abs_err_sigma = math.sqrt(log_non_abs_err_count)\n",
    "abs_rel_err_sigma = math.sqrt(abs_rel_err_count)\n",
    "sqr_rel_err_sigma = math.sqrt(sqr_rel_err_count)\n",
    "print(difference_err_sigma, sqr_diff_err_sigma, inv_err_sigma, inv_sqr_err_sigma, log_err_sigma, log_sqr_err_sigma, log_non_abs_err_sigma, abs_rel_err_sigma, sqr_rel_err_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_errors = [difference_err_avg, sqr_diff_err_avg, inv_err_avg, inv_sqr_err_avg, log_err_avg, log_sqr_err_avg, log_non_abs_err_avg, abs_rel_err_avg, sqr_rel_err_avg]\n",
    "std_devs = [difference_err_sigma, sqr_diff_err_sigma, inv_err_sigma, inv_sqr_err_sigma, log_err_sigma, log_sqr_err_sigma, log_non_abs_err_sigma, abs_rel_err_sigma, sqr_rel_err_sigma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Err = 0.153 \t +-\t0.003\nErr = 0.204 \t +-\t0.003\nErr = 216.921 \t +-\t7.476\nErr = 449.876 \t +-\t15.512\nErr = 1.440 \t +-\t0.044\nErr = 1.691 \t +-\t0.053\nErr = 1.078 \t +-\t0.032\nErr = 17.409 \t +-\t0.595\nErr = 57917.196 \t +-\t1997.687\n"
    }
   ],
   "source": [
    "for mean, std in zip(mean_errors, std_devs):\n",
    "    print(\"Err = \"\"{:.3f}\".format(mean), \"\\t +-\\t\"\"{:.3f}\".format(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef3aa90c31a943ad92ae5af843c5490d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ""
    }
   ],
   "source": [
    "val_preds, val_gts = predict_and_gt(val_dl, val_size, batch_sz, re_load_trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e1478040981b4d8ba65a6ac53946ee6a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ""
    }
   ],
   "source": [
    "test_preds, test_gts = predict_and_gt(test_dl, val_size, batch_sz, re_load_trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for re-loading saved variables\n",
    "# f = open('test_gts.pckl', 'wb')\n",
    "# pickle.dump(test_gts, f)\n",
    "# f.close()\n",
    "path_saved = 'C:/Users/Ben/OneDrive - Bournemouth University/Computer Vision/Datasets/Saved_preds/test_gts.pckl'\n",
    "f = open(path_saved, 'rb')\n",
    "test_gts = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_saved = 'C:/Users/Ben/OneDrive - Bournemouth University/Computer Vision/Datasets/Saved_preds/test_preds.pckl'\n",
    "f = open(path_saved, 'rb')\n",
    "test_preds = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_means, test_stds = mean_and_std_errors(test_preds, test_gts, val_size, batch_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0.15292886050578838,\n 0.20256076692918262,\n 169.7575138927575,\n 337.4968641438327,\n 1.3821823514353018,\n 1.6306671019519026,\n 1.0416930808944187,\n 14.377977729056317,\n 35337.66780938274]"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "test_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0.08957746447381942,\n 0.09489953776467451,\n 162.3142804656832,\n 322.856735180481,\n 1.1664623689199738,\n 1.4037154706957042,\n 0.8418862470761609,\n 13.60159222443305,\n 33821.33976314626]"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "test_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_saved = 'C:/Users/Ben/OneDrive - Bournemouth University/Computer Vision/Datasets/Saved_preds/val_preds.pckl'\n",
    "f = open(path_saved, 'rb')\n",
    "val_preds = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_saved = 'C:/Users/Ben/OneDrive - Bournemouth University/Computer Vision/Datasets/Saved_preds/val_gts.pckl'\n",
    "f = open(path_saved, 'rb')\n",
    "val_gts = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_means, val_stds = mean_and_std_errors(val_preds, val_gts, val_size, batch_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0.1523476418379662,\n 0.20036514733386704,\n 206.6017020603569,\n 403.54414631860675,\n 1.4401937428260463,\n 1.6913869644066872,\n 1.0672831635531381,\n 15.333102927912666,\n 12584.9235118754]"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "val_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0.09106520633516951,\n 0.09579359667519721,\n 197.57826652032824,\n 386.07085383566994,\n 1.222547923855846,\n 1.4624174598570419,\n 0.867006191162321,\n 14.516336031463855,\n 12044.806188680475]"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "val_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_saved = 'C:/Users/Ben/OneDrive - Bournemouth University/Computer Vision/Datasets/Saved_preds/val_stds.pckl'\n",
    "f = open(path_saved, 'wb')\n",
    "pickle.dump(val_stds, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train dataloader is called tr_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Whirrrrrr calculating..... 0 of 392.8125\nWhirrrrrr calculating..... 1 of 392.8125\nWhirrrrrr calculating..... 2 of 392.8125\nWhirrrrrr calculating..... 3 of 392.8125\nWhirrrrrr calculating..... 4 of 392.8125\nWhirrrrrr calculating..... 5 of 392.8125\nWhirrrrrr calculating..... 6 of 392.8125\nWhirrrrrr calculating..... 7 of 392.8125\nWhirrrrrr calculating..... 8 of 392.8125\nWhirrrrrr calculating..... 9 of 392.8125\nWhirrrrrr calculating..... 10 of 392.8125\nWhirrrrrr calculating..... 11 of 392.8125\nWhirrrrrr calculating..... 12 of 392.8125\nWhirrrrrr calculating..... 13 of 392.8125\nWhirrrrrr calculating..... 14 of 392.8125\nWhirrrrrr calculating..... 15 of 392.8125\nWhirrrrrr calculating..... 16 of 392.8125\nWhirrrrrr calculating..... 17 of 392.8125\nWhirrrrrr calculating..... 18 of 392.8125\nWhirrrrrr calculating..... 19 of 392.8125\nWhirrrrrr calculating..... 20 of 392.8125\nWhirrrrrr calculating..... 21 of 392.8125\nWhirrrrrr calculating..... 22 of 392.8125\nWhirrrrrr calculating..... 23 of 392.8125\nWhirrrrrr calculating..... 24 of 392.8125\nWhirrrrrr calculating..... 25 of 392.8125\nWhirrrrrr calculating..... 26 of 392.8125\nWhirrrrrr calculating..... 27 of 392.8125\nWhirrrrrr calculating..... 28 of 392.8125\nWhirrrrrr calculating..... 29 of 392.8125\nWhirrrrrr calculating..... 30 of 392.8125\nWhirrrrrr calculating..... 31 of 392.8125\nWhirrrrrr calculating..... 32 of 392.8125\nWhirrrrrr calculating..... 33 of 392.8125\nWhirrrrrr calculating..... 34 of 392.8125\nWhirrrrrr calculating..... 35 of 392.8125\nWhirrrrrr calculating..... 36 of 392.8125\nWhirrrrrr calculating..... 37 of 392.8125\nWhirrrrrr calculating..... 38 of 392.8125\nWhirrrrrr calculating..... 39 of 392.8125\nWhirrrrrr calculating..... 40 of 392.8125\nWhirrrrrr calculating..... 41 of 392.8125\nWhirrrrrr calculating..... 42 of 392.8125\nWhirrrrrr calculating..... 43 of 392.8125\nWhirrrrrr calculating..... 44 of 392.8125\nWhirrrrrr calculating..... 45 of 392.8125\nWhirrrrrr calculating..... 46 of 392.8125\nWhirrrrrr calculating..... 47 of 392.8125\nWhirrrrrr calculating..... 48 of 392.8125\nWhirrrrrr calculating..... 49 of 392.8125\nWhirrrrrr calculating..... 50 of 392.8125\nWhirrrrrr calculating..... 51 of 392.8125\nWhirrrrrr calculating..... 52 of 392.8125\nWhirrrrrr calculating..... 53 of 392.8125\nWhirrrrrr calculating..... 54 of 392.8125\nWhirrrrrr calculating..... 55 of 392.8125\nWhirrrrrr calculating..... 56 of 392.8125\nWhirrrrrr calculating..... 57 of 392.8125\nWhirrrrrr calculating..... 58 of 392.8125\nWhirrrrrr calculating..... 59 of 392.8125\nWhirrrrrr calculating..... 60 of 392.8125\nWhirrrrrr calculating..... 61 of 392.8125\nWhirrrrrr calculating..... 62 of 392.8125\nWhirrrrrr calculating..... 63 of 392.8125\nWhirrrrrr calculating..... 64 of 392.8125\nWhirrrrrr calculating..... 65 of 392.8125\nWhirrrrrr calculating..... 66 of 392.8125\nWhirrrrrr calculating..... 67 of 392.8125\nWhirrrrrr calculating..... 68 of 392.8125\nWhirrrrrr calculating..... 69 of 392.8125\nWhirrrrrr calculating..... 70 of 392.8125\nWhirrrrrr calculating..... 71 of 392.8125\nWhirrrrrr calculating..... 72 of 392.8125\nWhirrrrrr calculating..... 73 of 392.8125\nWhirrrrrr calculating..... 74 of 392.8125\nWhirrrrrr calculating..... 75 of 392.8125\nWhirrrrrr calculating..... 76 of 392.8125\nWhirrrrrr calculating..... 77 of 392.8125\nWhirrrrrr calculating..... 78 of 392.8125\nWhirrrrrr calculating..... 79 of 392.8125\nWhirrrrrr calculating..... 80 of 392.8125\nWhirrrrrr calculating..... 81 of 392.8125\nWhirrrrrr calculating..... 82 of 392.8125\nWhirrrrrr calculating..... 83 of 392.8125\nWhirrrrrr calculating..... 84 of 392.8125\nWhirrrrrr calculating..... 85 of 392.8125\nWhirrrrrr calculating..... 86 of 392.8125\nWhirrrrrr calculating..... 87 of 392.8125\nWhirrrrrr calculating..... 88 of 392.8125\nWhirrrrrr calculating..... 89 of 392.8125\nWhirrrrrr calculating..... 90 of 392.8125\nWhirrrrrr calculating..... 91 of 392.8125\nWhirrrrrr calculating..... 92 of 392.8125\nWhirrrrrr calculating..... 93 of 392.8125\nWhirrrrrr calculating..... 94 of 392.8125\nWhirrrrrr calculating..... 95 of 392.8125\nWhirrrrrr calculating..... 96 of 392.8125\nWhirrrrrr calculating..... 97 of 392.8125\nWhirrrrrr calculating..... 98 of 392.8125\nWhirrrrrr calculating..... 99 of 392.8125\nWhirrrrrr calculating..... 100 of 392.8125\nWhirrrrrr calculating..... 101 of 392.8125\nWhirrrrrr calculating..... 102 of 392.8125\nWhirrrrrr calculating..... 103 of 392.8125\nWhirrrrrr calculating..... 104 of 392.8125\nWhirrrrrr calculating..... 105 of 392.8125\nWhirrrrrr calculating..... 106 of 392.8125\nWhirrrrrr calculating..... 107 of 392.8125\nWhirrrrrr calculating..... 108 of 392.8125\nWhirrrrrr calculating..... 109 of 392.8125\nWhirrrrrr calculating..... 110 of 392.8125\nWhirrrrrr calculating..... 111 of 392.8125\nWhirrrrrr calculating..... 112 of 392.8125\nWhirrrrrr calculating..... 113 of 392.8125\nWhirrrrrr calculating..... 114 of 392.8125\nWhirrrrrr calculating..... 115 of 392.8125\nWhirrrrrr calculating..... 116 of 392.8125\nWhirrrrrr calculating..... 117 of 392.8125\nWhirrrrrr calculating..... 118 of 392.8125\nWhirrrrrr calculating..... 119 of 392.8125\nWhirrrrrr calculating..... 120 of 392.8125\nWhirrrrrr calculating..... 121 of 392.8125\nWhirrrrrr calculating..... 122 of 392.8125\nWhirrrrrr calculating..... 123 of 392.8125\nWhirrrrrr calculating..... 124 of 392.8125\nWhirrrrrr calculating..... 125 of 392.8125\nWhirrrrrr calculating..... 126 of 392.8125\nWhirrrrrr calculating..... 127 of 392.8125\nWhirrrrrr calculating..... 128 of 392.8125\nWhirrrrrr calculating..... 129 of 392.8125\nWhirrrrrr calculating..... 130 of 392.8125\nWhirrrrrr calculating..... 131 of 392.8125\nWhirrrrrr calculating..... 132 of 392.8125\nWhirrrrrr calculating..... 133 of 392.8125\nWhirrrrrr calculating..... 134 of 392.8125\nWhirrrrrr calculating..... 135 of 392.8125\nWhirrrrrr calculating..... 136 of 392.8125\nWhirrrrrr calculating..... 137 of 392.8125\nWhirrrrrr calculating..... 138 of 392.8125\nWhirrrrrr calculating..... 139 of 392.8125\nWhirrrrrr calculating..... 140 of 392.8125\nWhirrrrrr calculating..... 141 of 392.8125\nWhirrrrrr calculating..... 142 of 392.8125\nWhirrrrrr calculating..... 143 of 392.8125\nWhirrrrrr calculating..... 144 of 392.8125\nWhirrrrrr calculating..... 145 of 392.8125\nWhirrrrrr calculating..... 146 of 392.8125\nWhirrrrrr calculating..... 147 of 392.8125\nWhirrrrrr calculating..... 148 of 392.8125\nWhirrrrrr calculating..... 149 of 392.8125\nWhirrrrrr calculating..... 150 of 392.8125\nWhirrrrrr calculating..... 151 of 392.8125\nWhirrrrrr calculating..... 152 of 392.8125\nWhirrrrrr calculating..... 153 of 392.8125\nWhirrrrrr calculating..... 154 of 392.8125\nWhirrrrrr calculating..... 155 of 392.8125\nWhirrrrrr calculating..... 156 of 392.8125\nWhirrrrrr calculating..... 157 of 392.8125\nWhirrrrrr calculating..... 158 of 392.8125\nWhirrrrrr calculating..... 159 of 392.8125\nWhirrrrrr calculating..... 160 of 392.8125\nWhirrrrrr calculating..... 161 of 392.8125\nWhirrrrrr calculating..... 162 of 392.8125\nWhirrrrrr calculating..... 163 of 392.8125\nWhirrrrrr calculating..... 164 of 392.8125\nWhirrrrrr calculating..... 165 of 392.8125\nWhirrrrrr calculating..... 166 of 392.8125\nWhirrrrrr calculating..... 167 of 392.8125\nWhirrrrrr calculating..... 168 of 392.8125\nWhirrrrrr calculating..... 169 of 392.8125\nWhirrrrrr calculating..... 170 of 392.8125\nWhirrrrrr calculating..... 171 of 392.8125\nWhirrrrrr calculating..... 172 of 392.8125\nWhirrrrrr calculating..... 173 of 392.8125\nWhirrrrrr calculating..... 174 of 392.8125\nWhirrrrrr calculating..... 175 of 392.8125\nWhirrrrrr calculating..... 176 of 392.8125\nWhirrrrrr calculating..... 177 of 392.8125\nWhirrrrrr calculating..... 178 of 392.8125\nWhirrrrrr calculating..... 179 of 392.8125\nWhirrrrrr calculating..... 180 of 392.8125\nWhirrrrrr calculating..... 181 of 392.8125\nWhirrrrrr calculating..... 182 of 392.8125\nWhirrrrrr calculating..... 183 of 392.8125\nWhirrrrrr calculating..... 184 of 392.8125\nWhirrrrrr calculating..... 185 of 392.8125\nWhirrrrrr calculating..... 186 of 392.8125\nWhirrrrrr calculating..... 187 of 392.8125\nWhirrrrrr calculating..... 188 of 392.8125\nWhirrrrrr calculating..... 189 of 392.8125\nWhirrrrrr calculating..... 190 of 392.8125\nWhirrrrrr calculating..... 191 of 392.8125\nWhirrrrrr calculating..... 192 of 392.8125\nWhirrrrrr calculating..... 193 of 392.8125\nWhirrrrrr calculating..... 194 of 392.8125\nWhirrrrrr calculating..... 195 of 392.8125\nWhirrrrrr calculating..... 196 of 392.8125\nWhirrrrrr calculating..... 197 of 392.8125\nWhirrrrrr calculating..... 198 of 392.8125\nWhirrrrrr calculating..... 199 of 392.8125\nWhirrrrrr calculating..... 200 of 392.8125\nWhirrrrrr calculating..... 201 of 392.8125\nWhirrrrrr calculating..... 202 of 392.8125\nWhirrrrrr calculating..... 203 of 392.8125\nWhirrrrrr calculating..... 204 of 392.8125\nWhirrrrrr calculating..... 205 of 392.8125\nWhirrrrrr calculating..... 206 of 392.8125\nWhirrrrrr calculating..... 207 of 392.8125\nWhirrrrrr calculating..... 208 of 392.8125\nWhirrrrrr calculating..... 209 of 392.8125\nWhirrrrrr calculating..... 210 of 392.8125\nWhirrrrrr calculating..... 211 of 392.8125\nWhirrrrrr calculating..... 212 of 392.8125\nWhirrrrrr calculating..... 213 of 392.8125\nWhirrrrrr calculating..... 214 of 392.8125\nWhirrrrrr calculating..... 215 of 392.8125\nWhirrrrrr calculating..... 216 of 392.8125\nWhirrrrrr calculating..... 217 of 392.8125\nWhirrrrrr calculating..... 218 of 392.8125\nWhirrrrrr calculating..... 219 of 392.8125\nWhirrrrrr calculating..... 220 of 392.8125\nWhirrrrrr calculating..... 221 of 392.8125\nWhirrrrrr calculating..... 222 of 392.8125\nWhirrrrrr calculating..... 223 of 392.8125\nWhirrrrrr calculating..... 224 of 392.8125\nWhirrrrrr calculating..... 225 of 392.8125\nWhirrrrrr calculating..... 226 of 392.8125\nWhirrrrrr calculating..... 227 of 392.8125\nWhirrrrrr calculating..... 228 of 392.8125\nWhirrrrrr calculating..... 229 of 392.8125\nWhirrrrrr calculating..... 230 of 392.8125\nWhirrrrrr calculating..... 231 of 392.8125\nWhirrrrrr calculating..... 232 of 392.8125\nWhirrrrrr calculating..... 233 of 392.8125\nWhirrrrrr calculating..... 234 of 392.8125\nWhirrrrrr calculating..... 235 of 392.8125\nWhirrrrrr calculating..... 236 of 392.8125\nWhirrrrrr calculating..... 237 of 392.8125\nWhirrrrrr calculating..... 238 of 392.8125\nWhirrrrrr calculating..... 239 of 392.8125\nWhirrrrrr calculating..... 240 of 392.8125\nWhirrrrrr calculating..... 241 of 392.8125\nWhirrrrrr calculating..... 242 of 392.8125\nWhirrrrrr calculating..... 243 of 392.8125\nWhirrrrrr calculating..... 244 of 392.8125\nWhirrrrrr calculating..... 245 of 392.8125\nWhirrrrrr calculating..... 246 of 392.8125\nWhirrrrrr calculating..... 247 of 392.8125\nWhirrrrrr calculating..... 248 of 392.8125\nWhirrrrrr calculating..... 249 of 392.8125\nWhirrrrrr calculating..... 250 of 392.8125\nWhirrrrrr calculating..... 251 of 392.8125\nWhirrrrrr calculating..... 252 of 392.8125\nWhirrrrrr calculating..... 253 of 392.8125\nWhirrrrrr calculating..... 254 of 392.8125\nWhirrrrrr calculating..... 255 of 392.8125\nWhirrrrrr calculating..... 256 of 392.8125\nWhirrrrrr calculating..... 257 of 392.8125\nWhirrrrrr calculating..... 258 of 392.8125\nWhirrrrrr calculating..... 259 of 392.8125\nWhirrrrrr calculating..... 260 of 392.8125\nWhirrrrrr calculating..... 261 of 392.8125\nWhirrrrrr calculating..... 262 of 392.8125\nWhirrrrrr calculating..... 263 of 392.8125\nWhirrrrrr calculating..... 264 of 392.8125\nWhirrrrrr calculating..... 265 of 392.8125\nWhirrrrrr calculating..... 266 of 392.8125\nWhirrrrrr calculating..... 267 of 392.8125\nWhirrrrrr calculating..... 268 of 392.8125\nWhirrrrrr calculating..... 269 of 392.8125\nWhirrrrrr calculating..... 270 of 392.8125\nWhirrrrrr calculating..... 271 of 392.8125\nWhirrrrrr calculating..... 272 of 392.8125\nWhirrrrrr calculating..... 273 of 392.8125\nWhirrrrrr calculating..... 274 of 392.8125\nWhirrrrrr calculating..... 275 of 392.8125\nWhirrrrrr calculating..... 276 of 392.8125\nWhirrrrrr calculating..... 277 of 392.8125\nWhirrrrrr calculating..... 278 of 392.8125\nWhirrrrrr calculating..... 279 of 392.8125\nWhirrrrrr calculating..... 280 of 392.8125\nWhirrrrrr calculating..... 281 of 392.8125\nWhirrrrrr calculating..... 282 of 392.8125\nWhirrrrrr calculating..... 283 of 392.8125\nWhirrrrrr calculating..... 284 of 392.8125\nWhirrrrrr calculating..... 285 of 392.8125\nWhirrrrrr calculating..... 286 of 392.8125\nWhirrrrrr calculating..... 287 of 392.8125\nWhirrrrrr calculating..... 288 of 392.8125\nWhirrrrrr calculating..... 289 of 392.8125\nWhirrrrrr calculating..... 290 of 392.8125\nWhirrrrrr calculating..... 291 of 392.8125\nWhirrrrrr calculating..... 292 of 392.8125\nWhirrrrrr calculating..... 293 of 392.8125\nWhirrrrrr calculating..... 294 of 392.8125\nWhirrrrrr calculating..... 295 of 392.8125\nWhirrrrrr calculating..... 296 of 392.8125\nWhirrrrrr calculating..... 297 of 392.8125\nWhirrrrrr calculating..... 298 of 392.8125\nWhirrrrrr calculating..... 299 of 392.8125\nWhirrrrrr calculating..... 300 of 392.8125\nWhirrrrrr calculating..... 301 of 392.8125\nWhirrrrrr calculating..... 302 of 392.8125\nWhirrrrrr calculating..... 303 of 392.8125\nWhirrrrrr calculating..... 304 of 392.8125\nWhirrrrrr calculating..... 305 of 392.8125\nWhirrrrrr calculating..... 306 of 392.8125\nWhirrrrrr calculating..... 307 of 392.8125\nWhirrrrrr calculating..... 308 of 392.8125\nWhirrrrrr calculating..... 309 of 392.8125\nWhirrrrrr calculating..... 310 of 392.8125\nWhirrrrrr calculating..... 311 of 392.8125\nWhirrrrrr calculating..... 312 of 392.8125\nWhirrrrrr calculating..... 313 of 392.8125\nWhirrrrrr calculating..... 314 of 392.8125\nWhirrrrrr calculating..... 315 of 392.8125\nWhirrrrrr calculating..... 316 of 392.8125\nWhirrrrrr calculating..... 317 of 392.8125\nWhirrrrrr calculating..... 318 of 392.8125\nWhirrrrrr calculating..... 319 of 392.8125\nWhirrrrrr calculating..... 320 of 392.8125\nWhirrrrrr calculating..... 321 of 392.8125\nWhirrrrrr calculating..... 322 of 392.8125\nWhirrrrrr calculating..... 323 of 392.8125\nWhirrrrrr calculating..... 324 of 392.8125\nWhirrrrrr calculating..... 325 of 392.8125\nWhirrrrrr calculating..... 326 of 392.8125\nWhirrrrrr calculating..... 327 of 392.8125\nWhirrrrrr calculating..... 328 of 392.8125\nWhirrrrrr calculating..... 329 of 392.8125\nWhirrrrrr calculating..... 330 of 392.8125\nWhirrrrrr calculating..... 331 of 392.8125\nWhirrrrrr calculating..... 332 of 392.8125\nWhirrrrrr calculating..... 333 of 392.8125\nWhirrrrrr calculating..... 334 of 392.8125\nWhirrrrrr calculating..... 335 of 392.8125\nWhirrrrrr calculating..... 336 of 392.8125\nWhirrrrrr calculating..... 337 of 392.8125\nWhirrrrrr calculating..... 338 of 392.8125\nWhirrrrrr calculating..... 339 of 392.8125\nWhirrrrrr calculating..... 340 of 392.8125\nWhirrrrrr calculating..... 341 of 392.8125\nWhirrrrrr calculating..... 342 of 392.8125\nWhirrrrrr calculating..... 343 of 392.8125\nWhirrrrrr calculating..... 344 of 392.8125\nWhirrrrrr calculating..... 345 of 392.8125\nWhirrrrrr calculating..... 346 of 392.8125\nWhirrrrrr calculating..... 347 of 392.8125\nWhirrrrrr calculating..... 348 of 392.8125\nWhirrrrrr calculating..... 349 of 392.8125\nWhirrrrrr calculating..... 350 of 392.8125\nWhirrrrrr calculating..... 351 of 392.8125\nWhirrrrrr calculating..... 352 of 392.8125\nWhirrrrrr calculating..... 353 of 392.8125\nWhirrrrrr calculating..... 354 of 392.8125\nWhirrrrrr calculating..... 355 of 392.8125\nWhirrrrrr calculating..... 356 of 392.8125\nWhirrrrrr calculating..... 357 of 392.8125\nWhirrrrrr calculating..... 358 of 392.8125\nWhirrrrrr calculating..... 359 of 392.8125\nWhirrrrrr calculating..... 360 of 392.8125\nWhirrrrrr calculating..... 361 of 392.8125\nWhirrrrrr calculating..... 362 of 392.8125\nWhirrrrrr calculating..... 363 of 392.8125\nWhirrrrrr calculating..... 364 of 392.8125\nWhirrrrrr calculating..... 365 of 392.8125\nWhirrrrrr calculating..... 366 of 392.8125\nWhirrrrrr calculating..... 367 of 392.8125\nWhirrrrrr calculating..... 368 of 392.8125\nWhirrrrrr calculating..... 369 of 392.8125\nWhirrrrrr calculating..... 370 of 392.8125\nWhirrrrrr calculating..... 371 of 392.8125\nWhirrrrrr calculating..... 372 of 392.8125\nWhirrrrrr calculating..... 373 of 392.8125\nWhirrrrrr calculating..... 374 of 392.8125\nWhirrrrrr calculating..... 375 of 392.8125\nWhirrrrrr calculating..... 376 of 392.8125\nWhirrrrrr calculating..... 377 of 392.8125\nWhirrrrrr calculating..... 378 of 392.8125\nWhirrrrrr calculating..... 379 of 392.8125\nWhirrrrrr calculating..... 380 of 392.8125\nWhirrrrrr calculating..... 381 of 392.8125\nWhirrrrrr calculating..... 382 of 392.8125\nWhirrrrrr calculating..... 383 of 392.8125\nWhirrrrrr calculating..... 384 of 392.8125\nWhirrrrrr calculating..... 385 of 392.8125\nWhirrrrrr calculating..... 386 of 392.8125\nWhirrrrrr calculating..... 387 of 392.8125\nWhirrrrrr calculating..... 388 of 392.8125\nWhirrrrrr calculating..... 389 of 392.8125\nWhirrrrrr calculating..... 390 of 392.8125\nWhirrrrrr calculating..... 391 of 392.8125\nWhirrrrrr calculating..... 392 of 392.8125\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'count_var' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-a6474dff2bc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mcount_var\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtr_dl\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mbatch_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'count_var' is not defined"
     ]
    }
   ],
   "source": [
    "mean = 0.0\n",
    "count_mn = 0\n",
    "for images, _ in tr_dl:\n",
    "    images = images.float()\n",
    "    batch_samples = images.size(0) \n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    print(f\"Whirrrrrr calculating..... {count_mn} of {train_size/batch_sz}\")\n",
    "    count_mn+= 1\n",
    "mean = mean / len(tr_dl.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([ 98.2207, 101.6702, 102.9898])"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Whirrrrrr calculating..... 0 of 392.8125\nWhirrrrrr calculating..... 1 of 392.8125\nWhirrrrrr calculating..... 2 of 392.8125\nWhirrrrrr calculating..... 3 of 392.8125\nWhirrrrrr calculating..... 4 of 392.8125\nWhirrrrrr calculating..... 5 of 392.8125\nWhirrrrrr calculating..... 6 of 392.8125\nWhirrrrrr calculating..... 7 of 392.8125\nWhirrrrrr calculating..... 8 of 392.8125\nWhirrrrrr calculating..... 9 of 392.8125\nWhirrrrrr calculating..... 10 of 392.8125\nWhirrrrrr calculating..... 11 of 392.8125\nWhirrrrrr calculating..... 12 of 392.8125\nWhirrrrrr calculating..... 13 of 392.8125\nWhirrrrrr calculating..... 14 of 392.8125\nWhirrrrrr calculating..... 15 of 392.8125\nWhirrrrrr calculating..... 16 of 392.8125\nWhirrrrrr calculating..... 17 of 392.8125\nWhirrrrrr calculating..... 18 of 392.8125\nWhirrrrrr calculating..... 19 of 392.8125\nWhirrrrrr calculating..... 20 of 392.8125\nWhirrrrrr calculating..... 21 of 392.8125\nWhirrrrrr calculating..... 22 of 392.8125\nWhirrrrrr calculating..... 23 of 392.8125\nWhirrrrrr calculating..... 24 of 392.8125\nWhirrrrrr calculating..... 25 of 392.8125\nWhirrrrrr calculating..... 26 of 392.8125\nWhirrrrrr calculating..... 27 of 392.8125\nWhirrrrrr calculating..... 28 of 392.8125\nWhirrrrrr calculating..... 29 of 392.8125\nWhirrrrrr calculating..... 30 of 392.8125\nWhirrrrrr calculating..... 31 of 392.8125\nWhirrrrrr calculating..... 32 of 392.8125\nWhirrrrrr calculating..... 33 of 392.8125\nWhirrrrrr calculating..... 34 of 392.8125\nWhirrrrrr calculating..... 35 of 392.8125\nWhirrrrrr calculating..... 36 of 392.8125\nWhirrrrrr calculating..... 37 of 392.8125\nWhirrrrrr calculating..... 38 of 392.8125\nWhirrrrrr calculating..... 39 of 392.8125\nWhirrrrrr calculating..... 40 of 392.8125\nWhirrrrrr calculating..... 41 of 392.8125\nWhirrrrrr calculating..... 42 of 392.8125\nWhirrrrrr calculating..... 43 of 392.8125\nWhirrrrrr calculating..... 44 of 392.8125\nWhirrrrrr calculating..... 45 of 392.8125\nWhirrrrrr calculating..... 46 of 392.8125\nWhirrrrrr calculating..... 47 of 392.8125\nWhirrrrrr calculating..... 48 of 392.8125\nWhirrrrrr calculating..... 49 of 392.8125\nWhirrrrrr calculating..... 50 of 392.8125\nWhirrrrrr calculating..... 51 of 392.8125\nWhirrrrrr calculating..... 52 of 392.8125\nWhirrrrrr calculating..... 53 of 392.8125\nWhirrrrrr calculating..... 54 of 392.8125\nWhirrrrrr calculating..... 55 of 392.8125\nWhirrrrrr calculating..... 56 of 392.8125\nWhirrrrrr calculating..... 57 of 392.8125\nWhirrrrrr calculating..... 58 of 392.8125\nWhirrrrrr calculating..... 59 of 392.8125\nWhirrrrrr calculating..... 60 of 392.8125\nWhirrrrrr calculating..... 61 of 392.8125\nWhirrrrrr calculating..... 62 of 392.8125\nWhirrrrrr calculating..... 63 of 392.8125\nWhirrrrrr calculating..... 64 of 392.8125\nWhirrrrrr calculating..... 65 of 392.8125\nWhirrrrrr calculating..... 66 of 392.8125\nWhirrrrrr calculating..... 67 of 392.8125\nWhirrrrrr calculating..... 68 of 392.8125\nWhirrrrrr calculating..... 69 of 392.8125\nWhirrrrrr calculating..... 70 of 392.8125\nWhirrrrrr calculating..... 71 of 392.8125\nWhirrrrrr calculating..... 72 of 392.8125\nWhirrrrrr calculating..... 73 of 392.8125\nWhirrrrrr calculating..... 74 of 392.8125\nWhirrrrrr calculating..... 75 of 392.8125\nWhirrrrrr calculating..... 76 of 392.8125\nWhirrrrrr calculating..... 77 of 392.8125\nWhirrrrrr calculating..... 78 of 392.8125\nWhirrrrrr calculating..... 79 of 392.8125\nWhirrrrrr calculating..... 80 of 392.8125\nWhirrrrrr calculating..... 81 of 392.8125\nWhirrrrrr calculating..... 82 of 392.8125\nWhirrrrrr calculating..... 83 of 392.8125\nWhirrrrrr calculating..... 84 of 392.8125\nWhirrrrrr calculating..... 85 of 392.8125\nWhirrrrrr calculating..... 86 of 392.8125\nWhirrrrrr calculating..... 87 of 392.8125\nWhirrrrrr calculating..... 88 of 392.8125\nWhirrrrrr calculating..... 89 of 392.8125\nWhirrrrrr calculating..... 90 of 392.8125\nWhirrrrrr calculating..... 91 of 392.8125\nWhirrrrrr calculating..... 92 of 392.8125\nWhirrrrrr calculating..... 93 of 392.8125\nWhirrrrrr calculating..... 94 of 392.8125\nWhirrrrrr calculating..... 95 of 392.8125\nWhirrrrrr calculating..... 96 of 392.8125\nWhirrrrrr calculating..... 97 of 392.8125\nWhirrrrrr calculating..... 98 of 392.8125\nWhirrrrrr calculating..... 99 of 392.8125\nWhirrrrrr calculating..... 100 of 392.8125\nWhirrrrrr calculating..... 101 of 392.8125\nWhirrrrrr calculating..... 102 of 392.8125\nWhirrrrrr calculating..... 103 of 392.8125\nWhirrrrrr calculating..... 104 of 392.8125\nWhirrrrrr calculating..... 105 of 392.8125\nWhirrrrrr calculating..... 106 of 392.8125\nWhirrrrrr calculating..... 107 of 392.8125\nWhirrrrrr calculating..... 108 of 392.8125\nWhirrrrrr calculating..... 109 of 392.8125\nWhirrrrrr calculating..... 110 of 392.8125\nWhirrrrrr calculating..... 111 of 392.8125\nWhirrrrrr calculating..... 112 of 392.8125\nWhirrrrrr calculating..... 113 of 392.8125\nWhirrrrrr calculating..... 114 of 392.8125\nWhirrrrrr calculating..... 115 of 392.8125\nWhirrrrrr calculating..... 116 of 392.8125\nWhirrrrrr calculating..... 117 of 392.8125\nWhirrrrrr calculating..... 118 of 392.8125\nWhirrrrrr calculating..... 119 of 392.8125\nWhirrrrrr calculating..... 120 of 392.8125\nWhirrrrrr calculating..... 121 of 392.8125\nWhirrrrrr calculating..... 122 of 392.8125\nWhirrrrrr calculating..... 123 of 392.8125\nWhirrrrrr calculating..... 124 of 392.8125\nWhirrrrrr calculating..... 125 of 392.8125\nWhirrrrrr calculating..... 126 of 392.8125\nWhirrrrrr calculating..... 127 of 392.8125\nWhirrrrrr calculating..... 128 of 392.8125\nWhirrrrrr calculating..... 129 of 392.8125\nWhirrrrrr calculating..... 130 of 392.8125\nWhirrrrrr calculating..... 131 of 392.8125\nWhirrrrrr calculating..... 132 of 392.8125\nWhirrrrrr calculating..... 133 of 392.8125\nWhirrrrrr calculating..... 134 of 392.8125\nWhirrrrrr calculating..... 135 of 392.8125\nWhirrrrrr calculating..... 136 of 392.8125\nWhirrrrrr calculating..... 137 of 392.8125\nWhirrrrrr calculating..... 138 of 392.8125\nWhirrrrrr calculating..... 139 of 392.8125\nWhirrrrrr calculating..... 140 of 392.8125\nWhirrrrrr calculating..... 141 of 392.8125\nWhirrrrrr calculating..... 142 of 392.8125\nWhirrrrrr calculating..... 143 of 392.8125\nWhirrrrrr calculating..... 144 of 392.8125\nWhirrrrrr calculating..... 145 of 392.8125\nWhirrrrrr calculating..... 146 of 392.8125\nWhirrrrrr calculating..... 147 of 392.8125\nWhirrrrrr calculating..... 148 of 392.8125\nWhirrrrrr calculating..... 149 of 392.8125\nWhirrrrrr calculating..... 150 of 392.8125\nWhirrrrrr calculating..... 151 of 392.8125\nWhirrrrrr calculating..... 152 of 392.8125\nWhirrrrrr calculating..... 153 of 392.8125\nWhirrrrrr calculating..... 154 of 392.8125\nWhirrrrrr calculating..... 155 of 392.8125\nWhirrrrrr calculating..... 156 of 392.8125\nWhirrrrrr calculating..... 157 of 392.8125\nWhirrrrrr calculating..... 158 of 392.8125\nWhirrrrrr calculating..... 159 of 392.8125\nWhirrrrrr calculating..... 160 of 392.8125\nWhirrrrrr calculating..... 161 of 392.8125\nWhirrrrrr calculating..... 162 of 392.8125\nWhirrrrrr calculating..... 163 of 392.8125\nWhirrrrrr calculating..... 164 of 392.8125\nWhirrrrrr calculating..... 165 of 392.8125\nWhirrrrrr calculating..... 166 of 392.8125\nWhirrrrrr calculating..... 167 of 392.8125\nWhirrrrrr calculating..... 168 of 392.8125\nWhirrrrrr calculating..... 169 of 392.8125\nWhirrrrrr calculating..... 170 of 392.8125\nWhirrrrrr calculating..... 171 of 392.8125\nWhirrrrrr calculating..... 172 of 392.8125\nWhirrrrrr calculating..... 173 of 392.8125\nWhirrrrrr calculating..... 174 of 392.8125\nWhirrrrrr calculating..... 175 of 392.8125\nWhirrrrrr calculating..... 176 of 392.8125\nWhirrrrrr calculating..... 177 of 392.8125\nWhirrrrrr calculating..... 178 of 392.8125\nWhirrrrrr calculating..... 179 of 392.8125\nWhirrrrrr calculating..... 180 of 392.8125\nWhirrrrrr calculating..... 181 of 392.8125\nWhirrrrrr calculating..... 182 of 392.8125\nWhirrrrrr calculating..... 183 of 392.8125\nWhirrrrrr calculating..... 184 of 392.8125\nWhirrrrrr calculating..... 185 of 392.8125\nWhirrrrrr calculating..... 186 of 392.8125\nWhirrrrrr calculating..... 187 of 392.8125\nWhirrrrrr calculating..... 188 of 392.8125\nWhirrrrrr calculating..... 189 of 392.8125\nWhirrrrrr calculating..... 190 of 392.8125\nWhirrrrrr calculating..... 191 of 392.8125\nWhirrrrrr calculating..... 192 of 392.8125\nWhirrrrrr calculating..... 193 of 392.8125\nWhirrrrrr calculating..... 194 of 392.8125\nWhirrrrrr calculating..... 195 of 392.8125\nWhirrrrrr calculating..... 196 of 392.8125\nWhirrrrrr calculating..... 197 of 392.8125\nWhirrrrrr calculating..... 198 of 392.8125\nWhirrrrrr calculating..... 199 of 392.8125\nWhirrrrrr calculating..... 200 of 392.8125\nWhirrrrrr calculating..... 201 of 392.8125\nWhirrrrrr calculating..... 202 of 392.8125\nWhirrrrrr calculating..... 203 of 392.8125\nWhirrrrrr calculating..... 204 of 392.8125\nWhirrrrrr calculating..... 205 of 392.8125\nWhirrrrrr calculating..... 206 of 392.8125\nWhirrrrrr calculating..... 207 of 392.8125\nWhirrrrrr calculating..... 208 of 392.8125\nWhirrrrrr calculating..... 209 of 392.8125\nWhirrrrrr calculating..... 210 of 392.8125\nWhirrrrrr calculating..... 211 of 392.8125\nWhirrrrrr calculating..... 212 of 392.8125\nWhirrrrrr calculating..... 213 of 392.8125\nWhirrrrrr calculating..... 214 of 392.8125\nWhirrrrrr calculating..... 215 of 392.8125\nWhirrrrrr calculating..... 216 of 392.8125\nWhirrrrrr calculating..... 217 of 392.8125\nWhirrrrrr calculating..... 218 of 392.8125\nWhirrrrrr calculating..... 219 of 392.8125\nWhirrrrrr calculating..... 220 of 392.8125\nWhirrrrrr calculating..... 221 of 392.8125\nWhirrrrrr calculating..... 222 of 392.8125\nWhirrrrrr calculating..... 223 of 392.8125\nWhirrrrrr calculating..... 224 of 392.8125\nWhirrrrrr calculating..... 225 of 392.8125\nWhirrrrrr calculating..... 226 of 392.8125\nWhirrrrrr calculating..... 227 of 392.8125\nWhirrrrrr calculating..... 228 of 392.8125\nWhirrrrrr calculating..... 229 of 392.8125\nWhirrrrrr calculating..... 230 of 392.8125\nWhirrrrrr calculating..... 231 of 392.8125\nWhirrrrrr calculating..... 232 of 392.8125\nWhirrrrrr calculating..... 233 of 392.8125\nWhirrrrrr calculating..... 234 of 392.8125\nWhirrrrrr calculating..... 235 of 392.8125\nWhirrrrrr calculating..... 236 of 392.8125\nWhirrrrrr calculating..... 237 of 392.8125\nWhirrrrrr calculating..... 238 of 392.8125\nWhirrrrrr calculating..... 239 of 392.8125\nWhirrrrrr calculating..... 240 of 392.8125\nWhirrrrrr calculating..... 241 of 392.8125\nWhirrrrrr calculating..... 242 of 392.8125\nWhirrrrrr calculating..... 243 of 392.8125\nWhirrrrrr calculating..... 244 of 392.8125\nWhirrrrrr calculating..... 245 of 392.8125\nWhirrrrrr calculating..... 246 of 392.8125\nWhirrrrrr calculating..... 247 of 392.8125\nWhirrrrrr calculating..... 248 of 392.8125\nWhirrrrrr calculating..... 249 of 392.8125\nWhirrrrrr calculating..... 250 of 392.8125\nWhirrrrrr calculating..... 251 of 392.8125\nWhirrrrrr calculating..... 252 of 392.8125\nWhirrrrrr calculating..... 253 of 392.8125\nWhirrrrrr calculating..... 254 of 392.8125\nWhirrrrrr calculating..... 255 of 392.8125\nWhirrrrrr calculating..... 256 of 392.8125\nWhirrrrrr calculating..... 257 of 392.8125\nWhirrrrrr calculating..... 258 of 392.8125\nWhirrrrrr calculating..... 259 of 392.8125\nWhirrrrrr calculating..... 260 of 392.8125\nWhirrrrrr calculating..... 261 of 392.8125\nWhirrrrrr calculating..... 262 of 392.8125\nWhirrrrrr calculating..... 263 of 392.8125\nWhirrrrrr calculating..... 264 of 392.8125\nWhirrrrrr calculating..... 265 of 392.8125\nWhirrrrrr calculating..... 266 of 392.8125\nWhirrrrrr calculating..... 267 of 392.8125\nWhirrrrrr calculating..... 268 of 392.8125\nWhirrrrrr calculating..... 269 of 392.8125\nWhirrrrrr calculating..... 270 of 392.8125\nWhirrrrrr calculating..... 271 of 392.8125\nWhirrrrrr calculating..... 272 of 392.8125\nWhirrrrrr calculating..... 273 of 392.8125\nWhirrrrrr calculating..... 274 of 392.8125\nWhirrrrrr calculating..... 275 of 392.8125\nWhirrrrrr calculating..... 276 of 392.8125\nWhirrrrrr calculating..... 277 of 392.8125\nWhirrrrrr calculating..... 278 of 392.8125\nWhirrrrrr calculating..... 279 of 392.8125\nWhirrrrrr calculating..... 280 of 392.8125\nWhirrrrrr calculating..... 281 of 392.8125\nWhirrrrrr calculating..... 282 of 392.8125\nWhirrrrrr calculating..... 283 of 392.8125\nWhirrrrrr calculating..... 284 of 392.8125\nWhirrrrrr calculating..... 285 of 392.8125\nWhirrrrrr calculating..... 286 of 392.8125\nWhirrrrrr calculating..... 287 of 392.8125\nWhirrrrrr calculating..... 288 of 392.8125\nWhirrrrrr calculating..... 289 of 392.8125\nWhirrrrrr calculating..... 290 of 392.8125\nWhirrrrrr calculating..... 291 of 392.8125\nWhirrrrrr calculating..... 292 of 392.8125\nWhirrrrrr calculating..... 293 of 392.8125\nWhirrrrrr calculating..... 294 of 392.8125\nWhirrrrrr calculating..... 295 of 392.8125\nWhirrrrrr calculating..... 296 of 392.8125\nWhirrrrrr calculating..... 297 of 392.8125\nWhirrrrrr calculating..... 298 of 392.8125\nWhirrrrrr calculating..... 299 of 392.8125\nWhirrrrrr calculating..... 300 of 392.8125\nWhirrrrrr calculating..... 301 of 392.8125\nWhirrrrrr calculating..... 302 of 392.8125\nWhirrrrrr calculating..... 303 of 392.8125\nWhirrrrrr calculating..... 304 of 392.8125\nWhirrrrrr calculating..... 305 of 392.8125\nWhirrrrrr calculating..... 306 of 392.8125\nWhirrrrrr calculating..... 307 of 392.8125\nWhirrrrrr calculating..... 308 of 392.8125\nWhirrrrrr calculating..... 309 of 392.8125\nWhirrrrrr calculating..... 310 of 392.8125\nWhirrrrrr calculating..... 311 of 392.8125\nWhirrrrrr calculating..... 312 of 392.8125\nWhirrrrrr calculating..... 313 of 392.8125\nWhirrrrrr calculating..... 314 of 392.8125\nWhirrrrrr calculating..... 315 of 392.8125\nWhirrrrrr calculating..... 316 of 392.8125\nWhirrrrrr calculating..... 317 of 392.8125\nWhirrrrrr calculating..... 318 of 392.8125\nWhirrrrrr calculating..... 319 of 392.8125\nWhirrrrrr calculating..... 320 of 392.8125\nWhirrrrrr calculating..... 321 of 392.8125\nWhirrrrrr calculating..... 322 of 392.8125\nWhirrrrrr calculating..... 323 of 392.8125\nWhirrrrrr calculating..... 324 of 392.8125\nWhirrrrrr calculating..... 325 of 392.8125\nWhirrrrrr calculating..... 326 of 392.8125\nWhirrrrrr calculating..... 327 of 392.8125\nWhirrrrrr calculating..... 328 of 392.8125\nWhirrrrrr calculating..... 329 of 392.8125\nWhirrrrrr calculating..... 330 of 392.8125\nWhirrrrrr calculating..... 331 of 392.8125\nWhirrrrrr calculating..... 332 of 392.8125\nWhirrrrrr calculating..... 333 of 392.8125\nWhirrrrrr calculating..... 334 of 392.8125\nWhirrrrrr calculating..... 335 of 392.8125\nWhirrrrrr calculating..... 336 of 392.8125\nWhirrrrrr calculating..... 337 of 392.8125\nWhirrrrrr calculating..... 338 of 392.8125\nWhirrrrrr calculating..... 339 of 392.8125\nWhirrrrrr calculating..... 340 of 392.8125\nWhirrrrrr calculating..... 341 of 392.8125\nWhirrrrrr calculating..... 342 of 392.8125\nWhirrrrrr calculating..... 343 of 392.8125\nWhirrrrrr calculating..... 344 of 392.8125\nWhirrrrrr calculating..... 345 of 392.8125\nWhirrrrrr calculating..... 346 of 392.8125\nWhirrrrrr calculating..... 347 of 392.8125\nWhirrrrrr calculating..... 348 of 392.8125\nWhirrrrrr calculating..... 349 of 392.8125\nWhirrrrrr calculating..... 350 of 392.8125\nWhirrrrrr calculating..... 351 of 392.8125\nWhirrrrrr calculating..... 352 of 392.8125\nWhirrrrrr calculating..... 353 of 392.8125\nWhirrrrrr calculating..... 354 of 392.8125\nWhirrrrrr calculating..... 355 of 392.8125\nWhirrrrrr calculating..... 356 of 392.8125\nWhirrrrrr calculating..... 357 of 392.8125\nWhirrrrrr calculating..... 358 of 392.8125\nWhirrrrrr calculating..... 359 of 392.8125\nWhirrrrrr calculating..... 360 of 392.8125\nWhirrrrrr calculating..... 361 of 392.8125\nWhirrrrrr calculating..... 362 of 392.8125\nWhirrrrrr calculating..... 363 of 392.8125\nWhirrrrrr calculating..... 364 of 392.8125\nWhirrrrrr calculating..... 365 of 392.8125\nWhirrrrrr calculating..... 366 of 392.8125\nWhirrrrrr calculating..... 367 of 392.8125\nWhirrrrrr calculating..... 368 of 392.8125\nWhirrrrrr calculating..... 369 of 392.8125\nWhirrrrrr calculating..... 370 of 392.8125\nWhirrrrrr calculating..... 371 of 392.8125\nWhirrrrrr calculating..... 372 of 392.8125\nWhirrrrrr calculating..... 373 of 392.8125\nWhirrrrrr calculating..... 374 of 392.8125\nWhirrrrrr calculating..... 375 of 392.8125\nWhirrrrrr calculating..... 376 of 392.8125\nWhirrrrrr calculating..... 377 of 392.8125\nWhirrrrrr calculating..... 378 of 392.8125\nWhirrrrrr calculating..... 379 of 392.8125\nWhirrrrrr calculating..... 380 of 392.8125\nWhirrrrrr calculating..... 381 of 392.8125\nWhirrrrrr calculating..... 382 of 392.8125\nWhirrrrrr calculating..... 383 of 392.8125\nWhirrrrrr calculating..... 384 of 392.8125\nWhirrrrrr calculating..... 385 of 392.8125\nWhirrrrrr calculating..... 386 of 392.8125\nWhirrrrrr calculating..... 387 of 392.8125\nWhirrrrrr calculating..... 388 of 392.8125\nWhirrrrrr calculating..... 389 of 392.8125\nWhirrrrrr calculating..... 390 of 392.8125\nWhirrrrrr calculating..... 391 of 392.8125\nWhirrrrrr calculating..... 392 of 392.8125\n"
    }
   ],
   "source": [
    "\n",
    "var = 0.0\n",
    "count_var =0\n",
    "for images, _ in tr_dl:\n",
    "    batch_samples = images.size(0)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    var += ((images - mean.unsqueeze(1))**2).sum([0,2])\n",
    "    print(f\"Whirrrrrr calculating..... {count_var} of {train_size/batch_sz}\")\n",
    "    count_var+= 1\n",
    "std = torch.sqrt(var / (len(tr_dl.dataset)*720*1280))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([63.4003, 64.1523, 64.4491])"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Moderate_tr_stats = mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean is tensor([ 98.2207, 101.6702, 102.9898])\n",
    "std is tensor([63.4003, 64.1523, 64.4491])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Normalize(mean=tensor([ 98.2207, 101.6702, 102.9898]), std=tensor([63.4003, 64.1523, 64.4491]))"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "transforms.Normalize(*Moderate_tr_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "*************MAKE SURE THE PATH FILE IN THE FOR LOOP IS THE BASE IMAGE DIRECTORY ON YOUR COMPUTER**************\n"
    }
   ],
   "source": [
    "normed_dataset = ModerateDataset(trans_on=True, transform=transforms.Normalize(*Moderate_tr_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_dl = DataLoader(normed_dataset,  batch_size=batch_sz, shuffle=True,  num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[tensor([[[[2, 2, 2,  ..., 3, 3, 3],\n           [2, 2, 2,  ..., 3, 3, 3],\n           [2, 2, 2,  ..., 3, 3, 3],\n           ...,\n           [3, 3, 3,  ..., 3, 3, 3],\n           [3, 3, 3,  ..., 3, 3, 3],\n           [3, 3, 3,  ..., 3, 3, 3]],\n \n          [[2, 2, 2,  ..., 2, 2, 2],\n           [2, 2, 2,  ..., 2, 2, 2],\n           [2, 2, 2,  ..., 2, 2, 2],\n           ...,\n           [2, 2, 2,  ..., 3, 3, 3],\n           [2, 2, 2,  ..., 3, 3, 3],\n           [2, 2, 2,  ..., 3, 3, 3]],\n \n          [[2, 2, 2,  ..., 2, 2, 2],\n           [2, 2, 2,  ..., 2, 2, 2],\n           [2, 2, 2,  ..., 2, 2, 2],\n           ...,\n           [2, 2, 2,  ..., 3, 3, 3],\n           [2, 2, 2,  ..., 3, 3, 3],\n           [2, 2, 2,  ..., 3, 3, 3]]],\n \n \n         [[[3, 3, 3,  ..., 0, 0, 0],\n           [3, 3, 3,  ..., 0, 0, 0],\n           [3, 3, 3,  ..., 0, 0, 0],\n           ...,\n           [2, 2, 2,  ..., 1, 1, 1],\n           [2, 2, 2,  ..., 1, 1, 1],\n           [2, 2, 2,  ..., 1, 1, 1]],\n \n          [[3, 3, 3,  ..., 0, 0, 0],\n           [3, 3, 3,  ..., 0, 0, 0],\n           [3, 3, 3,  ..., 0, 0, 0],\n           ...,\n           [2, 2, 2,  ..., 1, 1, 1],\n           [2, 2, 2,  ..., 1, 1, 1],\n           [2, 2, 2,  ..., 1, 1, 1]],\n \n          [[3, 3, 3,  ..., 0, 0, 0],\n           [3, 3, 3,  ..., 0, 0, 0],\n           [3, 3, 3,  ..., 0, 0, 0],\n           ...,\n           [2, 2, 2,  ..., 0, 0, 0],\n           [2, 2, 2,  ..., 0, 0, 0],\n           [2, 2, 2,  ..., 0, 0, 0]]],\n \n \n         [[[3, 3, 3,  ..., 0, 0, 0],\n           [3, 3, 3,  ..., 0, 0, 0],\n           [3, 3, 3,  ..., 0, 0, 0],\n           ...,\n           [1, 1, 1,  ..., 1, 1, 1],\n           [1, 1, 1,  ..., 1, 1, 1],\n           [1, 1, 1,  ..., 1, 1, 1]],\n \n          [[3, 3, 3,  ..., 0, 0, 0],\n           [3, 3, 3,  ..., 0, 0, 0],\n           [3, 3, 3,  ..., 0, 0, 0],\n           ...,\n           [1, 1, 1,  ..., 1, 1, 1],\n           [1, 1, 1,  ..., 1, 1, 1],\n           [1, 1, 1,  ..., 1, 1, 1]],\n \n          [[3, 3, 3,  ..., 0, 0, 0],\n           [3, 3, 3,  ..., 0, 0, 0],\n           [3, 3, 3,  ..., 0, 0, 0],\n           ...,\n           [0, 0, 0,  ..., 0, 0, 0],\n           [0, 0, 0,  ..., 0, 0, 0],\n           [0, 0, 0,  ..., 0, 0, 0]]],\n \n \n         ...,\n \n \n         [[[3, 2, 2,  ..., 0, 0, 0],\n           [2, 2, 2,  ..., 0, 0, 0],\n           [2, 2, 2,  ..., 0, 0, 0],\n           ...,\n           [1, 1, 1,  ..., 3, 3, 3],\n           [1, 1, 1,  ..., 3, 3, 3],\n           [1, 1, 2,  ..., 0, 3, 3]],\n \n          [[2, 2, 2,  ..., 0, 0, 0],\n           [2, 2, 2,  ..., 0, 0, 0],\n           [2, 2, 2,  ..., 0, 0, 0],\n           ...,\n           [1, 1, 1,  ..., 3, 3, 3],\n           [1, 1, 1,  ..., 3, 3, 3],\n           [1, 1, 1,  ..., 3, 3, 3]],\n \n          [[2, 2, 2,  ..., 0, 0, 0],\n           [2, 2, 2,  ..., 0, 0, 0],\n           [2, 2, 2,  ..., 0, 0, 0],\n           ...,\n           [1, 1, 1,  ..., 2, 3, 3],\n           [1, 1, 1,  ..., 2, 3, 3],\n           [1, 1, 1,  ..., 3, 3, 2]]],\n \n \n         [[[4, 4, 3,  ..., 0, 0, 0],\n           [4, 4, 4,  ..., 0, 0, 0],\n           [4, 4, 4,  ..., 0, 0, 0],\n           ...,\n           [1, 1, 1,  ..., 1, 1, 1],\n           [1, 1, 1,  ..., 1, 1, 1],\n           [1, 1, 1,  ..., 1, 1, 1]],\n \n          [[3, 3, 3,  ..., 0, 0, 0],\n           [3, 3, 3,  ..., 0, 0, 0],\n           [3, 3, 3,  ..., 0, 0, 0],\n           ...,\n           [1, 1, 1,  ..., 1, 1, 1],\n           [1, 1, 1,  ..., 1, 1, 1],\n           [1, 1, 1,  ..., 1, 1, 1]],\n \n          [[3, 3, 3,  ..., 0, 0, 0],\n           [3, 3, 3,  ..., 0, 0, 0],\n           [3, 3, 3,  ..., 0, 0, 0],\n           ...,\n           [1, 1, 1,  ..., 0, 0, 0],\n           [1, 1, 1,  ..., 0, 0, 0],\n           [1, 1, 1,  ..., 0, 0, 0]]],\n \n \n         [[[0, 0, 0,  ..., 2, 2, 2],\n           [0, 0, 0,  ..., 2, 2, 2],\n           [0, 0, 0,  ..., 2, 2, 2],\n           ...,\n           [1, 1, 1,  ..., 1, 1, 1],\n           [1, 1, 1,  ..., 1, 1, 1],\n           [1, 1, 1,  ..., 1, 1, 1]],\n \n          [[1, 1, 1,  ..., 2, 2, 2],\n           [0, 1, 1,  ..., 2, 2, 2],\n           [0, 0, 1,  ..., 2, 2, 2],\n           ...,\n           [1, 1, 1,  ..., 1, 1, 1],\n           [1, 1, 1,  ..., 1, 1, 1],\n           [1, 1, 1,  ..., 1, 1, 1]],\n \n          [[1, 1, 1,  ..., 2, 2, 2],\n           [0, 1, 1,  ..., 2, 2, 2],\n           [0, 1, 1,  ..., 2, 2, 2],\n           ...,\n           [1, 1, 1,  ..., 1, 1, 1],\n           [1, 1, 1,  ..., 1, 1, 1],\n           [1, 1, 1,  ..., 1, 1, 1]]]], dtype=torch.uint8),\n tensor([[[0.0415, 0.0415, 0.0415,  ..., 0.1278, 0.1278, 0.1278],\n          [0.0414, 0.0414, 0.0414,  ..., 0.1273, 0.1272, 0.1272],\n          [0.0413, 0.0413, 0.0413,  ..., 0.1267, 0.1267, 0.1267],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0042, 0.0042],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0042, 0.0042],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0042, 0.0042]],\n \n         [[0.0434, 0.0434, 0.0434,  ..., 0.1295, 0.1295, 0.1294],\n          [0.0433, 0.0433, 0.0433,  ..., 0.1289, 0.1289, 0.1288],\n          [0.0432, 0.0432, 0.0432,  ..., 0.1283, 0.1283, 0.1283],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n \n         [[0.0435, 0.0435, 0.0435,  ..., 0.1296, 0.1295, 0.1295],\n          [0.0434, 0.0434, 0.0434,  ..., 0.1290, 0.1289, 0.1289],\n          [0.0433, 0.0433, 0.0433,  ..., 0.1284, 0.1284, 0.1284],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0005, 0.0005],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0004, 0.0005],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0004, 0.0004]],\n \n         ...,\n \n         [[0.0433, 0.0433, 0.0433,  ..., 0.1321, 0.1320, 0.1320],\n          [0.0432, 0.0432, 0.0432,  ..., 0.1314, 0.1314, 0.1314],\n          [0.0431, 0.0431, 0.0431,  ..., 0.1308, 0.1308, 0.1308],\n          ...,\n          [0.0002, 0.0002, 0.0002,  ..., 0.0063, 0.0063, 0.0063],\n          [0.0002, 0.0002, 0.0002,  ..., 0.0063, 0.0063, 0.0063],\n          [0.0002, 0.0002, 0.0002,  ..., 0.0063, 0.0063, 0.0064]],\n \n         [[0.0435, 0.0435, 0.0435,  ..., 0.1305, 0.1305, 0.1305],\n          [0.0434, 0.0434, 0.0434,  ..., 0.1299, 0.1299, 0.1298],\n          [0.0433, 0.0433, 0.0433,  ..., 0.1293, 0.1292, 0.1292],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n \n         [[0.0421, 0.0421, 0.0421,  ..., 0.1336, 0.1336, 0.1335],\n          [0.0420, 0.0420, 0.0420,  ..., 0.1330, 0.1329, 0.1329],\n          [0.0419, 0.0419, 0.0419,  ..., 0.1323, 0.1323, 0.1323],\n          ...,\n          [0.0037, 0.0037, 0.0037,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0037, 0.0037, 0.0037,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0037, 0.0037, 0.0037,  ..., 0.0000, 0.0000, 0.0000]]])]"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "next(iter(normed_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kitti dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'C:/Users/Ben/OneDrive - Bournemouth University/Computer Vision/Datasets/Kitti/depth_selection/val_selection_cropped/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kitti(Dataset):\n",
    "  def __init__(self, data_root, transform=torchvision.transforms.ToTensor(),):\n",
    "    self.samples = {}\n",
    "    self.transform = transform\n",
    "    for file in os.listdir(data_root):\n",
    "      subfolder_list = [ os.path.join( data_root, file , subfolder ) for subfolder in os.listdir(os.path.join(data_root, file))]\n",
    "      self.samples[file] = subfolder_list\n",
    "\n",
    "    keys = [ key for key in self.samples.keys()]\n",
    "    self.RGB_DIRS   = self.samples[keys[0]]\n",
    "    self.DEPTH_DIRS = self.samples[keys[1]]\n",
    "\n",
    "    self.length = int(sum([len(self.samples[key]) for key in self.samples.keys()])*0.5)\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.length\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    RGB_IMAGES   = Image.open(self.RGB_DIRS[index])\n",
    "    DEPTH_IMAGES = Image.open(self.DEPTH_DIRS[index])\n",
    "\n",
    "    if self.transform:\n",
    "      RGB_IMAGES   = self.transform(RGB_IMAGES)\n",
    "      DEPTH_IMAGES = self.transform(DEPTH_IMAGES)    \n",
    "      \n",
    "    return RGB_IMAGES, DEPTH_IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "ki = Kitti(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor([[[0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          ...,\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.int32),\n tensor([[[1.0000, 1.0000, 1.0000,  ..., 0.0314, 0.0314, 0.0353],\n          [1.0000, 1.0000, 1.0000,  ..., 0.0353, 0.0353, 0.0353],\n          [1.0000, 1.0000, 1.0000,  ..., 0.0392, 0.0392, 0.0353],\n          ...,\n          [0.2627, 0.2627, 0.2588,  ..., 0.1686, 0.2196, 0.2863],\n          [0.2549, 0.2627, 0.2706,  ..., 0.1294, 0.1451, 0.1725],\n          [0.2588, 0.2627, 0.2667,  ..., 0.2314, 0.2235, 0.2275]],\n \n         [[1.0000, 1.0000, 1.0000,  ..., 0.0431, 0.0471, 0.0471],\n          [1.0000, 1.0000, 1.0000,  ..., 0.0392, 0.0353, 0.0392],\n          [1.0000, 1.0000, 1.0000,  ..., 0.0431, 0.0392, 0.0353],\n          ...,\n          [0.2706, 0.2745, 0.2706,  ..., 0.2902, 0.3098, 0.2980],\n          [0.2745, 0.2745, 0.2706,  ..., 0.2588, 0.2706, 0.2667],\n          [0.2667, 0.2627, 0.2627,  ..., 0.2706, 0.2824, 0.2902]],\n \n         [[1.0000, 1.0000, 1.0000,  ..., 0.0314, 0.0314, 0.0314],\n          [1.0000, 1.0000, 1.0000,  ..., 0.0314, 0.0314, 0.0314],\n          [1.0000, 1.0000, 1.0000,  ..., 0.0353, 0.0353, 0.0353],\n          ...,\n          [0.2745, 0.2667, 0.2667,  ..., 0.1490, 0.1608, 0.1608],\n          [0.2706, 0.2706, 0.2706,  ..., 0.1686, 0.1843, 0.1765],\n          [0.2745, 0.2824, 0.2784,  ..., 0.1725, 0.2118, 0.2039]]]))"
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "source": [
    "ki.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1000"
     },
     "metadata": {},
     "execution_count": 120
    }
   ],
   "source": [
    "total_Data = Kitti(root_dir)\n",
    "rgb_image, depth_image = total_Data[100]\n",
    "len(total_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Invalid shape (3, 352, 1216) for image data",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-121-6292d1758e9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0max1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdepth_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0max2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0max2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrgb_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\bul7cv\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1597\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1598\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1599\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1601\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\bul7cv\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    367\u001b[0m                 \u001b[1;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[1;32m--> 369\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\bul7cv\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    367\u001b[0m                 \u001b[1;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[1;32m--> 369\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\bul7cv\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5677\u001b[0m                               resample=resample, **kwargs)\n\u001b[0;32m   5678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5679\u001b[1;33m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5680\u001b[0m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5681\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\bul7cv\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mset_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    687\u001b[0m         if not (self._A.ndim == 2\n\u001b[0;32m    688\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m--> 689\u001b[1;33m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0m\u001b[0;32m    690\u001b[0m                             .format(self._A.shape))\n\u001b[0;32m    691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid shape (3, 352, 1216) for image data"
     ]
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.imshow(depth_image)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.imshow(rgb_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitti_dataloader = DataLoader(total_Data,  batch_size=16, shuffle=True,  num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([3, 352, 1216])"
     },
     "metadata": {},
     "execution_count": 129
    }
   ],
   "source": [
    "next(iter(kitti_dataloader))[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}